{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import torchvision\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a WebDataset\n",
    "\n",
    "## Using `tar`\n",
    "\n",
    "Since WebDatasets are just regular tar files, you can usually create them by just using the `tar` command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with\n",
    "\n",
    "```\n",
    "$ tar --sort=name -cf dataset.tar dataset/\n",
    "```\n",
    "\n",
    "If your dataset has some other directory layout, you may need a different file name in the archive from the name on disk. You can use the `--transform` argument to GNU tar to transform file names. You can also use the `-T` argument to read the files from a text file and embed other options in that text file.\n",
    "\n",
    "## The `tarp create` Command\n",
    "\n",
    "The [`tarp`](https://github.com/tmbdev/tarp) command is a little utility for manipulating `tar` archives. Its `create` subcommand makes it particularly simple to construct tar archives from files. The `tarp create` command takes a recipe for building\n",
    "a tar archive that contains lines of the form:\n",
    "\n",
    "```\n",
    "archive-name-1 source-name-1\n",
    "archive-name-2 source-name-2\n",
    "...\n",
    "```\n",
    "\n",
    "The source name can either be a file, \"text:something\", or \"pipe:something\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatically in Python\n",
    "\n",
    "You can also create a WebDataset with library functions in this library:\n",
    "\n",
    "- `webdataset.TarWriter` takes dictionaries containing key value pairs and writes them to disk\n",
    "- `webdataset.ShardWriter` takes dictionaries containing key value pairs and writes them to disk as a series of shards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Conversion of Any Dataset\n",
    "\n",
    "Here is a quick way of converting an existing dataset into a WebDataset; this will store all tensors as Python pickles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59000\r"
     ]
    }
   ],
   "source": [
    "dataset = torchvision.datasets.MNIST(root=\"./temp\", download=True)\n",
    "sink = wds.TarWriter(\"mnist.tar\")\n",
    "for index, (input, output) in enumerate(dataset):\n",
    "    if index%1000==0:\n",
    "        print(f\"{index:6d}\", end=\"\\r\", flush=True, file=sys.stderr)\n",
    "    sink.write({\n",
    "        \"__key__\": \"sample%06d\" % index,\n",
    "        \"input.pyd\": input,\n",
    "        \"output.pyd\": output,\n",
    "    })\n",
    "sink.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 tmb tmb 276490240 Oct 31 14:05 mnist.tar\n",
      "-r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000000.input.pyd\n",
      "-r--r--r-- bigdata/bigdata   5 2020-10-31 14:05 sample000000.output.pyd\n",
      "-r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000001.input.pyd\n",
      "-r--r--r-- bigdata/bigdata   5 2020-10-31 14:05 sample000001.output.pyd\n",
      "-r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000002.input.pyd\n",
      "-r--r--r-- bigdata/bigdata   5 2020-10-31 14:05 sample000002.output.pyd\n",
      "-r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000003.input.pyd\n",
      "-r--r--r-- bigdata/bigdata   5 2020-10-31 14:05 sample000003.output.pyd\n",
      "-r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000004.input.pyd\n",
      "-r--r--r-- bigdata/bigdata   5 2020-10-31 14:05 sample000004.output.pyd\n",
      "tar: write error\n"
     ]
    }
   ],
   "source": [
    "!ls -l mnist.tar\n",
    "!tar tvf mnist.tar | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing data as Python pickles allows most common Python datatypes to be stored, it is lossless, and the format is fast to decode.\n",
    "However, it is uncompressed and cannot be read by non-Python programs. It's often better to choose other storage formats, e.g.,\n",
    "taking advantage of common image compression formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Conversion of Any Dataset with Compression\n",
    "\n",
    "If you know that the input is an image and the output is an integer class, you can also write something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59000\r"
     ]
    }
   ],
   "source": [
    "dataset = torchvision.datasets.MNIST(root=\"./temp\", download=True)\n",
    "sink = wds.TarWriter(\"mnist.tar\")\n",
    "for index, (input, output) in enumerate(dataset):\n",
    "    if index%1000==0:\n",
    "        print(f\"{index:6d}\", end=\"\\r\", flush=True, file=sys.stderr)\n",
    "    sink.write({\n",
    "        \"__key__\": \"sample%06d\" % index,\n",
    "        \"ppm\": input,\n",
    "        \"cls\": output,\n",
    "    })\n",
    "sink.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 tmb tmb 276490240 Oct 31 14:05 mnist.tar\n",
      "-r--r--r-- bigdata/bigdata   1 2020-10-31 14:05 sample000000.cls\n",
      "-r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000000.ppm\n",
      "-r--r--r-- bigdata/bigdata   1 2020-10-31 14:05 sample000001.cls\n",
      "-r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000001.ppm\n",
      "-r--r--r-- bigdata/bigdata   1 2020-10-31 14:05 sample000002.cls\n",
      "-r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000002.ppm\n",
      "-r--r--r-- bigdata/bigdata   1 2020-10-31 14:05 sample000003.cls\n",
      "-r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000003.ppm\n",
      "-r--r--r-- bigdata/bigdata   1 2020-10-31 14:05 sample000004.cls\n",
      "-r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000004.ppm\n",
      "tar: write error\n"
     ]
    }
   ],
   "source": [
    "!ls -l mnist.tar\n",
    "!tar tvf mnist.tar | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we needed to do was to change the key from `.input.pyd` to `.ppm`; this will trigger using an image compressor (in this case, writing the image in PPM format). You can use different image types depending on what speed, compression, and quality tradeoffs you want to make. If you want to encode data yourself, you can simply convert it to a byte string yourself, store it under the desired key in the sample, and that binary string will get written out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `TarWriter`/`ShardWriter` with Binary Data (Lossless Writing)\n",
    "\n",
    "The `assert` statements in that loop are not necessary, but they document and illustrate the expectations for this\n",
    "particular dataset. Generally, the \".jpg\" encoder can actually encode a wide variety of array types as images. The\n",
    "\".cls\" encoder always requires an integer for encoding.\n",
    "\n",
    "Here is how you can use `TarWriter` for writing a dataset without using an encoder:\n",
    "\n",
    "```Python\n",
    "sink = wds.TarWriter(\"dest.tar\", encoder=False)\n",
    "for basename in basenames:\n",
    "    with open(f\"{basename}.png\", \"rb\") as stream):\n",
    "        image = stream.read()\n",
    "    cls = lookup_cls(basename)\n",
    "    sample = {\n",
    "        \"__key__\": basename,\n",
    "        \"input.png\": image,\n",
    "        \"target.cls\": cls\n",
    "    }\n",
    "    sink.write(sample)\n",
    "sink.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no encoder is used, if you want to be able to read this data with the default decoder, `image` must contain a byte string corresponding to a PNG image (as indicated by the \".png\" extension on its dictionary key), and `cls` must contain an integer encoded in ASCII (as indicated by the \".cls\" extension on its dictionary key)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
