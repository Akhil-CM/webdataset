{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Test](https://github.com/tmbdev/webdataset/workflows/Test/badge.svg)](https://github.com/tmbdev/webdataset/actions?query=workflow%3ATest)\n",
    "[![DeepSource](https://static.deepsource.io/deepsource-badge-light-mini.svg)](https://deepsource.io/gh/tmbdev/webdataset/?ref=repository-badge)\n",
    "\n",
    "# WebDataset\n",
    "\n",
    "WebDataset is a PyTorch Dataset (IterableDataset) implementation providing\n",
    "efficient access to datasets stored in POSIX tar archives and uses only sequential/streaming\n",
    "data access. This brings substantial performance advantage in many compute environments, and it\n",
    "is essential for very large scale training.\n",
    "\n",
    "WebDataset implements standard PyTorch `IterableDataset` interface and works with the PyTorch `DataLoader`.\n",
    "Access to datasets is as simple as:\n",
    "\n",
    "```Python\n",
    "dataset = wds.WebDataset(url).shuffle(1000).decode(\"torchrgb\").to_tuple(\"jpg;png\", \"json\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=16)\n",
    "\n",
    "for inputs, outputs in dataloader:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that code snippet, `url` can refer to a local file, a local HTTP server, a cloud storage object, an object\n",
    "on an object store, or even the output of arbitrary command pipelines.\n",
    "\n",
    "WebDataset fulfills a similar function to Tensorflow's TFRecord/tf.Example\n",
    "classes, but it is much easier to adopt because it does not actually\n",
    "require any kind of data conversion: data is stored in exactly the same\n",
    "format inside tar files as it is on disk, and all preprocessing and data\n",
    "augmentation code remains unchanged.\n",
    "\n",
    "# Installation and Documentation\n",
    "\n",
    "    $ pip install webdataset\n",
    "\n",
    "For the Github version:\n",
    "\n",
    "    $ pip install git+https://github.com/tmbdev/webdataset.git\n",
    "\n",
    "Documentation: [ReadTheDocs](http://webdataset.readthedocs.io)\n",
    "\n",
    "Examples:\n",
    "\n",
    "- [loading videos](https://github.com/tmbdev/webdataset/blob/master/docs/video-loading-example.ipynb)\n",
    "- [splitting raw videos into clips for training](https://github.com/tmbdev/webdataset/blob/master/docs/ytsamples-split.ipynb)\n",
    "- [converting the Falling Things dataset](https://github.com/tmbdev/webdataset/blob/master/docs/falling-things-make-shards.ipynb)\n",
    "\n",
    "# Introductory Videos\n",
    "\n",
    "Here are some videos talking about WebDataset and large scale deep learning:\n",
    "\n",
    "- [Introduction to Large Scale Deep Learning](https://www.youtube.com/watch?v=kNuA2wflygM)\n",
    "- [Loading Training Data with WebDataset](https://www.youtube.com/watch?v=mTv_ePYeBhs)\n",
    "- [Creating Datasets in WebDataset Format](https://www.youtube.com/watch?v=v_PacO-3OGQ)\n",
    "- [Tools for Working with Large Datasets](https://www.youtube.com/watch?v=kIv8zDpRUec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using WebDataset\n",
    "\n",
    "WebDataset reads dataset that are stored as tar files, with the simple convention that files that belong together and make up a training sample share the same basename. WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e39871fd9fd74f55.jpg\n",
      "e39871fd9fd74f55.json\n",
      "f18b91585c4d3f3e.jpg\n",
      "f18b91585c4d3f3e.json\n",
      "ede6e66b2fb59aab.jpg\n",
      "ede6e66b2fb59aab.json\n",
      "ed600d57fcee4f94.jpg\n",
      "ed600d57fcee4f94.json\n",
      "ff47e649b23f446d.jpg\n",
      "ff47e649b23f446d.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar | tar tf - | sed 10q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from torchvision import transforms\n",
    "import webdataset as wds\n",
    "from itertools import islice\n",
    "\n",
    "url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"\n",
    "url = f\"pipe:curl -L -s {url} || true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's use the `webdataset.Dataset` class to illustrate how the `webdataset` library works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__key__ 'e39871fd9fd74f55'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01\n",
      "json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli\n",
      "\n",
      "__key__ 'f18b91585c4d3f3e'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\n",
      "json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti\n",
      "\n",
      "__key__ 'ede6e66b2fb59aab'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\n",
      "json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = wds.WebDataset(url)\n",
    "\n",
    "for sample in islice(dataset, 0, 3):\n",
    "    for key, value in sample.items():\n",
    "        print(key, repr(value)[:50])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are common processing stages you can add to a dataset to make it a drop-in replacement for any existing dataset. For convenience, common operations are available through a \"fluent\" interface (as chained method calls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 1024, 3) float32 <class 'list'>\n",
      "(768, 768, 3) float32 <class 'list'>\n",
      "(1024, 1024, 3) float32 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "dataset = (\n",
    "    wds.WebDataset(url)\n",
    "    .shuffle(100)\n",
    "    .decode(\"rgb\")\n",
    "    .to_tuple(\"jpg;png\", \"json\")\n",
    ")\n",
    "\n",
    "for image, data in islice(dataset, 0, 3):\n",
    "    print(image.shape, image.dtype, type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `webdataset.Dataset` class has some common operations:\n",
    "\n",
    "- `shuffle(n)`: shuffle the dataset with a buffer of size `n`; also shuffles shards (see below)\n",
    "- `decode(decoder, ...)`: automatically decode files (most commonly, you can just specify `\"pil\"`, `\"rgb\"`, `\"rgb8\"`, `\"rgbtorch\"`, etc.)\n",
    "- `rename(new=\"old1;old2\", ...)`: rename fields\n",
    "- `map(f)`: apply `f` to each sample\n",
    "- `map_dict(key=f, ...)`: apply `f` to its corresponding key\n",
    "- `map_tuple(f, g, ...)`: apply `f`, `g`, etc. to their corresponding values in the tuple\n",
    "- `pipe(f)`: `f` should be a function that takes an iterator and returns a new iterator\n",
    "\n",
    "Stages commonly take a `handler=` argument, which is a function that gets called when there is an exception; you can write whatever function you want, but common functions are:\n",
    "\n",
    "- `webdataset.ignore_and_stop`\n",
    "- `webdataset.ignore_and_continue`\n",
    "- `webdataset.warn_and_stop`\n",
    "- `webdataset.warn_and_continue`\n",
    "- `webdataset.reraise_exception`\n",
    "\n",
    "\n",
    "Here is an example that uses `torchvision` data augmentation the same way you might use it with a `FileDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.float32 <class 'list'>\n",
      "torch.Size([3, 224, 224]) torch.float32 <class 'list'>\n",
      "torch.Size([3, 224, 224]) torch.float32 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preproc = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "dataset = (\n",
    "    wds.WebDataset(url)\n",
    "    .shuffle(100)\n",
    "    .decode(\"pil\")\n",
    "    .to_tuple(\"jpg;png\", \"json\")\n",
    "    .map_tuple(preproc, identity)\n",
    ")\n",
    "\n",
    "for image, data in islice(dataset, 0, 3):\n",
    "    print(image.shape, image.dtype, type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full PyTorch ImageNet sample code converted to WebDataset at [tmbdev/pytorch-imagenet-wds](http://github.com/tmbdev/pytorch-imagenet-wds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharding and Parallel I/O\n",
    "\n",
    "In order to be able to shuffle data better and to process and load data in parallel, it is a good idea to shard it; that is, to split up the dataset into several `.tar` files.\n",
    "\n",
    "WebDataset uses standard UNIX brace notation for sharded dataset. For example, the OpenImages dataset consists of 554 shards, each containing about 1 Gbyte of images. You can open the entire dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\"\n",
    "url = f\"pipe:curl -L -s {url} || true\"\n",
    "dataset = (\n",
    "    wds.WebDataset(url)\n",
    "    .shuffle(100)\n",
    "    .decode(\"pil\")\n",
    "    .to_tuple(\"jpg;png\", \"json\")\n",
    "    .map_tuple(preproc, identity)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used with a standard Torch `DataLoader`, this will now perform parallel I/O and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=16)\n",
    "images, targets = next(iter(dataloader))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Size([16, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way of using `IterableDataset` with `DataLoader` is to do the batching explicitly in the `Dataset`. In addition, you need to set a nominal length for the `Dataset` in order to avoid warnings from `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\"\n",
    "url = f\"pipe:curl -L -s {url} || true\"\n",
    "bs = 20\n",
    "\n",
    "dataset = (\n",
    "    wds.WebDataset(url, length=int(1e9) // bs)\n",
    "    .shuffle(100)\n",
    "    .decode(\"pil\")\n",
    "    .to_tuple(\"jpg;png\", \"json\")\n",
    "    .map_tuple(preproc, identity)\n",
    "    .batched(20)\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=None)\n",
    "images, targets = next(iter(dataloader))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ResizedDataset` is also helpful for connecting iterable datasets to `DataLoader`: it lets you set both a nominal and an actual epoch size; it will repeatedly iterate through the entire dataset and return data in chunks with the given epoch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Decoding\n",
    "\n",
    "(to be written)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a WebDataset\n",
    "\n",
    "Since WebDatasets are just regular tar files, you can usually create them by just using the `tar` command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Bash"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "$ tar --sort=name -cf dataset.tar dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset has some other directory layout, you can either rearrange the files on disk, or you can use `tar --transform` to get the right kinds of names in your tar file.\n",
    "\n",
    "You can also create a WebDataset with library functions in this library:\n",
    "\n",
    "- `webdataset.TarWriter` takes dictionaries containing key value pairs and writes them to disk\n",
    "- `webdataset.ShardWriter` takes dictionaries containing key value pairs and writes them to disk as a series of shards\n",
    "\n",
    "Here is a quick way of converting an existing dataset into a WebDataset; this will store all tensors as Python pickles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "sink = wds.TarWriter(\"dest.tar\")\n",
    "dataset = open_my_dataset()\n",
    "for index, (input, output) in dataset:\n",
    "    sink.write({\n",
    "        \"__key__\": \"sample%06d\" % index,\n",
    "        \"input.pyd\": input,\n",
    "        \"output.pyd\": output,\n",
    "    })\n",
    "sink.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing data as Python pickles allows most common Python datatypes to be stored, it is lossless, and the format is fast to decode.\n",
    "However, it is uncompressed and cannot be read by non-Python programs. It's often better to choose other storage formats, e.g.,\n",
    "taking advantage of common image compression formats.\n",
    "\n",
    "If you know that the input is an image and the output is an integer class, you can also write something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "sink = wds.TarWriter(\"dest.tar\")\n",
    "dataset = open_my_dataset()\n",
    "for index, (input, output) in dataset:\n",
    "    assert input.ndim == 3 and input.shape[2] == 3\n",
    "    assert input.dtype = np.float32 and np.amin(input) >= 0 and np.amax(input) <= 1\n",
    "    assert type(output) == int\n",
    "    sink.write({\n",
    "        \"__key__\": \"sample%06d\" % index,\n",
    "        \"input.jpg\": input,\n",
    "        \"output.cls\": output,\n",
    "    })\n",
    "sink.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `assert` statements in that loop are not necessary, but they document and illustrate the expectations for this\n",
    "particular dataset. Generally, the \".jpg\" encoder can actually encode a wide variety of array types as images. The\n",
    "\".cls\" encoder always requires an integer for encoding.\n",
    "\n",
    "Here is how you can use `TarWriter` for writing a dataset without using an encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "sink = wds.TarWriter(\"dest.tar\", encoder=False)\n",
    "for basename in basenames:\n",
    "    with open(f\"{basename}.png\", \"rb\") as stream):\n",
    "        image = stream.read()\n",
    "    cls = lookup_cls(basename)\n",
    "    sample = {\n",
    "        \"__key__\": basename,\n",
    "        \"input.png\": image,\n",
    "        \"target.cls\": cls\n",
    "    }\n",
    "    sink.write(sample)\n",
    "sink.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no encoder is used, if you want to be able to read this data with the default decoder, `image` must contain a byte string corresponding to a PNG image (as indicated by the \".png\" extension on its dictionary key), and `cls` must contain an integer encoded in ASCII (as indicated by the \".cls\" extension on its dictionary key).\n",
    "\n",
    "# Writing Filters and Offline Augmentation\n",
    "\n",
    "Webdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class(data):\n",
    "    # mock implementation\n",
    "    return 0\n",
    "\n",
    "def augment_wds(input, output, maxcount=999999999):\n",
    "    src = (\n",
    "        wds.Dataset(input)\n",
    "        .decode(\"pil\")\n",
    "        .to_tuple(\"__key__\", \"jpg;png\", \"json\")\n",
    "        .map_tuple(identity, preproc, identity)\n",
    "    )\n",
    "    with wds.TarWriter(output) as dst:\n",
    "        for key, image, data in islice(src, 0, maxcount):\n",
    "            print(key)\n",
    "            image = image.numpy().transpose(1, 2, 0)\n",
    "            image -= amin(image)\n",
    "            image /= amax(image)\n",
    "            sample = {\n",
    "                \"__key__\": key,\n",
    "                \"png\": image,\n",
    "                \"cls\": extract_class(data)\n",
    "            }\n",
    "            dst.write(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the augmentation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"\n",
    "url = f\"pipe:curl -L -s {url} || true\"\n",
    "augment_wds(url, \"_temp.tar\", maxcount=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e39871fd9fd74f55\n",
    "f18b91585c4d3f3e\n",
    "ede6e66b2fb59aab\n",
    "ed600d57fcee4f94\n",
    "ff47e649b23f446d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that things worked correctly, let's look at the output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash\n",
    "tar tf _temp.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e39871fd9fd74f55.cls\n",
    "e39871fd9fd74f55.png\n",
    "f18b91585c4d3f3e.cls\n",
    "f18b91585c4d3f3e.png\n",
    "ede6e66b2fb59aab.cls\n",
    "ede6e66b2fb59aab.png\n",
    "ed600d57fcee4f94.cls\n",
    "ed600d57fcee4f94.png\n",
    "ff47e649b23f446d.cls\n",
    "ff47e649b23f446d.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system.\n",
    "\n",
    "For example, using Dask, you could process all 554 shards in parallel using code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "shards = braceexpand.braceexpand(\"{000000..000554}\")\n",
    "inputs = [f\"gs://bucket/openimages-{shard}.tar\" for shard in shards]\n",
    "outputs = [f\"gs://bucket2/openimages-augmented-{shard}.tar\" for shard in shards]\n",
    "results = [dask.delayed(augment_wds)(args) for args in zip(inputs, outputs)]\n",
    "dask.compute(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker.\n",
    "\n",
    "For very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes `Job` template, or using a workflow engine like Argo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebDataset = Composition of Iterable Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `webdataset` library is really just a collection of useful pipeline stages that you can put together in many different ways, and the `webdataset.Dataset` class is just a convenient wrapper for building up a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Dataset` defined like:\n",
    "    \n",
    "```Python\n",
    "wds.Dataset(url).shuffle(100).decode(\"pil\")\n",
    "```\n",
    "\n",
    "is actually a composition of multiple classes:\n",
    "\n",
    "```Python\n",
    "dataset = DecodeProcessor(ShuffleProcessor(Dataset(url), 100))\n",
    "```\n",
    "\n",
    "or, equivalently:\n",
    "\n",
    "```Python\n",
    "raw = Dataset(url)\n",
    "shuffled = ShuffleProcessor(raw, 100)\n",
    "dataset = DecodeProcessor(shuffled)\n",
    "```\n",
    "\n",
    "Here, `Dataset`, `ShuffleProcessor`, and `DecodeProcessor` are just `IterableDataset` implementations that take other `IterableDataset` implementations as a source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can implement these processing classes directly by subclassing `IterableDataset`, but that's kind of repetitive too. In fact, at the heart of almost every `IterableDataset` is a function that maps an input iterator to an output iterator. Here is a simple processor that augments samples by adding noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sample(source, noise=0.01):\n",
    "    for inputs, targets in source:\n",
    "        inputs += torch.randn_like(inputs) * noise\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can turn this into an `IterableDataset` by writing a wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentSampleProcessor(IterableDataset):\n",
    "    \n",
    "    def __init__(self, source_dataset, noise=0.01):\n",
    "        self.source_dataset = source_dataset\n",
    "        self.noise = noise\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return augment_sample(iter(source_dataset), noise=self.noise)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this gets kind of tedious to write over and over again, the `webdataset` library just provides a wrapper that automates this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AugmentSampleProcessor = wds.Processor(augment_sample, noise=0.01)\n",
    "isinstance(AugmentSampleProcessor, IterableDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a further shorthand, you can actually create processing chains with the `.then` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    wds.WebDataset(url)\n",
    "    .then(wds.shuffle, 100)\n",
    "    .then(wds.decode, \"pil\")\n",
    "    .then(augment_sample, noise=0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can compose a dataset using the `.compose` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    wds.WebDataset(url)\n",
    "    .then(wds.shuffle, 100)\n",
    "    .then(wds.decode, \"pil\")\n",
    "    .compose(AugmentSampleProcessor)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, `webdataset.WebDataset` is just a shorthand for a simple processing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    wds.ShardList(url)\n",
    "    .then(wds.url_opener)\n",
    "    .then(wds.tar_file_expander)\n",
    "    .then(wds.group_by_keys)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__key__', 'jpg', 'json'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset)).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stages in this processing pipeline are:\n",
    "\n",
    "- `ShardList` generates an iterator of URLs; you may need to modify this to generate different shard subsets for multinode computation\n",
    "- `url_opener` opens each URL in turn and returns a bytestream\n",
    "- `tarfile_expander` takes each bytestream in its input and generates an iterator over the tar files in that stream\n",
    "- `group_by_keys` groups files together into training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to those tar-file related processing functions, there are additional processing functions for common dataset operations:\n",
    "\n",
    "- `info` -- provide info about training samples from an IterableDataset\n",
    "- `shuffle` -- shuffle the IterableDataset\n",
    "- `select` -- select samples from the IterableDataset\n",
    "- `decode` -- apply decoding functions to each key-value pair in a sample\n",
    "- `map` -- apply a mapping function to each sample\n",
    "- `rename` -- rename the keys in key-value pairs in a sample\n",
    "- `map_dict` -- apply functions to selected key-value pairs\n",
    "- `to_tuple` -- convert dictionary-based samples to tuple-based samples\n",
    "- `map_tuple` -- apply functions to the elements of each tuple-based sample\n",
    "- `batched` -- batch samples together using a collation functions\n",
    "- `unbatched` -- unbatch batched samples\n",
    "\n",
    "Use `help(wds.batched)` etc. to get more information on each function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you prefer `WebDataset` or `Dataset` is a matter of style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Shards across Nodes and Workers\n",
    "\n",
    "Unlike traditional PyTorch `Dataset` instances, `WebDataset` splits data across nodes at the shard level. This functionality is handled inside the `ShardList` class. To customize splitting data across nodes, you can either write your own `ShardList` function, or you can give the `ShardList` class a `splitter=` argument when you create it. The `splitter` function should select a subset of shards from the given list of shards based on the current node and worker ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources\n",
    "\n",
    "The `ShardList` class takes either a string or a list of URLs as an argument. If it is given a string, the string is expanded using the `braceexpand` library. So, the following are equivalent:\n",
    "\n",
    "```Python\n",
    "ShardList(\"dataset-{000..001}.tar\")\n",
    "ShardList([\"dataset-000.tar\", \"dataset-001.tar\"])\n",
    "```\n",
    "\n",
    "The url strings in a shard list are handled by default by the `webdataset.url_opener` filter. It recognizes three simple kinds of strings: \"-\", \"/path/to/file\", and \"pipe:command\":\n",
    "\n",
    "- the string \"-\", referring to stdin\n",
    "- a UNIX path, opened as a regular file\n",
    "- a URL-like string with the schema \"pipe:\"; such URLs are opened with `subprocess.Popen`. For example:\n",
    "    - `pipe:curl -s -L http://server/file` accesses a file via HTTP\n",
    "    - `pipe:gsutil cat gs://bucket/file` accesses a file on GCS\n",
    "    - `pipe:az cp --container bucket --name file --file /dev/stdout` accesses a file on Azure\n",
    "    - `pipe:ssh host cat file` accesses a file via `ssh`\n",
    "\n",
    "It might seem at first glance to be \"more efficient\" to use built-in Python libraries for accessing object stores rather than subprocesses, but efficient object store access from Python really requires spawning a separate process anyway, so this approach to accessing object stores is not only convenient, it also is as efficient as we can make it in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Libraries and Software\n",
    "\n",
    "The [AIStore](http://github.com/NVIDIA/aistore) server provides an efficient backend for WebDataset; it functions like a combination of web server, content distribution network, P2P network, and distributed file system. Together, AIStore and WebDataset can serve input data from rotational drives distributed across many servers at the speed of local SSDs to many GPUs, at a fraction of the cost. We can easily achieve hundreds of MBytes/s of I/O per GPU even in large, distributed training jobs.\n",
    "\n",
    "The [tarproc](http://github.com/tmbdev/tarproc) utilities provide command line manipulation and processing of webdatasets and other tar files, including splitting, concatenation, and `xargs`-like functionality.\n",
    "\n",
    "The [tensorcom](http://github.com/tmbdev/tensorcom/) library provides fast three-tiered I/O; it can be inserted between [AIStore](http://github.com/NVIDIA/aistore) and [WebDataset](http://github.com/tmbdev/webdataset) to permit distributed data augmentation and I/O. It is particularly useful when data augmentation requires more CPU than the GPU server has available.\n",
    "\n",
    "You can find the full PyTorch ImageNet sample code converted to WebDataset at [tmbdev/pytorch-imagenet-wds](http://github.com/tmbdev/pytorch-imagenet-wds)\n",
    "\n",
    "# TODO\n",
    "\n",
    "- refactor `autodecode.py`; allow for cascade of decoders\n",
    "- add single file decompression to `autodecode.py`\n",
    "- integrate Tensorcom library\n",
    "- key rewriting / custom key grouping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
