{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Test](https://github.com/tmbdev/webdataset/workflows/Test/badge.svg)](https://github.com/tmbdev/webdataset/actions?query=workflow%3ATest)\n",
    "[![DeepSource](https://static.deepsource.io/deepsource-badge-light-mini.svg)](https://deepsource.io/gh/tmbdev/webdataset/?ref=repository-badge)\n",
    "\n",
    "# WebDataset\n",
    "\n",
    "WebDataset is a PyTorch Dataset (IterableDataset) implementation providing\n",
    "efficient access to datasets stored in POSIX tar archives and uses only sequential/streaming\n",
    "data access. This brings substantial performance advantage in many compute environments, and it\n",
    "is essential for very large scale training.\n",
    "\n",
    "While WebDataset scales to very large problems, it also works well with smaller datasets and simplifies\n",
    "creation, management, and distribution of training data for deep learning.\n",
    "\n",
    "WebDataset implements standard PyTorch `IterableDataset` interface and works with the PyTorch `DataLoader`.\n",
    "Access to datasets is as simple as:\n",
    "\n",
    "```Python\n",
    "import webdataset as wds\n",
    "\n",
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(url),\n",
    "    wds.tarfile_to_samples(),\n",
    "    wds.shuffle(100),\n",
    "    wds.decode(\"torchrgb\"),\n",
    "    wds.to_tuple(\"jpg;png\", \"json\"),\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=16)\n",
    "\n",
    "for inputs, outputs in dataloader:\n",
    "    ...\n",
    "```\n",
    "\n",
    "In that code snippet, `url` can refer to a local file, an HTTP server, a cloud storage object, an object\n",
    "on an object store, or even the output of arbitrary command pipelines.\n",
    "\n",
    "WebDataset fulfills a similar function to Tensorflow's TFRecord/tf.Example\n",
    "classes, but it is much easier to adopt because it does not actually\n",
    "require any kind of data conversion: data is stored in exactly the same\n",
    "format inside tar files as it is on disk, and all preprocessing and data\n",
    "augmentation code remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and Documentation\n",
    "\n",
    "    $ pip install webdataset\n",
    "\n",
    "For the Github version:\n",
    "\n",
    "    $ pip install git+https://github.com/tmbdev/webdataset.git\n",
    "\n",
    "Documentation: [ReadTheDocs](http://webdataset.readthedocs.io)\n",
    "\n",
    "Examples:\n",
    "\n",
    "- [loading videos](https://github.com/tmbdev/webdataset/blob/master/docs/video-loading-example.ipynb)\n",
    "- [splitting raw videos into clips for training](https://github.com/tmbdev/webdataset/blob/master/docs/ytsamples-split.ipynb)\n",
    "- [converting the Falling Things dataset](https://github.com/tmbdev/webdataset/blob/master/docs/falling-things-make-shards.ipynb)\n",
    "\n",
    "# Dependencies\n",
    "\n",
    "The WebDataset library only requires PyTorch, NumPy, and a small library called `braceexpand`.\n",
    "\n",
    "WebDataset loads a few additional libraries dynamically only when they are actually needed and only in the decoder:\n",
    "\n",
    "- PIL/Pillow for image decoding\n",
    "- `torchvision`, `torchvideo`, `torchaudio` for image/video/audio decoding\n",
    "- `msgpack` for MessagePack decoding\n",
    "- the `curl` command line tool for accessing HTTP servers\n",
    "- the Google/Amazon/Azure command line tools for accessing cloud storage buckets\n",
    "\n",
    "Loading of one of these libraries is triggered by configuring a decoder that attempts to decode content in the given format and encountering a file in that format during decoding. (Eventually, the torch... dependencies will be refactored into those libraries.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Videos\n",
    "\n",
    "Here are some videos talking about WebDataset and large scale deep learning:\n",
    "\n",
    "- [Introduction to Large Scale Deep Learning](https://www.youtube.com/watch?v=kNuA2wflygM)\n",
    "- [Loading Training Data with WebDataset](https://www.youtube.com/watch?v=mTv_ePYeBhs)\n",
    "- [Creating Datasets in WebDataset Format](https://www.youtube.com/watch?v=v_PacO-3OGQ)\n",
    "- [Tools for Working with Large Datasets](https://www.youtube.com/watch?v=kIv8zDpRUec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using WebDataset\n",
    "\n",
    "WebDataset reads dataset that are stored as tar files, with the simple convention that files that belong together and make up a training sample share the same basename. WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e39871fd9fd74f55.jpg\n",
      "e39871fd9fd74f55.json\n",
      "f18b91585c4d3f3e.jpg\n",
      "f18b91585c4d3f3e.json\n",
      "ede6e66b2fb59aab.jpg\n",
      "ede6e66b2fb59aab.json\n",
      "ed600d57fcee4f94.jpg\n",
      "ed600d57fcee4f94.json\n",
      "ff47e649b23f446d.jpg\n",
      "ff47e649b23f446d.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar | tar tf - | sed 10q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from torchvision import transforms\n",
    "import webdataset as wds\n",
    "from itertools import islice\n",
    "\n",
    "url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\"\n",
    "url = f\"pipe:curl -L -s {url} || true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's use the `webdataset.Dataset` class to illustrate how the `webdataset` library works.\n",
    "\n",
    "We start off with a generator for a list of file names or data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'pipe:curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar || true'},\n",
       " {'url': 'pipe:curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000001.tar || true'},\n",
       " {'url': 'pipe:curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000002.tar || true'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = wds.SimpleShardList(url)\n",
    "list(islice(dataset, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of those sources refers to a tar file containing a list of samples. The `tarfile_to_samples` filter opens the files, read their contents, and turns the samples into dictionaries representing the data contained in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__key__ 'e39871fd9fd74f55'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01\n",
      "json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli\n",
      "\n",
      "__key__ 'f18b91585c4d3f3e'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\n",
      "json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti\n",
      "\n",
      "__key__ 'ede6e66b2fb59aab'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\n",
      "json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(url),\n",
    "    wds.tarfile_to_samples(),\n",
    ")\n",
    "\n",
    "for sample in islice(dataset, 0, 3):\n",
    "    for key, value in sample.items():\n",
    "        print(key, repr(value)[:50])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are common processing stages you can add to a dataset to make it a drop-in replacement for any existing dataset in your PyTorch code.\n",
    "\n",
    "Generally, we want to decode images and convert the dictionaries to tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 768, 3) float32 <class 'numpy.ndarray'>\n",
      "(768, 768, 3) float32 <class 'numpy.ndarray'>\n",
      "(768, 1024, 3) float32 <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(url),\n",
    "    wds.tarfile_to_samples(),\n",
    "    wds.decode(\"rgb\"),\n",
    "    wds.to_tuple(\"jpg;png\", \"json\")\n",
    ")\n",
    "\n",
    "for image, data in islice(dataset, 0, 3):\n",
    "    print(image.shape, image.dtype, type(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `webdataset` library has some common operations:\n",
    "\n",
    "- `shuffle(n)`: shuffle the dataset with a buffer of size `n`; also shuffles shards (see below)\n",
    "- `decode(decoder, ...)`: automatically decode files (most commonly, you can just specify `\"pil\"`, `\"rgb\"`, `\"rgb8\"`, `\"rgbtorch\"`, etc.)\n",
    "- `rename(new=\"old1;old2\", ...)`: rename fields\n",
    "- `map(f)`: apply `f` to each sample\n",
    "- `map_dict(key=f, ...)`: apply `f` to its corresponding key\n",
    "- `map_tuple(f, g, ...)`: apply `f`, `g`, etc. to their corresponding values in the tuple\n",
    "- `pipe(f)`: `f` should be a function that takes an iterator and returns a new iterator\n",
    "\n",
    "Stages commonly take a `handler=` argument, which is a function that gets called when there is an exception; you can write whatever function you want, but common functions are:\n",
    "\n",
    "- `webdataset.ignore_and_stop`\n",
    "- `webdataset.ignore_and_continue`\n",
    "- `webdataset.warn_and_stop`\n",
    "- `webdataset.warn_and_continue`\n",
    "- `webdataset.reraise_exception`\n",
    "\n",
    "\n",
    "Here is an example that uses `torchvision` data augmentation the same way you might use it with a `FileDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.float32 <class 'list'>\n",
      "torch.Size([3, 224, 224]) torch.float32 <class 'list'>\n",
      "torch.Size([3, 224, 224]) torch.float32 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preproc = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(url),\n",
    "    wds.tarfile_to_samples(),\n",
    "    wds.decode(\"pil\"),\n",
    "    wds.to_tuple(\"jpg;png\", \"json\"),\n",
    "    wds.map_tuple(preproc, None)\n",
    ")\n",
    "\n",
    "for image, data in islice(dataset, 0, 3):\n",
    "    print(image.shape, image.dtype, type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it Works\n",
    "\n",
    "WebDataset is powerful and it may look complex from the outside, but its structure is quite simple: most of\n",
    "the code consists of functions mapping an input iterator to an output iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(source, noise=0.01):\n",
    "    for inputs, targets in source:\n",
    "        inputs = inputs + noise * torch.randn_like(inputs)\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o write new processing stages, a function like this is all you ever have to write. \n",
    "The rest is really bookkeeping: we need to be able\n",
    "to repeatedly invoke functions like this for every epoch, and we need to chain them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 683, 1024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(url),\n",
    "    wds.tarfile_to_samples(),\n",
    "    wds.shuffle(100),\n",
    "    wds.decode(\"torchrgb\"),\n",
    "    wds.to_tuple(\"jpg;png\", \"json\"),\n",
    "    add_noise\n",
    ")\n",
    "\n",
    "image, cls = next(iter(dataset))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `functools.partial` function if you want to pass parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(url),\n",
    "    wds.tarfile_to_samples(),\n",
    "    wds.shuffle(100),\n",
    "    wds.decode(\"torchrgb\"),\n",
    "    wds.to_tuple(\"jpg;png\", \"json\"),\n",
    "    partial(add_noise, noise=0.1)\n",
    ")\n",
    "\n",
    "image, cls = next(iter(dataset))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharding and Parallel I/O\n",
    "\n",
    "WebDataset datasets are usually split into many shards; this is both to achieve parallel I/O and to shuffle data.\n",
    "\n",
    "Sets of shards can be given as a list of files, or they can be written using the brace notation, as in `openimages-train-{000000..000554}.tar`.\n",
    "\n",
    "For example, the OpenImages dataset consists of 554 shards, each containing about 1 Gbyte of images. The data pipeline will iterate through each of these shards in turn.\n",
    "\n",
    "Since loading is often carried out in multiple subprocesses, we are inserting the `wds.split_by_worker` function, which will use PyTorch APIs to determine which worker it is running in and assign a subset of shards to each worker.\n",
    "\n",
    "In addition, since we don't want to train on those shards in the same order during each epoch, we insert a `shuffle` before the `tarfile_to_samples` in order to shuffle the shards as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024, 768]) [{'ImageID': 'f0624d7707b1f46c', 'Source': 'xclick\n"
     ]
    }
   ],
   "source": [
    "url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\"\n",
    "url = f\"pipe:curl -L -s {url} || true\"\n",
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(url),\n",
    "    wds.split_by_worker,\n",
    "    wds.shuffle(50),\n",
    "    wds.tarfile_to_samples(),\n",
    "    wds.shuffle(100),\n",
    "    wds.decode(\"torchrgb\"),\n",
    "    wds.to_tuple(\"jpg;png\", \"json\"),\n",
    ")\n",
    "x, y = next(iter(dataset))\n",
    "print(x.shape, str(y)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch recommends that when using `IterableDataset` with `DataLoader` to carry out the the batching explicitly in the `IterableDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3, 224, 224])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\"\n",
    "url = f\"pipe:curl -L -s {url} || true\"\n",
    "bs = 20\n",
    "\n",
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(url),\n",
    "    wds.split_by_worker,\n",
    "    wds.shuffle(50),\n",
    "    wds.tarfile_to_samples(),\n",
    "    wds.shuffle(100),\n",
    "    wds.decode(\"pil\"),\n",
    "    wds.to_tuple(\"jpg;png\", \"json\"),\n",
    "    wds.map_tuple(preproc, None),\n",
    "    wds.batched(bs),\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=None)\n",
    "images, targets = next(iter(dataloader))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to mix up samples from different workers anyway, you can rebatch the data after the `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3, 224, 224])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = wds.DataPipeline(\n",
    "    torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=None),\n",
    "    wds.unbatched(),\n",
    "    wds.shuffle(500),\n",
    "    wds.batched(bs)\n",
    ")\n",
    "images, targets = next(iter(dataloader))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataPipeline` class also lets you change the epoch size of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Decoding\n",
    "\n",
    "Data decoding is a special kind of transformations of samples. You could simply write a decoding function like this:\n",
    "\n",
    "```Python\n",
    "def my_sample_decoder(sample):\n",
    "    result = dict(__key__=sample[\"__key__\"])\n",
    "    for key, value in sample.items():\n",
    "        if key == \"png\" or key.endswith(\".png\"):\n",
    "            result[key] = mageio.imread(io.BytesIO(value))\n",
    "        elif ...:\n",
    "            ...\n",
    "    return result\n",
    "\n",
    "dataset = wds.Processor(wds.map, my_sample_decoder)(dataset)\n",
    "```\n",
    "\n",
    "This gets tedious, though, and it also unnecessarily hardcodes the sample's keys into the processing pipeline. To help with this, there is a helper class that simplifies this kind of code. The primary use of `Decoder` is for decoding compressed image, video, and audio formats, as well as unzipping `.gz` files.\n",
    "\n",
    "Here is an example of automatically decoding `.png` images with `imread` and using the default `torch_video` and `torch_audio` decoders for video and audio:\n",
    "\n",
    "```Python\n",
    "def my_png_decoder(key, value):\n",
    "    if not key.endswith(\".png\"):\n",
    "        return None\n",
    "    assert isinstance(value, bytes)\n",
    "    return imageio.imread(io.BytesIO(value))\n",
    "\n",
    "dataset = wds.Decoder(my_png_decoder, wds.torch_video, wds.torch_audio)(dataset)\n",
    "```\n",
    "\n",
    "You can use whatever criteria you like for deciding how to decode values in samples. When used with standard `WebDataset` format files, the keys are the full extensions of the file names inside a `.tar` file. For consistency, it's recommended that you primarily rely on the extensions (e.g., `.png`, `.mp4`) to decide which decoders to use. There is a special helper function that simplifies this:\n",
    "\n",
    "```Python\n",
    "def my_decoder(value):\n",
    "    return imageio.imread(io.BytesIO(value))\n",
    "    \n",
    "dataset = wds.Decoder(wds.handle_extension(\".png\", my_decoder))(dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinode Training\n",
    "\n",
    "For multinode training, you have two options:\n",
    "\n",
    "1. use shard resampling\n",
    "2. split the shards among both nodes and workers\n",
    "\n",
    "Using (2) seems like the more natural approach and is exactly analogous to training on single nodes, but it is tricky to get right, since different nodes may receive different numbers of samples, something that PyTorch's `DistributedDataParallel` cannot handle.\n",
    "\n",
    "In general, using shard resampling is the simpler approach and works very well for large scale datasets and training jobs. The `ResampledShards` class by default initializes itself nondeterministically on each worker, so each worker samples a different set of shards. Alternatively, you can initialize it with a per-worker seed and it will step through a deterministic sequence of shards that varies every epoch.\n",
    "\n",
    "Since resampling generates an infinite stream of batches, you need to explicitly specify an epoch length if your training framework relies on epochs; the `with_epoch` method takes care of this. The `with_epoch` method is also available on the `WebLoader` wrapper for `torch.utils.data.DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, torch.Size([8, 3, 224, 224]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.ResampledShards(url),\n",
    "    wds.tarfile_to_samples(),\n",
    "    wds.shuffle(100),\n",
    "    wds.decode(\"pil\"),\n",
    "    wds.to_tuple(\"jpg;png\", \"json\"),\n",
    "    wds.map_tuple(preproc, None),\n",
    "    wds.batched(8),\n",
    ")\n",
    "dataloader = wds.WebLoader(dataset, num_workers=7, batch_size=None).with_epoch(20)\n",
    "one_epoch = list(iter(dataloader))\n",
    "len(one_epoch), one_epoch[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluid Interface (Backwards Compatibility)\n",
    "\n",
    "The library provides a backwards compatible fluid interface called `WebDataset`. It just uses a slightly different (and more concise) syntax for constructing a `DataPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 224, 224])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = (\n",
    "    wds.WebDataset(url, shardshuffle=100)\n",
    "    .shuffle(1000)\n",
    "    .decode(\"pil\")\n",
    "    .to_tuple(\"jpg;png\", \"json\")\n",
    "    .map_tuple(preproc, None)\n",
    "    .batched(8)\n",
    ")\n",
    "sample = next(iter(dataset))\n",
    "sample[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of the fluid interface is discouraged because it hides many options for the configuration of shard selection, splitting, and shuffling inside an ever growing argument list for `WebDataset`; the pipeline interface makes shard processing and decoding more explicit and straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Smaller\" Datasets and Desktop Computing\n",
    "\n",
    "WebDataset is an ideal solution for training on petascale datasets kept on high performance distributed data stores like AIStore, AWS/S3, and Google Cloud. Compared to data center GPU servers, desktop machines have much slower network connections, but training jobs on desktop machines often also use much smaller datasets. WebDataset also is very useful for such smaller datasets, and it can easily be used for developing and testing on small datasets and then scaling up to large datasets by simply using more shards.\n",
    "\n",
    "\n",
    "Here are different usage scenarios:\n",
    "\n",
    "- **desktop deep learning, smaller datasets**\n",
    "    - copy all shards to local disk manually\n",
    "    - use automatic shard caching\n",
    "- **prototyping, development, testing of jobs for large scale training**\n",
    "    - copy a small subset of shards to local disk\n",
    "    - use automatic shard caching with a small subrange of shards\n",
    "    - use DBCache sample caching\n",
    "- **cloud training against cloud buckets**\n",
    "    - use WebDataset directly with remote URLs\n",
    "- **on premises training with high performance store (e.g., AIStore) and fast networks**\n",
    "    - use WebDataset directly with remote URLs\n",
    "- **on premises training with slower object stores and/or slower networks**\n",
    "    - use automatic shard caching or DBCache\n",
    "- **training with IterableDataset sources other than WebDataset**\n",
    "    - use DBCache\n",
    "    \n",
    "Let's look at how these different methods work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Copying of Shards\n",
    "\n",
    "Let's take the OpenImages dataset as an example; it's half a terabyte large. For development and testing, you may not want to download the entire dataset, but you may also not want to use the dataset remotely. With WebDataset, you can just download a small number of shards and use them during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar > /tmp/openimages-train-000000.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"/tmp/openimages-train-000000.tar\"),\n",
    "    wds.tarfile_to_samples(),\n",
    ")\n",
    "repr(next(iter(dataset)))[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the WebDataset class works the same way on local files as it does on remote files. Furthermore, unlike other kinds of dataset formats and archive formats, downloaded datasets are immediately useful and don't need to be unpacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a WebDataset\n",
    "\n",
    "## Using `tar`\n",
    "\n",
    "Since WebDatasets are just regular tar files, you can usually create them by just using the `tar` command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with\n",
    "\n",
    "```\n",
    "$ tar --sort=name -cf dataset.tar dataset/\n",
    "```\n",
    "\n",
    "If your dataset has some other directory layout, you may need a different file name in the archive from the name on disk. You can use the `--transform` argument to GNU tar to transform file names. You can also use the `-T` argument to read the files from a text file and embed other options in that text file.\n",
    "\n",
    "## The `tarp create` Command\n",
    "\n",
    "The [`tarp`](https://github.com/tmbdev/tarp) command is a little utility for manipulating `tar` archives. Its `create` subcommand makes it particularly simple to construct tar archives from files. The `tarp create` command takes a recipe for building\n",
    "a tar archive that contains lines of the form:\n",
    "\n",
    "```\n",
    "archive-name-1 source-name-1\n",
    "archive-name-2 source-name-2\n",
    "...\n",
    "```\n",
    "\n",
    "The source name can either be a file, \"text:something\", or \"pipe:something\".\n",
    "\n",
    "## Programmatically in Python\n",
    "\n",
    "You can also create a WebDataset with library functions in this library:\n",
    "\n",
    "- `webdataset.TarWriter` takes dictionaries containing key value pairs and writes them to disk\n",
    "- `webdataset.ShardWriter` takes dictionaries containing key value pairs and writes them to disk as a series of shards\n",
    "\n",
    "Here is a quick way of converting an existing dataset into a WebDataset; this will store all tensors as Python pickles:\n",
    "\n",
    "```Python\n",
    "sink = wds.TarWriter(\"dest.tar\")\n",
    "dataset = open_my_dataset()\n",
    "for index, (input, output) in dataset:\n",
    "    sink.write({\n",
    "        \"__key__\": \"sample%06d\" % index,\n",
    "        \"input.pyd\": input,\n",
    "        \"output.pyd\": output,\n",
    "    })\n",
    "sink.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing data as Python pickles allows most common Python datatypes to be stored, it is lossless, and the format is fast to decode.\n",
    "However, it is uncompressed and cannot be read by non-Python programs. It's often better to choose other storage formats, e.g.,\n",
    "taking advantage of common image compression formats.\n",
    "\n",
    "If you know that the input is an image and the output is an integer class, you can also write something like this:\n",
    "\n",
    "```Python\n",
    "sink = wds.TarWriter(\"dest.tar\")\n",
    "dataset = open_my_dataset()\n",
    "for index, (input, output) in dataset:\n",
    "    assert input.ndim == 3 and input.shape[2] == 3\n",
    "    assert input.dtype = np.float32 and np.amin(input) >= 0 and np.amax(input) <= 1\n",
    "    assert type(output) == int\n",
    "    sink.write({\n",
    "        \"__key__\": \"sample%06d\" % index,\n",
    "        \"input.jpg\": input,\n",
    "        \"output.cls\": output,\n",
    "    })\n",
    "sink.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `assert` statements in that loop are not necessary, but they document and illustrate the expectations for this\n",
    "particular dataset. Generally, the \".jpg\" encoder can actually encode a wide variety of array types as images. The\n",
    "\".cls\" encoder always requires an integer for encoding.\n",
    "\n",
    "Here is how you can use `TarWriter` for writing a dataset without using an encoder:\n",
    "\n",
    "```Python\n",
    "sink = wds.TarWriter(\"dest.tar\", encoder=False)\n",
    "for basename in basenames:\n",
    "    with open(f\"{basename}.png\", \"rb\") as stream):\n",
    "        image = stream.read()\n",
    "    cls = lookup_cls(basename)\n",
    "    sample = {\n",
    "        \"__key__\": basename,\n",
    "        \"input.png\": image,\n",
    "        \"target.cls\": cls\n",
    "    }\n",
    "    sink.write(sample)\n",
    "sink.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no encoder is used, if you want to be able to read this data with the default decoder, `image` must contain a byte string corresponding to a PNG image (as indicated by the \".png\" extension on its dictionary key), and `cls` must contain an integer encoded in ASCII (as indicated by the \".cls\" extension on its dictionary key).\n",
    "\n",
    "# Writing Filters and Offline Augmentation\n",
    "\n",
    "Webdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class(data):\n",
    "    # mock implementation\n",
    "    return 0\n",
    "\n",
    "def augment_wds(input, output, maxcount=999999999):\n",
    "    src = wds.DataPipeline(\n",
    "        wds.SimpleShardList(input),\n",
    "        wds.tarfile_to_samples(),\n",
    "        wds.decode(\"pil\"),\n",
    "        wds.to_tuple(\"__key__\", \"jpg;png\", \"json\"),\n",
    "        wds.map_tuple(None, preproc, None),\n",
    "    )\n",
    "    with wds.TarWriter(output) as dst:\n",
    "        for key, image, data in islice(src, 0, maxcount):\n",
    "            print(key)\n",
    "            image = image.numpy().transpose(1, 2, 0)\n",
    "            image -= amin(image)\n",
    "            image /= amax(image)\n",
    "            sample = {\n",
    "                \"__key__\": key,\n",
    "                \"png\": image,\n",
    "                \"cls\": extract_class(data)\n",
    "            }\n",
    "            dst.write(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the augmentation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"\n",
    "url = f\"pipe:curl -L -s {url} || true\"\n",
    "augment_wds(url, \"_temp.tar\", maxcount=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that things worked correctly, let's look at the output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar tf _temp.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system.\n",
    "\n",
    "For example, using Dask, you could process all 554 shards in parallel using code like this:\n",
    "\n",
    "```Python\n",
    "shards = braceexpand.braceexpand(\"{000000..000554}\")\n",
    "inputs = [f\"gs://bucket/openimages-{shard}.tar\" for shard in shards]\n",
    "outputs = [f\"gs://bucket2/openimages-augmented-{shard}.tar\" for shard in shards]\n",
    "results = [dask.delayed(augment_wds)(args) for args in zip(inputs, outputs)]\n",
    "dask.compute(*results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker.\n",
    "\n",
    "For very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes `Job` template, or using a workflow engine like Argo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you prefer `WebDataset` or `Dataset` is a matter of style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax for URL Sources\n",
    "\n",
    "The `SimpleShardList` and `ResampledShards` take either a string or a list of URLs as an argument. If it is given a string, the string is expanded using the `braceexpand` library. So, the following are equivalent:\n",
    "\n",
    "```Python\n",
    "ShardList(\"dataset-{000..001}.tar\")\n",
    "ShardList([\"dataset-000.tar\", \"dataset-001.tar\"])\n",
    "```\n",
    "\n",
    "The url strings in a shard list are handled by default by the `webdataset.url_opener` filter. It recognizes three simple kinds of strings: \"-\", \"/path/to/file\", and \"pipe:command\":\n",
    "\n",
    "- the string \"-\", referring to stdin\n",
    "- a UNIX path, opened as a regular file\n",
    "- a URL-like string with the schema \"pipe:\"; such URLs are opened with `subprocess.Popen`. For example:\n",
    "    - `pipe:curl -s -L http://server/file` accesses a file via HTTP\n",
    "    - `pipe:gsutil cat gs://bucket/file` accesses a file on GCS\n",
    "    - `pipe:az cp --container bucket --name file --file /dev/stdout` accesses a file on Azure\n",
    "    - `pipe:ssh host cat file` accesses a file via `ssh`\n",
    "\n",
    "It might seem at first glance to be \"more efficient\" to use built-in Python libraries for accessing object stores rather than subprocesses, but efficient object store access from Python really requires spawning a separate process anyway, so this approach to accessing object stores is not only convenient, it also is as efficient as we can make it in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Libraries and Software\n",
    "\n",
    "The [AIStore](http://github.com/NVIDIA/aistore) server provides an efficient backend for WebDataset; it functions like a combination of web server, content distribution network, P2P network, and distributed file system. Together, AIStore and WebDataset can serve input data from rotational drives distributed across many servers at the speed of local SSDs to many GPUs, at a fraction of the cost. We can easily achieve hundreds of MBytes/s of I/O per GPU even in large, distributed training jobs.\n",
    "\n",
    "The [tarproc](http://github.com/tmbdev/tarproc) utilities provide command line manipulation and processing of webdatasets and other tar files, including splitting, concatenation, and `xargs`-like functionality.\n",
    "\n",
    "The [tensorcom](http://github.com/tmbdev/tensorcom/) library provides fast three-tiered I/O; it can be inserted between [AIStore](http://github.com/NVIDIA/aistore) and [WebDataset](http://github.com/tmbdev/webdataset) to permit distributed data augmentation and I/O. It is particularly useful when data augmentation requires more CPU than the GPU server has available.\n",
    "\n",
    "You can find the full PyTorch ImageNet sample code converted to WebDataset at [tmbdev/pytorch-imagenet-wds](http://github.com/tmbdev/pytorch-imagenet-wds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
