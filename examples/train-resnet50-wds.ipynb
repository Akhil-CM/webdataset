{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import webdataset as wds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "import time\n",
    "\n",
    "def enumerate_report(seq, delta, growth=1.0):\n",
    "    last = 0\n",
    "    count = 0\n",
    "    for count, item in enumerate(seq):\n",
    "        now = time.time()\n",
    "        if now - last > delta:\n",
    "            last = now\n",
    "            yield count, item, True\n",
    "        else:\n",
    "            yield count, item, False\n",
    "        delta *= growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard TorchVision transformations.\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# The dataset returns dictionaries. This is a small function we transform it\n",
    "# with to get the augmented image and the label.\n",
    "\n",
    "def make_sample(sample, val=False):\n",
    "    image = sample[\"jpg\"]\n",
    "    label = sample[\"cls\"]\n",
    "    if val:\n",
    "        return transform_val(image), label\n",
    "    else:\n",
    "        return transform_train(image), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not running in colab, caching data locally in ./_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-0.4568, -0.4226, -0.4054,  ..., -0.4397, -0.3883, -0.4226],\n",
      "          [-0.4226, -0.4226\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# These are standard PyTorch datasets. Download is incremental into the cache.\n",
    "\n",
    "!mkdir -p ./_cache\n",
    "\n",
    "bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\"\n",
    "\n",
    "# WebDataset is designed to work without any local storage. If you have a fast\n",
    "# data server (AIStore, object store, cloud storage for cloud jobs), you don't need\n",
    "# to cache the data locally. If you are using a local filesystem, you can use\n",
    "# the cache_dir argument to cache the data locally. By default, shards will just\n",
    "# be downloaded into the cache directory with their original names. You can share\n",
    "# the cache directory between multiple jobs and between the webdataset and wids\n",
    "# libraries. You can't share the cache directory between different datasets\n",
    "# if there are name conflicts between the files of those datasets.\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    cache_dir = None\n",
    "    print(\"running on colab, streaming data directly from storage\")\n",
    "else:\n",
    "    cache_dir = \"./_cache\"\n",
    "    print(f\"not running in colab, caching data locally in {cache_dir}\")\n",
    "\n",
    "# First, we create the datasets. We use resampled=True to make training completely\n",
    "# independent of the number of workers.\n",
    "\n",
    "trainset = wds.WebDataset(bucket+\"/imagenet-train-{000000..001281}.tar\", resampled=True, cache_dir=cache_dir)\n",
    "valset = wds.WebDataset(bucket+\"/imagenet-val-{000000..000049}.tar\", cache_dir=cache_dir)\n",
    "\n",
    "# We shuffle the training data and decode the images using the PIL decoder.\n",
    "# We then decode the images and apply make_sample.\n",
    "\n",
    "trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample)\n",
    "valset = valset.decode(\"pil\").map(partial(make_sample, val=True))\n",
    "\n",
    "# For IterableDataset, the batching needs to be carried out in the dataset itself.\n",
    "\n",
    "trainset = trainset.batched(64)\n",
    "valset = valset.batched(64)\n",
    "\n",
    "# Make sure it works so far\n",
    "\n",
    "print(repr(next(iter(trainset)))[:100])\n",
    "\n",
    "# Create the dataloaders. WebLoader is just a convenient wrapper around\n",
    "# DataLoader with some useful methods.\n",
    "\n",
    "trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4)\n",
    "valloader = wds.WebLoader(valset, batch_size=None, num_workers=4)\n",
    "\n",
    "# For training, we unbatch the dataset, set the number of samples per epoch, then\n",
    "# rebatch. This reproduces approximately the original epoch size and it shuffles\n",
    "# between shards.\n",
    "\n",
    "trainloader = trainloader.unbatched().batched(64).with_epoch(1282 * 100 // 64)\n",
    "\n",
    "# Again, make sure it works so far.\n",
    "\n",
    "images, classes = next(iter(trainloader))\n",
    "print(images.shape, classes.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# The usual PyTorch model definition. We use an uninitialized ResNet50 model.\n",
    "\n",
    "model = resnet50(pretrained=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    13] loss: 4.68286 correct: 0.05168\n",
      "[1,    40] loss: 4.24923 correct: 0.07500\n",
      "[1,    67] loss: 3.94172 correct: 0.08675\n",
      "[1,    94] loss: 3.61655 correct: 0.11503\n",
      "[1,   121] loss: 3.05803 correct: 0.17438\n",
      "[1,   149] loss: 2.56617 correct: 0.25484\n",
      "[1,   175] loss: 2.17077 correct: 0.33797\n",
      "[1,   202] loss: 1.91672 correct: 0.40672\n",
      "[1,   229] loss: 1.73701 correct: 0.46203\n",
      "[1,   255] loss: 1.61650 correct: 0.49906\n",
      "[1,   282] loss: 1.47733 correct: 0.53547\n",
      "[1,   309] loss: 1.36030 correct: 0.56609\n",
      "[1,   336] loss: 1.27227 correct: 0.59391\n",
      "[1,   363] loss: 1.23422 correct: 0.60672\n",
      "[1,   389] loss: 1.20162 correct: 0.62094\n",
      "[1,   416] loss: 1.16268 correct: 0.63297\n",
      "[1,   443] loss: 1.09108 correct: 0.65938\n",
      "[1,   469] loss: 1.03599 correct: 0.67359\n",
      "[1,   495] loss: 1.00575 correct: 0.68234\n",
      "[1,   522] loss: 0.99541 correct: 0.68297\n",
      "[1,   548] loss: 0.99640 correct: 0.68188\n",
      "[1,   575] loss: 0.98761 correct: 0.68234\n",
      "[1,   602] loss: 0.97867 correct: 0.69172\n",
      "[1,   628] loss: 0.96813 correct: 0.70047\n",
      "[1,   655] loss: 0.97464 correct: 0.69672\n",
      "[1,   682] loss: 0.95794 correct: 0.69969\n",
      "[1,   709] loss: 0.92592 correct: 0.71078\n",
      "[1,   736] loss: 0.91599 correct: 0.70562\n",
      "[1,   762] loss: 0.87556 correct: 0.71813\n",
      "[1,   789] loss: 0.84415 correct: 0.72750\n",
      "[1,   816] loss: 0.81490 correct: 0.73922\n",
      "[1,   842] loss: 0.77427 correct: 0.75703\n",
      "[1,   869] loss: 0.76641 correct: 0.76578\n",
      "[1,   896] loss: 0.74264 correct: 0.77609\n",
      "[1,   923] loss: 0.75256 correct: 0.77266\n",
      "[1,   950] loss: 0.75057 correct: 0.76859\n",
      "[1,   978] loss: 0.73409 correct: 0.77375\n",
      "[1,  1005] loss: 0.72231 correct: 0.77766\n",
      "[1,  1032] loss: 0.66168 correct: 0.79891\n",
      "[1,  1060] loss: 0.63168 correct: 0.80437\n",
      "[1,  1088] loss: 0.61549 correct: 0.80641\n",
      "[1,  1115] loss: 0.62591 correct: 0.80219\n",
      "[1,  1142] loss: 0.63300 correct: 0.80250\n",
      "[1,  1169] loss: 0.63375 correct: 0.80250\n",
      "[1,  1196] loss: 0.62334 correct: 0.80703\n",
      "[1,  1222] loss: 0.59360 correct: 0.81500\n",
      "[1,  1248] loss: 0.55927 correct: 0.82422\n",
      "[1,  1275] loss: 0.51067 correct: 0.84234\n",
      "[1,  1301] loss: 0.51594 correct: 0.84172\n",
      "[1,  1328] loss: 0.49552 correct: 0.84953\n",
      "[1,  1355] loss: 0.50646 correct: 0.84406\n",
      "[1,  1382] loss: 0.52785 correct: 0.83500\n",
      "[1,  1409] loss: 0.51725 correct: 0.83688\n",
      "[1,  1435] loss: 0.55655 correct: 0.82563\n",
      "[1,  1462] loss: 0.56388 correct: 0.82828\n",
      "[1,  1489] loss: 0.56638 correct: 0.82312\n",
      "[1,  1515] loss: 0.55528 correct: 0.82453\n",
      "[1,  1542] loss: 0.52745 correct: 0.83234\n",
      "[1,  1569] loss: 0.51945 correct: 0.83437\n",
      "[1,  1595] loss: 0.52136 correct: 0.83922\n",
      "[1,  1622] loss: 0.52067 correct: 0.84391\n",
      "[1,  1648] loss: 0.51964 correct: 0.83891\n",
      "[1,  1675] loss: 0.51279 correct: 0.83750\n",
      "[1,  1702] loss: 0.48680 correct: 0.84391\n",
      "[1,  1728] loss: 0.49487 correct: 0.84094\n",
      "[1,  1755] loss: 0.48796 correct: 0.84562\n",
      "[1,  1782] loss: 0.47851 correct: 0.85125\n",
      "[1,  1808] loss: 0.49624 correct: 0.84719\n",
      "[1,  1835] loss: 0.50249 correct: 0.84562\n",
      "[1,  1862] loss: 0.51648 correct: 0.84531\n",
      "[1,  1888] loss: 0.49378 correct: 0.85000\n",
      "[1,  1914] loss: 0.48521 correct: 0.85219\n",
      "[1,  1941] loss: 0.43996 correct: 0.86500\n",
      "[1,  1967] loss: 0.44135 correct: 0.86516\n",
      "[1,  1994] loss: 0.43505 correct: 0.86484\n",
      "[2,     1] loss: 0.44590 correct: 0.86250\n",
      "[2,    28] loss: 0.48814 correct: 0.85016\n",
      "[2,    56] loss: 0.46979 correct: 0.85094\n",
      "[2,    82] loss: 0.46600 correct: 0.85266\n",
      "[2,   109] loss: 0.45225 correct: 0.85625\n",
      "[2,   136] loss: 0.43288 correct: 0.86313\n",
      "[2,   162] loss: 0.42505 correct: 0.86656\n",
      "[2,   190] loss: 0.41648 correct: 0.86766\n",
      "[2,   218] loss: 0.42081 correct: 0.86906\n",
      "[2,   246] loss: 0.41600 correct: 0.87328\n",
      "[2,   274] loss: 0.42494 correct: 0.87328\n",
      "[2,   302] loss: 0.42377 correct: 0.87484\n",
      "[2,   329] loss: 0.40249 correct: 0.87875\n",
      "[2,   358] loss: 0.41465 correct: 0.87234\n",
      "[2,   385] loss: 0.39651 correct: 0.87359\n",
      "[2,   413] loss: 0.38068 correct: 0.87750\n",
      "[2,   441] loss: 0.40811 correct: 0.87094\n",
      "[2,   469] loss: 0.41456 correct: 0.86922\n",
      "[2,   496] loss: 0.41329 correct: 0.87000\n",
      "[2,   524] loss: 0.40522 correct: 0.87094\n",
      "[2,   553] loss: 0.38121 correct: 0.88281\n",
      "[2,   580] loss: 0.37506 correct: 0.88625\n",
      "[2,   608] loss: 0.37566 correct: 0.88578\n",
      "[2,   637] loss: 0.37949 correct: 0.88375\n",
      "[2,   665] loss: 0.38350 correct: 0.88109\n",
      "[2,   693] loss: 0.36540 correct: 0.88578\n",
      "[2,   721] loss: 0.36614 correct: 0.88484\n",
      "[2,   749] loss: 0.33979 correct: 0.89109\n",
      "[2,   777] loss: 0.33045 correct: 0.89281\n",
      "[2,   806] loss: 0.34589 correct: 0.88766\n",
      "[2,   834] loss: 0.33939 correct: 0.89375\n",
      "[2,   863] loss: 0.35441 correct: 0.89078\n",
      "[2,   891] loss: 0.34211 correct: 0.89391\n",
      "[2,   920] loss: 0.34306 correct: 0.89453\n",
      "[2,   947] loss: 0.32867 correct: 0.89906\n",
      "[2,   975] loss: 0.32429 correct: 0.89734\n",
      "[2,  1004] loss: 0.33473 correct: 0.89453\n",
      "[2,  1033] loss: 0.32079 correct: 0.89797\n",
      "[2,  1061] loss: 0.33265 correct: 0.89578\n",
      "[2,  1089] loss: 0.33298 correct: 0.89687\n",
      "[2,  1116] loss: 0.33882 correct: 0.89594\n",
      "[2,  1144] loss: 0.33981 correct: 0.89672\n",
      "[2,  1172] loss: 0.32807 correct: 0.90187\n",
      "[2,  1199] loss: 0.32955 correct: 0.90078\n",
      "[2,  1227] loss: 0.34107 correct: 0.89828\n",
      "[2,  1255] loss: 0.34069 correct: 0.89734\n",
      "[2,  1283] loss: 0.34332 correct: 0.89531\n",
      "[2,  1311] loss: 0.34238 correct: 0.89547\n",
      "[2,  1340] loss: 0.34273 correct: 0.89422\n",
      "[2,  1368] loss: 0.33658 correct: 0.89812\n",
      "[2,  1397] loss: 0.33427 correct: 0.89719\n",
      "[2,  1426] loss: 0.32483 correct: 0.89938\n",
      "[2,  1455] loss: 0.31481 correct: 0.89984\n",
      "[2,  1484] loss: 0.31551 correct: 0.90016\n",
      "[2,  1512] loss: 0.32566 correct: 0.90016\n",
      "[2,  1540] loss: 0.34073 correct: 0.89812\n",
      "[2,  1569] loss: 0.33719 correct: 0.89891\n",
      "[2,  1598] loss: 0.32890 correct: 0.90156\n",
      "[2,  1627] loss: 0.30798 correct: 0.90328\n",
      "[2,  1656] loss: 0.30724 correct: 0.90266\n",
      "[2,  1684] loss: 0.30949 correct: 0.90172\n",
      "[2,  1712] loss: 0.29794 correct: 0.90438\n",
      "[2,  1740] loss: 0.31600 correct: 0.89953\n",
      "[2,  1768] loss: 0.31239 correct: 0.89984\n",
      "[2,  1796] loss: 0.33238 correct: 0.89719\n",
      "[2,  1825] loss: 0.32711 correct: 0.89922\n",
      "[2,  1854] loss: 0.31128 correct: 0.90375\n",
      "[2,  1883] loss: 0.30683 correct: 0.90531\n",
      "[2,  1911] loss: 0.30580 correct: 0.90469\n",
      "[2,  1939] loss: 0.29564 correct: 0.91000\n",
      "[2,  1968] loss: 0.30153 correct: 0.90875\n",
      "[2,  1996] loss: 0.28952 correct: 0.91016\n",
      "[3,     1] loss: 0.28092 correct: 0.91312\n",
      "[3,    29] loss: 0.29769 correct: 0.90734\n",
      "[3,    58] loss: 0.29199 correct: 0.91063\n",
      "[3,    85] loss: 0.29574 correct: 0.91141\n",
      "[3,   112] loss: 0.27291 correct: 0.91828\n",
      "[3,   139] loss: 0.28397 correct: 0.91344\n",
      "[3,   166] loss: 0.27718 correct: 0.91359\n",
      "[3,   194] loss: 0.27472 correct: 0.91281\n",
      "[3,   222] loss: 0.28121 correct: 0.90984\n",
      "[3,   250] loss: 0.26882 correct: 0.91531\n",
      "[3,   278] loss: 0.26439 correct: 0.91797\n",
      "[3,   306] loss: 0.27142 correct: 0.91531\n",
      "[3,   335] loss: 0.26467 correct: 0.91859\n",
      "[3,   364] loss: 0.28093 correct: 0.91094\n",
      "[3,   392] loss: 0.30082 correct: 0.90391\n",
      "[3,   420] loss: 0.29916 correct: 0.90406\n",
      "[3,   448] loss: 0.28414 correct: 0.90781\n",
      "[3,   476] loss: 0.27989 correct: 0.91359\n",
      "[3,   504] loss: 0.26853 correct: 0.91719\n",
      "[3,   533] loss: 0.26550 correct: 0.91609\n",
      "[3,   561] loss: 0.27061 correct: 0.91516\n",
      "[3,   590] loss: 0.26562 correct: 0.91422\n",
      "[3,   619] loss: 0.26216 correct: 0.91609\n",
      "[3,   647] loss: 0.25647 correct: 0.91625\n",
      "[3,   675] loss: 0.25584 correct: 0.91922\n",
      "[3,   702] loss: 0.23377 correct: 0.92703\n",
      "[3,   730] loss: 0.25992 correct: 0.92156\n",
      "[3,   759] loss: 0.27076 correct: 0.91938\n",
      "[3,   787] loss: 0.27893 correct: 0.91625\n",
      "[3,   815] loss: 0.27533 correct: 0.91531\n",
      "[3,   843] loss: 0.26799 correct: 0.91703\n",
      "[3,   872] loss: 0.25425 correct: 0.92078\n",
      "[3,   900] loss: 0.24097 correct: 0.92359\n",
      "[3,   927] loss: 0.24178 correct: 0.92297\n",
      "[3,   954] loss: 0.23893 correct: 0.92469\n",
      "[3,   983] loss: 0.24566 correct: 0.92422\n",
      "[3,  1012] loss: 0.24095 correct: 0.92625\n",
      "[3,  1040] loss: 0.25063 correct: 0.92563\n",
      "[3,  1068] loss: 0.25220 correct: 0.92188\n",
      "[3,  1096] loss: 0.25785 correct: 0.91922\n",
      "[3,  1124] loss: 0.27116 correct: 0.91484\n",
      "[3,  1152] loss: 0.26213 correct: 0.91797\n",
      "[3,  1180] loss: 0.25361 correct: 0.92172\n",
      "[3,  1208] loss: 0.23591 correct: 0.92703\n",
      "[3,  1236] loss: 0.23771 correct: 0.92750\n",
      "[3,  1265] loss: 0.23686 correct: 0.92609\n",
      "[3,  1292] loss: 0.25091 correct: 0.92219\n",
      "[3,  1321] loss: 0.25160 correct: 0.92031\n",
      "[3,  1350] loss: 0.24026 correct: 0.92625\n",
      "[3,  1378] loss: 0.23370 correct: 0.92594\n",
      "[3,  1407] loss: 0.21801 correct: 0.93141\n",
      "[3,  1435] loss: 0.21041 correct: 0.93281\n",
      "[3,  1462] loss: 0.22060 correct: 0.92984\n",
      "[3,  1491] loss: 0.22822 correct: 0.92766\n",
      "[3,  1519] loss: 0.22935 correct: 0.92578\n",
      "[3,  1547] loss: 0.22328 correct: 0.92750\n",
      "[3,  1575] loss: 0.21884 correct: 0.93203\n",
      "[3,  1603] loss: 0.22619 correct: 0.93219\n",
      "[3,  1631] loss: 0.23925 correct: 0.93141\n",
      "[3,  1659] loss: 0.23088 correct: 0.93234\n",
      "[3,  1687] loss: 0.22758 correct: 0.93078\n",
      "[3,  1716] loss: 0.21140 correct: 0.93391\n",
      "[3,  1744] loss: 0.20280 correct: 0.93563\n",
      "[3,  1771] loss: 0.19462 correct: 0.93656\n",
      "[3,  1797] loss: 0.20023 correct: 0.93516\n",
      "[3,  1824] loss: 0.19764 correct: 0.93641\n",
      "[3,  1851] loss: 0.20952 correct: 0.93312\n",
      "[3,  1877] loss: 0.22410 correct: 0.93125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data, verbose \u001b[38;5;129;01min\u001b[39;00m enumerate_report(trainloader, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "losses, accuracies = deque(maxlen=100), deque(maxlen=100)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data, verbose in enumerate_report(trainloader, 5):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = outputs.cpu().detach().argmax(dim=1, keepdim=True)\n",
    "        correct = pred.eq(labels.cpu().view_as(pred)).sum().item()\n",
    "        accuracy = correct / float(len(labels))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        if verbose and len(losses) > 5:\n",
    "            print('[%d, %5d] loss: %.5f correct: %.5f' % (epoch + 1, i + 1, np.mean(losses), np.mean(accuracies)))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
