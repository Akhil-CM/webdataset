{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebDataset + Distributed PyTorch Training\n",
    "\n",
    "This notebook illustrates how to use the Web Indexed Dataset (`wids`) library for distributed PyTorch training using `DistributedDataParallel`.\n",
    "\n",
    "Using `webdataset` results in training code that is almost identical to plain PyTorch except for the dataset creation.\n",
    "Since `WebDataset` is an iterable dataset, you need to account for that when creating the `DataLoader`. Furthermore, for\n",
    "distributed training, easy restarts, etc., it is convenient to use a resampled dataset; this is in contrast to\n",
    "sampling without replacement for each epoch as used more commonly for small, local training. (If you want to use\n",
    "sampling without replacement with webdataset format datasets, see the companion `wids`-based training notebooks.)\n",
    "\n",
    "Training with `WebDataset` can be carried out completely without local storage; this is the usual setup in the cloud\n",
    "and on high speed compute clusters. When running locally on a desktop, you may want to cache the data, and for that,\n",
    "you set a `cache_dir` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import datasets, transforms\n",
    "import ray\n",
    "import webdataset as wds\n",
    "import dataclasses\n",
    "import time\n",
    "from collections import deque\n",
    "from typing import Optional\n",
    "\n",
    "def enumerate_report(seq, delta, growth=1.0):\n",
    "    last = 0\n",
    "    count = 0\n",
    "    for count, item in enumerate(seq):\n",
    "        now = time.time()\n",
    "        if now - last > delta:\n",
    "            last = now\n",
    "            yield count, item, True\n",
    "        else:\n",
    "            yield count, item, False\n",
    "        delta *= growth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading for Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets are just collections of shards in the cloud. We usually specify\n",
    "# them using {lo..hi} brace notation (there is also a YAML spec for more complex\n",
    "# datasets).\n",
    "\n",
    "bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\"\n",
    "trainset_url = bucket+\"/imagenet-train-{000000..001281}.tar\"\n",
    "valset_url = bucket+\"/imagenet-val-{000000..000049}.tar\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not running in colab, caching data locally in ./_cache\n"
     ]
    }
   ],
   "source": [
    "# If running in the cloud or with a fast network storage system, we don't\n",
    "# need any local storage.\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    cache_dir = None\n",
    "    print(\"running on colab, streaming data directly from storage\")\n",
    "else:\n",
    "    cache_dir = \"./_cache\"\n",
    "    print(f\"not running in colab, caching data locally in {cache_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataloader pipeline is a fairly typical `IterableDataset` pipeline\n",
    "# for PyTorch\n",
    "\n",
    "def make_dataloader_train():\n",
    "    \"\"\"Create a DataLoader for training on the ImageNet dataset using WebDataset.\"\"\"\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    def make_sample(sample):\n",
    "        return transform(sample[\"jpg\"]), sample[\"cls\"]\n",
    "\n",
    "    # This is the basic WebDataset definition: it starts with a URL and add shuffling,\n",
    "    # decoding, and augmentation. Note `resampled=True`; this is essential for\n",
    "    # distributed training to work correctly.\n",
    "    trainset = wds.WebDataset(trainset_url, resampled=True, cache_dir=cache_dir)\n",
    "    trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample)\n",
    "\n",
    "    # For IterableDataset objects, the batching needs to happen in the dataset.\n",
    "    trainset = trainset.batched(64)\n",
    "    trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4)\n",
    "\n",
    "    # We unbatch, shuffle, and rebatch to mix samples from different workers.\n",
    "    trainloader = trainloader.unbatched().shuffle(1000).batched(batch_size)\n",
    "\n",
    "    # A resampled dataset is infinite size, but we can recreate a fixed epoch length.\n",
    "    trainloader = trainloader.with_epoch(1282 * 100 // 64)\n",
    "\n",
    "    return trainloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOPEN https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-001180.tar {}\n",
      "GOPENGOPENGOPEN   https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-000330.tarhttps://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-000437.tarhttps://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-000453.tar   {}{}{}\n",
      "\n",
      "\n",
      "pipe exit [0 2410154:2410234] ((\"curl -f -s -L 'https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-000437.tar'\",), {'shell': True, 'bufsize': 8192}) {}\n",
      "pipe exit [0 2410130:2410194] ((\"curl -f -s -L 'https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-001180.tar'\",), {'shell': True, 'bufsize': 8192}) {}\n",
      "pipe exit [0 2410166:2410235] ((\"curl -f -s -L 'https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-000453.tar'\",), {'shell': True, 'bufsize': 8192}) {}\n",
      "pipe exit [0 2410142:2410233] ((\"curl -f -s -L 'https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-000330.tar'\",), {'shell': True, 'bufsize': 8192}) {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Let's try it out\n",
    "\n",
    "def make_dataloader(split=\"train\"):\n",
    "    \"\"\"Make a dataloader for training or validation.\"\"\"\n",
    "    if split == \"train\":\n",
    "        return make_dataloader_train()\n",
    "    elif split == \"val\":\n",
    "        return make_dataloader_val() # not implemented for this notebook\n",
    "    else:\n",
    "        raise ValueError(f\"unknown split {split}\")\n",
    "\n",
    "# Try it out.\n",
    "os.environ[\"GOPEN_VERBOSE\"] = \"1\"\n",
    "sample = next(iter(make_dataloader()))\n",
    "print(sample[0].shape, sample[1].shape)\n",
    "os.environ[\"GOPEN_VERBOSE\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard PyTorch Training\n",
    "\n",
    "This is completely standard PyTorch training; nothing changes by using WebDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We gather all the configuration info into a single typed dataclass.\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    epochs: int = 1\n",
    "    maxsteps: int = int(1e18)\n",
    "    lr: float = 0.001\n",
    "    momentum: float = 0.9\n",
    "    rank: Optional[int] = None\n",
    "    world_size: int = 2\n",
    "    backend: str = \"nccl\"\n",
    "    master_addr: str = \"localhost\"\n",
    "    master_port: str = \"12355\"\n",
    "    report_s: float = 15.0\n",
    "    report_growth: float = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    # Define the model, loss function, and optimizer\n",
    "    model = resnet50(pretrained=False).cuda()\n",
    "    if config.rank is not None:\n",
    "        model = DistributedDataParallel(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
    "\n",
    "    # Data loading code\n",
    "    trainloader = make_dataloader(split='train')\n",
    "\n",
    "    losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        for i, data, verbose in enumerate_report(trainloader, config.report_s):\n",
    "            inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # update statistics\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            accuracy = (outputs.argmax(1) == labels).float().mean()  # calculate accuracy\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(accuracy.item())\n",
    "\n",
    "            if verbose and len(losses) > 0:\n",
    "                avgloss = sum(losses)/len(losses)\n",
    "                avgaccuracy = sum(accuracies)/len(accuracies)\n",
    "                print(f\"rank {config.rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += len(labels)\n",
    "            if steps > config.maxsteps:\n",
    "                print(\"finished training (maxsteps)\", steps, config.maxsteps, file=sys.stderr)\n",
    "                return\n",
    "\n",
    "    print(\"finished Training\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "rank None epoch     0/        0 loss    7.029 acc    0.000         0\n",
      "finished training (maxsteps) 1024 1000\n"
     ]
    }
   ],
   "source": [
    "# A quick smoke test of the training function.\n",
    "\n",
    "config = Config()\n",
    "config.epochs = 1\n",
    "config.maxsteps = 1000\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Distributed Training with Ray\n",
    "\n",
    "Ray is a convenient distributed computing framework. We are using it here to start up the training\n",
    "jobs on multiple GPUs. You can use `torch.distributed.launch` or other such tools as well with the above\n",
    "code. Ray has the advantage that it is runtime environment independent; you set up your Ray cluster\n",
    "in whatever way works for your environment, and afterwards, this code will run in it without change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def train_on_ray(rank, config):\n",
    "    \"\"\"Set up distributed torch env and train the model on this node.\"\"\"\n",
    "    # Set up distributed PyTorch.\n",
    "    if rank is not None:\n",
    "        os.environ['MASTER_ADDR'] = config.master_addr\n",
    "        os.environ['MASTER_PORT'] = config.master_port\n",
    "        dist.init_process_group(backend=config.backend, rank=rank, world_size=config.world_size)\n",
    "        config.rank = rank\n",
    "        # Ray will automatically set CUDA_VISIBLE_DEVICES for each task.\n",
    "    train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 10:38:48,453\tINFO worker.py:1664 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m /home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m /home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/        0 loss    6.862 acc    0.000         0\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m /home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m /home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/       17 loss    6.174 acc    0.042       544\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/       30 loss    5.666 acc    0.048       960\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/       54 loss    4.945 acc    0.061      1728\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/       77 loss    4.502 acc    0.075      2464\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      100 loss    4.178 acc    0.082      3200\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      123 loss    3.531 acc    0.102      3936\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      146 loss    3.234 acc    0.108      4672\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      169 loss    3.102 acc    0.102      5408\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      192 loss    3.026 acc    0.100      6144\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      214 loss    2.986 acc    0.092      6848\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      237 loss    2.955 acc    0.087      7584\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      260 loss    2.933 acc    0.095      8320\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      283 loss    2.915 acc    0.102      9056\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      306 loss    2.905 acc    0.103      9792\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      329 loss    2.889 acc    0.108     10528\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      352 loss    2.877 acc    0.110     11264\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      374 loss    2.871 acc    0.106     11968\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      382 loss    2.849 acc    0.119     12224\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      397 loss    2.857 acc    0.110     12704\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      405 loss    2.839 acc    0.127     12960\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      420 loss    2.846 acc    0.120     13440\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      428 loss    2.816 acc    0.133     13696\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      443 loss    2.836 acc    0.115     14176\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      451 loss    2.810 acc    0.133     14432\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      466 loss    2.826 acc    0.114     14912\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      474 loss    2.800 acc    0.140     15168\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      489 loss    2.823 acc    0.110     15648\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      497 loss    2.796 acc    0.131     15904\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      512 loss    2.819 acc    0.103     16384\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      520 loss    2.797 acc    0.129     16640\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      535 loss    2.806 acc    0.108     17120\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      543 loss    2.798 acc    0.117     17376\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      558 loss    2.800 acc    0.103     17856\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      581 loss    2.800 acc    0.102     18592\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      604 loss    2.790 acc    0.107     19328\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      627 loss    2.783 acc    0.113     20064\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      649 loss    2.785 acc    0.117     20768\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      657 loss    2.754 acc    0.127     21024\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      672 loss    2.781 acc    0.123     21504\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      680 loss    2.746 acc    0.127     21760\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      695 loss    2.767 acc    0.124     22240\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      703 loss    2.735 acc    0.132     22496\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      717 loss    2.763 acc    0.125     22944\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      726 loss    2.732 acc    0.132     23232\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      740 loss    2.744 acc    0.135     23680\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      749 loss    2.732 acc    0.141     23968\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      763 loss    2.733 acc    0.141     24416\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      771 loss    2.729 acc    0.145     24672\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      786 loss    2.725 acc    0.146     25152\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      794 loss    2.724 acc    0.145     25408\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      809 loss    2.719 acc    0.144     25888\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      817 loss    2.720 acc    0.144     26144\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      832 loss    2.722 acc    0.141     26624\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      840 loss    2.719 acc    0.143     26880\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      855 loss    2.710 acc    0.138     27360\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      863 loss    2.704 acc    0.137     27616\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      878 loss    2.718 acc    0.140     28096\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      886 loss    2.702 acc    0.141     28352\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      901 loss    2.716 acc    0.144     28832\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      909 loss    2.697 acc    0.140     29088\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      924 loss    2.712 acc    0.141     29568\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      932 loss    2.688 acc    0.143     29824\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      947 loss    2.712 acc    0.142     30304\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      955 loss    2.692 acc    0.145     30560\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      970 loss    2.706 acc    0.146     31040\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/      978 loss    2.687 acc    0.147     31296\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/      993 loss    2.690 acc    0.153     31776\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1015 loss    2.683 acc    0.158     32480\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1023 loss    2.681 acc    0.151     32736\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1037 loss    2.678 acc    0.160     33184\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1046 loss    2.680 acc    0.149     33472\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1060 loss    2.675 acc    0.166     33920\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1068 loss    2.673 acc    0.155     34176\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1083 loss    2.665 acc    0.170     34656\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1091 loss    2.671 acc    0.159     34912\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1106 loss    2.665 acc    0.175     35392\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1114 loss    2.666 acc    0.173     35648\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1129 loss    2.661 acc    0.184     36128\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1151 loss    2.661 acc    0.185     36832\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1159 loss    2.647 acc    0.181     37088\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1174 loss    2.654 acc    0.191     37568\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1197 loss    2.638 acc    0.187     38304\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1220 loss    2.619 acc    0.191     39040\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1243 loss    2.615 acc    0.191     39776\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1265 loss    2.617 acc    0.194     40480\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1273 loss    2.626 acc    0.179     40736\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1288 loss    2.606 acc    0.199     41216\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1296 loss    2.613 acc    0.190     41472\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1311 loss    2.603 acc    0.197     41952\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1319 loss    2.605 acc    0.190     42208\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1334 loss    2.588 acc    0.198     42688\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1342 loss    2.610 acc    0.187     42944\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1357 loss    2.582 acc    0.191     43424\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1365 loss    2.602 acc    0.188     43680\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1379 loss    2.574 acc    0.194     44128\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1387 loss    2.601 acc    0.192     44384\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1402 loss    2.584 acc    0.192     44864\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1410 loss    2.602 acc    0.188     45120\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1425 loss    2.599 acc    0.188     45600\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1448 loss    2.603 acc    0.182     46336\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m config \u001b[38;5;241m=\u001b[39m Config()\n\u001b[1;32m     14\u001b[0m config\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdistributed_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mdistributed_training\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      8\u001b[0m num_gpus \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mavailable_resources()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m config\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(config\u001b[38;5;241m.\u001b[39mworld_size, num_gpus)\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_on_ray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/proj/webdataset/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/proj/webdataset/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/proj/webdataset/venv/lib/python3.10/site-packages/ray/_private/worker.py:2557\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m     )\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2557\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/proj/webdataset/venv/lib/python3.10/site-packages/ray/_private/worker.py:769\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    764\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m         )\n\u001b[1;32m    768\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 769\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_ms\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, metadata \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3211\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:449\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1471 loss    2.600 acc    0.190     47072\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1493 loss    2.601 acc    0.194     47776\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1501 loss    2.573 acc    0.199     48032\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1516 loss    2.582 acc    0.206     48512\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1524 loss    2.564 acc    0.203     48768\n",
      "\u001b[36m(train_on_ray pid=2410917)\u001b[0m rank 1 epoch     0/     1539 loss    2.571 acc    0.214     49248\n",
      "\u001b[36m(train_on_ray pid=2410918)\u001b[0m rank 0 epoch     0/     1547 loss    2.562 acc    0.206     49504\n"
     ]
    }
   ],
   "source": [
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "\n",
    "ray.available_resources()['GPU']\n",
    "\n",
    "def distributed_training(config):\n",
    "    \"\"\"Perform distributed training with the given config.\"\"\"\n",
    "    num_gpus = ray.available_resources()['GPU']\n",
    "    config.world_size = min(config.world_size, num_gpus)\n",
    "    results = ray.get([train_on_ray.remote(i, config) for i in range(config.world_size)])\n",
    "    print(results)\n",
    "\n",
    "config = Config()\n",
    "config.epochs = 3\n",
    "distributed_training(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
