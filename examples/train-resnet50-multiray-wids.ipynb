{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Optional, Any, Union, Callable, Iterable, Iterator, NamedTuple, Set, Sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import ray\n",
    "import wids\n",
    "import dataclasses\n",
    "import time\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "\n",
    "def enumerate_report(seq, delta, growth=1.0):\n",
    "    last = 0\n",
    "    count = 0\n",
    "    for count, item in enumerate(seq):\n",
    "        now = time.time()\n",
    "        if now - last > delta:\n",
    "            last = now\n",
    "            yield count, item, True\n",
    "        else:\n",
    "            yield count, item, False\n",
    "        delta *= growth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading for Distributed Training\n",
    "\n",
    "The datasets we use for training are stored in the cloud.\n",
    "We use `fake-imagenet`, which is 1/10th the size of Imagenet\n",
    "and artificially generated, but it has the same number of\n",
    "shards and trains quickly.\n",
    "\n",
    "Note that unlike the `webdataset` library, `wids` always needs\n",
    "a local cache directory (it will use `/tmp` if you don't give it\n",
    "anything explicitly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not running in colab, caching data locally in ./_cache\n"
     ]
    }
   ],
   "source": [
    "bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\"\n",
    "trainset_url = bucket+\"/imagenet-train-{000000..001281}.tar\"\n",
    "valset_url = bucket+\"/imagenet-val-{000000..000049}.tar\"\n",
    "cache_dir = \"./_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gs://webdataset/fake-imagenet/imagenet-train.json base: gs://webdataset/fake-imagenet name: imagenet-train nfiles: 1282 nbytes: 31242280960 samples: 128200 cache: ./_cache\n",
      "/home/tmb/proj/webdataset/wids/wids.py:710: UserWarning: DistributedChunkedSampler is called without distributed initialized; assuming single process\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# This is a typical PyTorch dataset, except that we read from the cloud.\n",
    "\n",
    "def make_dataset_train():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    def make_sample(sample):\n",
    "        image = sample[\".jpg\"]\n",
    "        label = sample[\".cls\"]\n",
    "        return transform_train(image), label\n",
    "\n",
    "    trainset = wids.ShardListDataset(\"gs://webdataset/fake-imagenet/imagenet-train.json\", cache_dir=\"./_cache\", keep=True)\n",
    "    trainset = trainset.add_transform(make_sample)\n",
    "\n",
    "    return trainset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really the only thing that is ever so slightly special about the `wids` library:\n",
    "you should use the special `DistributedChunkedSampler` for sampling.\n",
    "\n",
    "The regular `DistributedSampler` will technically work, but because of its poor locality\n",
    "of reference, will be significantly slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To keep locality of reference in the dataloader, we use a special sampler\n",
    "# for distributed training, DistributedChunkedSampler.\n",
    "\n",
    "def make_dataloader_train():\n",
    "    dataset = make_dataset_train()\n",
    "    sampler = wids.DistributedChunkedSampler(dataset, chunksize=1000, shuffle=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler, num_workers=4)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def make_dataloader(split=\"train\"):\n",
    "    \"\"\"Make a dataloader for training or validation.\"\"\"\n",
    "    if split == \"train\":\n",
    "        return make_dataloader_train()\n",
    "    elif split == \"val\":\n",
    "        return make_dataloader_val()\n",
    "    else:\n",
    "        raise ValueError(f\"unknown split {split}\")\n",
    "\n",
    "# Try it out.\n",
    "sample = next(iter(make_dataloader()))\n",
    "print(sample[0].shape, sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Distributed Training Code\n",
    "\n",
    "Really, all that's needed for distributed training is the `DistributedDataParallel` wrapper around the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(rank=None, epochs=1, maxsteps=1000000000000000000, lr=0.001, momentum=0.9, world_size=8, backend='nccl', master_addr='localhost', master_port='12355', report_s=15.0, report_growth=1.1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For convenience, we collect all the configuration parameters into\n",
    "# a dataclass.\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    rank: Optional[int] = None\n",
    "    epochs: int = 1\n",
    "    maxsteps: int = int(1e18)\n",
    "    lr: float = 0.001\n",
    "    momentum: float = 0.9\n",
    "    world_size: int = 8\n",
    "    backend: str = \"nccl\"\n",
    "    master_addr: str = \"localhost\"\n",
    "    master_port: str = \"12355\"\n",
    "    report_s: float = 15.0\n",
    "    report_growth: float = 1.1\n",
    "\n",
    "Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A typical PyTorch training function.\n",
    "\n",
    "def train(config):\n",
    "    # Define the model, loss function, and optimizer\n",
    "    model = resnet50(pretrained=False).cuda()\n",
    "    if config.rank is not None:\n",
    "        model = DistributedDataParallel(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
    "\n",
    "    # Data loading code\n",
    "    trainloader = make_dataloader(split='train')\n",
    "\n",
    "    losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        for i, data, verbose in enumerate_report(trainloader, config.report_s):\n",
    "            inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # just bookkeping and progress report\n",
    "            steps += len(labels)\n",
    "            accuracy = (outputs.argmax(1) == labels).float().mean()  # calculate accuracy\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(accuracy.item())\n",
    "            if verbose and len(losses) > 0:\n",
    "                avgloss = sum(losses)/len(losses)\n",
    "                avgaccuracy = sum(accuracies)/len(accuracies)\n",
    "                print(f\"rank {config.rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr)\n",
    "            if steps > config.maxsteps:\n",
    "                print(\"finished training (maxsteps)\", steps, config.maxsteps, file=sys.stderr)\n",
    "                return\n",
    "\n",
    "    print(\"finished Training\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "gs://webdataset/fake-imagenet/imagenet-train.json base: gs://webdataset/fake-imagenet name: imagenet-train nfiles: 1282 nbytes: 31242280960 samples: 128200 cache: ./_cache\n",
      "rank None epoch     0/        0 loss    7.180 acc    0.000        32\n",
      "finished training (maxsteps) 1024 1000\n"
     ]
    }
   ],
   "source": [
    "# A quick smoke test.\n",
    "\n",
    "config = Config()\n",
    "config.epochs = 1\n",
    "config.maxsteps = 1000\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training in Ray\n",
    "\n",
    "The code above can be used with any distributed computing framwork, including `torch.distributed.launch`.\n",
    "\n",
    "Below is simply an example of how to launch the training jobs with the Ray framework. Ray is nice\n",
    "for distributed training because it makes the Python code independent of the runtime environment\n",
    "(Kubernetes, Slurm, ad-hoc networking, etc.). Meaning, the code below will work regardless of how\n",
    "you start up your Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distributed training function to be used with Ray.\n",
    "# Since this is started via Ray remote, we set up the distributed\n",
    "# training environment here.\n",
    "\n",
    "def train_in_ray(rank, config):\n",
    "    # Set up distributed PyTorch.\n",
    "    if rank is not None:\n",
    "        config.rank = rank\n",
    "        os.environ['MASTER_ADDR'] = config.master_addr\n",
    "        os.environ['MASTER_PORT'] = config.master_port\n",
    "        print(f\"rank {rank} initializing process group\", file=sys.stderr)\n",
    "        dist.init_process_group(backend=config.backend, rank=rank, world_size=config.world_size)\n",
    "        print(f\"rank {rank} done initializing process group\", file=sys.stderr)\n",
    "    train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#gpus available in the cluster 2.0\n",
      "[{'NodeID': '1548755e0d56fbbaea367df3cb590eeb6af479d17a2f1c9b48a2e8a4', 'Alive': True, 'NodeManagerAddress': '10.20.13.236', 'NodeManagerHostname': 'bragi', 'NodeManagerPort': 42255, 'ObjectManagerPort': 38851, 'ObjectStoreSocketName': '/tmp/ray/session_2023-12-12_03-18-27_260854_2295938/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2023-12-12_03-18-27_260854_2295938/sockets/raylet', 'MetricsExportPort': 63404, 'NodeName': '10.20.13.236', 'RuntimeEnvAgentPort': 42330, 'alive': True, 'Resources': {'GPU': 2.0, 'CPU': 12.0, 'accelerator_type:TITAN': 1.0, 'memory': 17423568078.0, 'node:10.20.13.236': 1.0, 'node:__internal_head__': 1.0, 'object_store_memory': 8711784038.0}, 'Labels': {'ray.io/node_id': '1548755e0d56fbbaea367df3cb590eeb6af479d17a2f1c9b48a2e8a4'}}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Ray cluster if it hasn't been initialized yet.\n",
    "\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "print(\"#gpus available in the cluster\", ray.available_resources()['GPU'])\n",
    "print(ray.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(rank=None,\n",
      "       epochs=10,\n",
      "       maxsteps=1000000000000000000,\n",
      "       lr=0.001,\n",
      "       momentum=0.9,\n",
      "       world_size=2,\n",
      "       backend='nccl',\n",
      "       master_addr='localhost',\n",
      "       master_port='12355',\n",
      "       report_s=15.0,\n",
      "       report_growth=1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 initializing process group\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 done initializing process group\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m /home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m /home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m gs://webdataset/fake-imagenet/imagenet-train.json base: gs://webdataset/fake-imagenet name: imagenet-train nfiles: 1282 nbytes: 31242280960 samples: 128200 cache: ./_cache\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/        0 loss    6.821 acc    0.000        32\n",
      "\u001b[36m(train_remote pid=2318157)\u001b[0m rank 1 initializing process group\n",
      "\u001b[36m(train_remote pid=2318157)\u001b[0m rank 1 done initializing process group\n",
      "\u001b[36m(train_remote pid=2318157)\u001b[0m /home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "\u001b[36m(train_remote pid=2318157)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_remote pid=2318157)\u001b[0m /home/tmb/proj/webdataset/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "\u001b[36m(train_remote pid=2318157)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[36m(train_remote pid=2318157)\u001b[0m gs://webdataset/fake-imagenet/imagenet-train.json base: gs://webdataset/fake-imagenet name: imagenet-train nfiles: 1282 nbytes: 31242280960 samples: 128200 cache: ./_cache\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/       22 loss    5.805 acc    0.042       736\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/       44 loss    5.088 acc    0.055      1440\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/       66 loss    4.615 acc    0.060      2144\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/       88 loss    4.289 acc    0.064      2848\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      110 loss    3.813 acc    0.078      3552\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      132 loss    3.378 acc    0.085      4256\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      154 loss    3.175 acc    0.092      4960\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      176 loss    3.070 acc    0.091      5664\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      198 loss    3.007 acc    0.092      6368\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      220 loss    2.965 acc    0.096      7072\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      242 loss    2.933 acc    0.099      7776\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      264 loss    2.914 acc    0.098      8480\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      286 loss    2.894 acc    0.105      9184\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      308 loss    2.875 acc    0.108      9888\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      330 loss    2.863 acc    0.112     10592\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      352 loss    2.842 acc    0.116     11296\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      374 loss    2.829 acc    0.123     12000\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      396 loss    2.824 acc    0.117     12704\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      418 loss    2.824 acc    0.116     13408\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      440 loss    2.816 acc    0.115     14112\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      462 loss    2.813 acc    0.114     14816\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      484 loss    2.804 acc    0.119     15520\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      506 loss    2.786 acc    0.122     16224\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      528 loss    2.770 acc    0.128     16928\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      550 loss    2.768 acc    0.128     17632\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      572 loss    2.769 acc    0.125     18336\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      594 loss    2.763 acc    0.127     19040\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      616 loss    2.773 acc    0.127     19744\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      638 loss    2.772 acc    0.132     20448\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      660 loss    2.762 acc    0.141     21152\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      682 loss    2.763 acc    0.146     21856\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      704 loss    2.759 acc    0.149     22560\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      726 loss    2.743 acc    0.151     23264\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m config \u001b[38;5;241m=\u001b[39m Config()\n\u001b[1;32m     14\u001b[0m config\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdistributed_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m, in \u001b[0;36mdistributed_training\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      8\u001b[0m config\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(config\u001b[38;5;241m.\u001b[39mworld_size, num_gpus))\n\u001b[1;32m      9\u001b[0m pprint(config)\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_remote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/proj/webdataset/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/proj/webdataset/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/proj/webdataset/venv/lib/python3.10/site-packages/ray/_private/worker.py:2557\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m     )\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2557\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/proj/webdataset/venv/lib/python3.10/site-packages/ray/_private/worker.py:769\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    764\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m         )\n\u001b[1;32m    768\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 769\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_ms\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, metadata \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3211\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:449\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_remote pid=2318156)\u001b[0m rank 0 epoch     0/      748 loss    2.733 acc    0.151     23968\n"
     ]
    }
   ],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def train_remote(rank, args):\n",
    "    # Ray will automatically set CUDA_VISIBLE_DEVICES for each task.\n",
    "    train_in_ray(rank, args)\n",
    "\n",
    "def distributed_training(config):\n",
    "    num_gpus = ray.available_resources()['GPU']\n",
    "    config.world_size = int(min(config.world_size, num_gpus))\n",
    "    pprint(config)\n",
    "    results = ray.get([train_remote.remote(i, config) for i in range(config.world_size)])\n",
    "    print(results)\n",
    "\n",
    "config = Config()\n",
    "config.epochs = 10\n",
    "distributed_training(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
