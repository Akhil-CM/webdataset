{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import datasets, transforms\n",
    "import ray\n",
    "import webdataset as wds\n",
    "import dataclasses\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "def enumerate_report(seq, delta, growth=1.0):\n",
    "    last = 0\n",
    "    count = 0\n",
    "    for count, item in enumerate(seq):\n",
    "        now = time.time()\n",
    "        if now - last > delta:\n",
    "            last = now\n",
    "            yield count, item, True\n",
    "        else:\n",
    "            yield count, item, False\n",
    "        delta *= growth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\"\n",
    "trainset_url = bucket+\"/imagenet-train-{000000..001281}.tar\"\n",
    "valset_url = bucket+\"/imagenet-val-{000000..000049}.tar\"\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    cache_dir = None\n",
    "    print(\"running on colab, streaming data directly from storage\")\n",
    "else:\n",
    "    cache_dir = \"./_cache\"\n",
    "    print(f\"not running in colab, caching data locally in {cache_dir}\")\n",
    "\n",
    "def make_dataloader_train():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    def make_sample(sample):\n",
    "        return transform(sample[\"jpg\"]), sample[\"cls\"]\n",
    "    trainset = wds.WebDataset(trainset_url, resampled=True, cache_dir=cache_dir)\n",
    "    trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample)\n",
    "    trainset = trainset.batched(64)\n",
    "    trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4)\n",
    "    trainloader = trainloader.unbatched().batched(64).with_epoch(1282 * 100 // 64)\n",
    "    return trainloader\n",
    "\n",
    "def make_dataloader(split=\"train\"):\n",
    "    if split == \"train\":\n",
    "        return make_dataloader_train()\n",
    "    elif split == \"val\":\n",
    "        return make_dataloader_val()\n",
    "    else:\n",
    "        raise ValueError(f\"unknown split {split}\")\n",
    "\n",
    "sample = next(iter(make_dataloader()))\n",
    "print(sample[0].shape, sample[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Args:\n",
    "    epochs: int = 1\n",
    "    maxsteps: int = int(1e18)\n",
    "    lr: float = 0.001\n",
    "    momentum: float = 0.9\n",
    "    rank: int = 0\n",
    "    world_size: int = 2\n",
    "    backend: str = \"nccl\"\n",
    "    master_addr: str = \"localhost\"\n",
    "    master_port: str = \"12355\"\n",
    "    report_s: float = 15.0\n",
    "    report_growth: float = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, args):\n",
    "    # Set up distributed PyTorch.\n",
    "    if rank is not None:\n",
    "        os.environ['MASTER_ADDR'] = args.master_addr\n",
    "        os.environ['MASTER_PORT'] = args.master_port\n",
    "        print(f\"rank {rank} initializing process group\", file=sys.stderr)\n",
    "        dist.init_process_group(backend=args.backend, rank=rank, world_size=args.world_size)\n",
    "        print(f\"rank {rank} done initializing process group\", file=sys.stderr)\n",
    "\n",
    "    # Define the model, loss function, and optimizer\n",
    "    model = resnet50(pretrained=False).cuda()\n",
    "    if rank is not None:\n",
    "        model = DistributedDataParallel(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Data loading code\n",
    "    trainloader = make_dataloader(split='train')\n",
    "\n",
    "    losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(args.epochs):\n",
    "        for i, data, verbose in enumerate_report(trainloader, args.report_s):\n",
    "            inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # update statistics\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            accuracy = (outputs.argmax(1) == labels).float().mean()  # calculate accuracy\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(accuracy.item())\n",
    "\n",
    "            if verbose and len(losses) > 0:\n",
    "                avgloss = sum(losses)/len(losses)\n",
    "                avgaccuracy = sum(accuracies)/len(accuracies)\n",
    "                print(f\"rank {rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += len(labels)\n",
    "            if steps > args.maxsteps:\n",
    "                print(\"finished training (maxsteps)\", steps, args.maxsteps, file=sys.stderr)\n",
    "                return\n",
    "\n",
    "    print(\"finished Training\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "args = Args()\n",
    "args.epochs = 1\n",
    "args.maxsteps = 100000\n",
    "train(None, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "ray.available_resources()['GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def train_remote(rank, args):\n",
    "    # Ray will automatically set CUDA_VISIBLE_DEVICES for each task.\n",
    "    train(rank, args)\n",
    "\n",
    "def distributed_training(world_size=2):\n",
    "    args = Args()\n",
    "    num_gpus = ray.available_resources()['GPU']\n",
    "    args.world_size = min(world_size, num_gpus)\n",
    "    results = ray.get([train_remote.remote(i, args) for i in range(args.world_size)])\n",
    "    print(results)\n",
    "\n",
    "distributed_training(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
