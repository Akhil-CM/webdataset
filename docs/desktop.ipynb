{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Smaller\" Datasets and Desktop Computing\n",
    "\n",
    "WebDataset is an ideal solution for training on petascale datasets kept on high performance distributed data stores like AIStore, AWS/S3, and Google Cloud. Compared to data center GPU servers, desktop machines have much slower network connections, but training jobs on desktop machines often also use much smaller datasets. WebDataset also is very useful for such smaller datasets, and it can easily be used for developing and testing on small datasets and then scaling up to large datasets by simply using more shards.\n",
    "\n",
    "\n",
    "Here are different usage scenarios:\n",
    "\n",
    "- **desktop deep learning, smaller datasets**\n",
    "    - copy all shards to local disk manually\n",
    "    - use automatic shard caching\n",
    "- **prototyping, development, testing of jobs for large scale training**\n",
    "    - copy a small subset of shards to local disk\n",
    "    - use automatic shard caching with a small subrange of shards\n",
    "    - use DBCache sample caching\n",
    "- **cloud training against cloud buckets**\n",
    "    - use WebDataset directly with remote URLs\n",
    "- **on premises training with high performance store (e.g., AIStore) and fast networks**\n",
    "    - use WebDataset directly with remote URLs\n",
    "- **on premises training with slower object stores and/or slower networks**\n",
    "    - use automatic shard caching or DBCache\n",
    "- **training with IterableDataset sources other than WebDataset**\n",
    "    - use DBCache\n",
    "    \n",
    "Let's look at how these different methods work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Copying of Shards\n",
    "\n",
    "Let's take the OpenImages dataset as an example; it's half a terabyte large. For development and testing, you may not want to download the entire dataset, but you may also not want to use the dataset remotely. With WebDataset, you can just download a small number of shards and use them during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar > /tmp/openimages-train-000000.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'__key__': 'e39871fd9fd74f55', 'jpg': b'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x01\\\\x01:\\\\x01:\\\\x00\\\\x00\\\\xff\\\\xdb\\\\x00C\\\\x00\\\\x06\\\\x04\\\\x05\\\\x06\\\\x05\\\\x04\\\\x06\\\\x06\\\\x05\\\\x06\\\\x07\\\\x07\\\\x06\\\\x08\\\\n\\\\x10\\\\n\\\\n\\\\t\\\\t\\\\n\\\\x14\\\\x0e\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = wds.WebDataset(\"/tmp/openimages-train-000000.tar\")\n",
    "repr(next(iter(dataset)))[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the WebDataset class works the same way on local files as it does on remote files. Furthermore, unlike other kinds of dataset formats and archive formats, downloaded datasets are immediately useful and don't need to be unpacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Shard Caching\n",
    "\n",
    "Downloading a few shards manually is useful for development and testing. But WebDataset permits us to automate downloading and caching of shards. This is accomplished by giving a `cache_dir` argument to the WebDataset constructor.\n",
    "\n",
    "Here, we make two passes through the dataset, using the cached version on the second pass.\n",
    "\n",
    "Note that caching happens in parallel with iterating through the dataset. This means that if you write a WebDataset-based I/O pipeline, training starts immediately; the training job does not have to wait for any shards to download first.\n",
    "\n",
    "Automatic shard caching is useful for distributing deep learning code, for academic computer labs, and for cloud computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[caching <webdataset.gopen.Pipe object at 0x7fb21e058310> at ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a.~1448717~ ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== first pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[done caching ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a ]\n",
      "[finished ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a]\n",
      "[opening cached ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== second pass\n",
      "__key__ 'e39871fd9fd74f55'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01\n",
      "json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli\n",
      "\n",
      "__key__ 'f18b91585c4d3f3e'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\n",
      "json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti\n",
      "\n",
      "__key__ 'ede6e66b2fb59aab'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\n",
      "json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti\n",
      "\n",
      "__key__ 'ed600d57fcee4f94'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01\n",
      "json b'[{\"ImageID\": \"ed600d57fcee4f94\", \"Source\": \"acti\n",
      "\n",
      "total 987920\n",
      "-rw-rw-r-- 1 tmb tmb 1011630080 Oct 30 10:48 9fd87fa8-d42e-3be4-a3a6-839de961b98a\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./cache\n",
    "\n",
    "# just using one URL for demonstration\n",
    "url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"\n",
    "dataset = wds.WebDataset(url, cache_dir=\"./cache\")\n",
    "\n",
    "print(\"=== first pass\")\n",
    "\n",
    "for sample in dataset:\n",
    "    pass\n",
    "\n",
    "print(\"=== second pass\")\n",
    "\n",
    "for i, sample in enumerate(dataset):\n",
    "    for key, value in sample.items():\n",
    "        print(key, repr(value)[:50])\n",
    "    print()\n",
    "    if i >= 3: break\n",
    "        \n",
    "!ls -l ./cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Sample Caching\n",
    "\n",
    "WebDataset also provides a way of caching training samples directly. This works with samples coming from any IterableDataset as input. The cache is stored in an SQLite3 database.\n",
    "\n",
    "Sample-based caching is implemented by the `DBCache` class. You specify a filename for the database and the maximum number of samples you want to cache. Samples will initially be read from the original IterableDataset, but after either the samples run out or the maximum number of samples has been reached, subsequently, samples will be served from the database cache stored on local disk. The database cache persists between invocations of the job.\n",
    "\n",
    "Automatic sample caching is useful for developing and testing deep learning jobs, as well as for caching data coming from slow IterableDataset sources, such as network-based database connections or other slower data sources.\n",
    "\n",
    "You can add or remove the caching stage at will (e.g., to switch between testing and production), without affecting the rest of the code at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DBCache opened ./cache.db size 1000 total 0]\n",
      "[DBCache total 0 size 1000 more caching]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== first pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DBCache finished caching total 1000 (size 1000)]\n",
      "[DBCache starting dbiter total 1000 size 1000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== second pass\n",
      "__key__ 'e39871fd9fd74f55'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01\n",
      "json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli\n",
      "\n",
      "__key__ 'f18b91585c4d3f3e'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\n",
      "json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti\n",
      "\n",
      "__key__ 'ede6e66b2fb59aab'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\n",
      "json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti\n",
      "\n",
      "__key__ 'ed600d57fcee4f94'\n",
      "jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01\n",
      "json b'[{\"ImageID\": \"ed600d57fcee4f94\", \"Source\": \"acti\n",
      "\n",
      "-rw-r--r-- 1 tmb tmb 485199872 Oct 30 10:49 ./cache.db\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./cache.db\n",
    "\n",
    "dataset = wds.WebDataset(url).compose(wds.DBCache, \"./cache.db\", 1000)\n",
    "\n",
    "print(\"=== first pass\")\n",
    "\n",
    "for sample in dataset:\n",
    "    pass\n",
    "\n",
    "print(\"=== second pass\")\n",
    "\n",
    "for i, sample in enumerate(dataset):\n",
    "    for key, value in sample.items():\n",
    "        print(key, repr(value)[:50])\n",
    "    print()\n",
    "    if i >= 3: break\n",
    "        \n",
    "!ls -l ./cache.db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
