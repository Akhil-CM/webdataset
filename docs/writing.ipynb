{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Filters and Offline Augmentation\n",
    "\n",
    "Webdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class(data):\n",
    "    # mock implementation\n",
    "    return 0\n",
    "\n",
    "def augment_wds(input, output, maxcount=999999999):\n",
    "    src = (\n",
    "        wds.WebDataset(input)\n",
    "        .decode(\"pil\")\n",
    "        .to_tuple(\"__key__\", \"jpg;png\", \"json\")\n",
    "        .map_tuple(identity, preproc, identity)\n",
    "    )\n",
    "    with wds.TarWriter(output) as dst:\n",
    "        for key, image, data in islice(src, 0, maxcount):\n",
    "            print(key)\n",
    "            image = image.numpy().transpose(1, 2, 0)\n",
    "            image -= amin(image)\n",
    "            image /= amax(image)\n",
    "            sample = {\n",
    "                \"__key__\": key,\n",
    "                \"png\": image,\n",
    "                \"cls\": extract_class(data)\n",
    "            }\n",
    "            dst.write(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the augmentation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e39871fd9fd74f55\n",
      "f18b91585c4d3f3e\n",
      "ede6e66b2fb59aab\n",
      "ed600d57fcee4f94\n",
      "ff47e649b23f446d\n"
     ]
    }
   ],
   "source": [
    "url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"\n",
    "url = f\"pipe:curl -L -s {url} || true\"\n",
    "augment_wds(url, \"_temp.tar\", maxcount=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that things worked correctly, let's look at the output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e39871fd9fd74f55.cls\n",
      "e39871fd9fd74f55.png\n",
      "f18b91585c4d3f3e.cls\n",
      "f18b91585c4d3f3e.png\n",
      "ede6e66b2fb59aab.cls\n",
      "ede6e66b2fb59aab.png\n",
      "ed600d57fcee4f94.cls\n",
      "ed600d57fcee4f94.png\n",
      "ff47e649b23f446d.cls\n",
      "ff47e649b23f446d.png\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tar tf _temp.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system.\n",
    "\n",
    "For example, using Dask, you could process all 554 shards in parallel using code like this:\n",
    "\n",
    "```Python\n",
    "shards = braceexpand.braceexpand(\"{000000..000554}\")\n",
    "inputs = [f\"gs://bucket/openimages-{shard}.tar\" for shard in shards]\n",
    "outputs = [f\"gs://bucket2/openimages-augmented-{shard}.tar\" for shard in shards]\n",
    "results = [dask.delayed(augment_wds)(args) for args in zip(inputs, outputs)]\n",
    "dask.compute(*results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker.\n",
    "\n",
    "For very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes `Job` template, or using a workflow engine like Argo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
