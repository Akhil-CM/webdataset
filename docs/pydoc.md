Documentation as generated by 'pydoc3' (default Python documentation
viewer). You can view this at the command line by typing
'pydoc3 objio.io'.
# module `webdataset.dataset`

```
Help on module webdataset.dataset in webdataset:

NAME
    webdataset.dataset

DESCRIPTION
    Train PyTorch models directly from POSIX tar archive, locally
    or over HTTP connections.

CLASSES
    torch.utils.data.dataset.IterableDataset(torch.utils.data.dataset.Dataset)
        WebDataset
    
    class WebDataset(torch.utils.data.dataset.IterableDataset)
     |  WebDataset(urls, *, extensions=None, decoder='rgb', transforms=None, pipeline=None, keys=<function base_plus_ext at 0x7fa5e0100200>, opener=<function reader at 0x7fa5e00ea680>, errors=True, verbose=False, shuffle=0, associate=None, prepare_for_worker=True, container=None)
     |  
     |  Iterate over sharded datasets.
     |  
     |  :param urls: shard spec or list of shards
     |  :param extensions: extensions to extract (Default value = None, can be either list of lists or "a;b c")
     |  :param decode: decoder to apply to files in tarfiles (Default value = True, based on extension)
     |  :param transforms: list of functions to apply to unbatched samples (Default value = None)
     |  :param pipeline: function that maps the iterator, e.g. for batching
     |  :param opener: either a function that returns a stream or a string that is invoked via Popen
     |  :param verbose: verbose output
     |  :param shuffle: if >0, then shuffle shards, and shuffle samples with a buffer of the given size
     |  :param associate: a callable or dictionary that returns additional information to associate with each sample
     |  :param prepare_for_worker: callable called in each worker before anything else is done
     |  :param container: if given, treats the tar file as a record file of containers (protobufs, msgpack, etc.)
     |  :param extra_meta: associates subset info with each sample record
     |  
     |  The decoder can be True (default decoder), False (no decoder), a callable (called
     |  decode the sample, or a dictionary mapping filename extensions to callables for
     |  the decoding.
     |  
     |  Method resolution order:
     |      WebDataset
     |      torch.utils.data.dataset.IterableDataset
     |      torch.utils.data.dataset.Dataset
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  __init__(self, urls, *, extensions=None, decoder='rgb', transforms=None, pipeline=None, keys=<function base_plus_ext at 0x7fa5e0100200>, opener=<function reader at 0x7fa5e00ea680>, errors=True, verbose=False, shuffle=0, associate=None, prepare_for_worker=True, container=None)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  __iter__(self)
     |      Iterate over samples.
     |  
     |  shard_selection(self)
     |      Contains the logic for self.subset shard selection.
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from torch.utils.data.dataset.IterableDataset:
     |  
     |  __add__(self, other)
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from torch.utils.data.dataset.Dataset:
     |  
     |  __getitem__(self, index)
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)

FUNCTIONS
    imagehandler(data, imagespec)
        Decode image data using the given `imagespec`.
        
        The `imagespec` specifies whether the image is decoded
        to numpy/torch/pi, decoded to uint8/float, and decoded
        to l/rgb/rgba:
        
        - l8: numpy uint8 l
        - rgb8: numpy uint8 rgb
        - rgba8: numpy uint8 rgba
        - l: numpy float l
        - rgb: numpy float rgb
        - rgba: numpy float rgba
        - torchl8: torch uint8 l
        - torchrgb8: torch uint8 rgb
        - torchrgba8: torch uint8 rgba
        - torchl: torch float l
        - torchrgb: torch float rgb
        - torch: torch float rgb
        - torchrgba: torch float rgba
        - pill: pil None l
        - pil: pil None rgb
        - pilrgb: pil None rgb
        - pilrgba: pil None rgba
    
    tariterator(fileobj, keys=<function base_plus_ext at 0x7fa5e0100200>, decoder=True, suffixes=None, errors=True, container=None)
        Iterate through training samples stored in a sharded tar file.
        
        :param fileobj: a Python file-like object
        :param check_sorted:  check whether the input is actually properly sorted (Default value = False)
        :param keys:  key extraction function (Default value = base_plus_ext)
        :param decoder: value decoding function (Default value = True)
        
        The key extraction function takes a string representing a pathname and
        returns a pair (__key__, suffix).
        
        The decoder takes the entire sample as a dict and returns the
        decoded sample as a dict.

DATA
    __all__ = ['WebDataset', 'tariterator', 'default_handlers', 'imagehand...
    default_handlers = {'l': {'class': <function maybe_int>, 'cls': <funct...

FILE
    /home/tmb/proj/webdataset/webdataset/dataset.py


```

# module `webdataset.writer`

```
Help on module webdataset.writer in webdataset:

NAME
    webdataset.writer

DESCRIPTION
    # Copyright (c) 2017-2019 NVIDIA CORPORATION. All rights reserved.
    # This file is part of the WebDataset library.
    # See the LICENSE file for licensing terms (BSD-style).
    #

CLASSES
    builtins.object
        ShardWriter
        TarWriter
    
    class ShardWriter(builtins.object)
     |  ShardWriter(pattern, maxcount=100000, maxsize=3000000000.0, post=None, **kw)
     |  
     |  Like TarWriter but splits into multiple shards.
     |  
     |  :param pattern: output file pattern
     |  :param maxcount: maximum number of records per shard (Default value = 100000)
     |  :param maxsize: maximum size of each shard (Default value = 3e9)
     |  :param kw: other options passed to TarWriter
     |  
     |  Methods defined here:
     |  
     |  __enter__(self)
     |  
     |  __exit__(self, *args, **kw)
     |  
     |  __init__(self, pattern, maxcount=100000, maxsize=3000000000.0, post=None, **kw)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  close(self)
     |  
     |  finish(self)
     |  
     |  next_stream(self)
     |  
     |  write(self, obj)
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
    
    class TarWriter(builtins.object)
     |  TarWriter(fileobj, user='bigdata', group='bigdata', mode=292, compress=None, encoder=True, keep_meta=False)
     |  
     |  A class for writing dictionaries to tar files.
     |  
     |  :param fileobj: fileobj: file name for tar file (.tgz/.tar) or open file descriptor
     |  :param encoder: sample encoding (Default value = None)
     |  :param compress:  (Default value = None)
     |  
     |  The following code will add two file to the tar archive: `a/b.png` and
     |  `a/b.output.png`.
     |  
     |  ```Python
     |      tarwriter = TarWriter(stream)
     |      image = imread("b.jpg")
     |      image2 = imread("b.out.jpg")
     |      sample = {"__key__": "a/b", "png": image, "output.png": image2}
     |      tarwriter.write(sample)
     |  ```
     |  
     |  Methods defined here:
     |  
     |  __enter__(self)
     |  
     |  __exit__(self, exc_type, exc_val, exc_tb)
     |  
     |  __init__(self, fileobj, user='bigdata', group='bigdata', mode=292, compress=None, encoder=True, keep_meta=False)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  close(self)
     |      Close the tar file.
     |  
     |  dwrite(self, key, **kw)
     |      Convenience function for `write`.
     |      
     |      Takes key as the first argument and key-value pairs for the rest.
     |      Replaces "_" with ".".
     |  
     |  write(self, obj)
     |      Write a dictionary to the tar file.
     |      
     |      :param obj: dictionary of objects to be stored
     |      :returns: size of the entry
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)

DATA
    __all__ = ['TarWriter', 'ShardWriter']

FILE
    /home/tmb/proj/webdataset/webdataset/writer.py


```

# module `webdataset.tenbin`

```
Help on module webdataset.tenbin in webdataset:

NAME
    webdataset.tenbin - Binary tensor encodings for PyTorch and NumPy.

DESCRIPTION
    This defines efficient binary encodings for tensors. The format is 8 byte
    aligned and can be used directly for computations when transmitted, say,
    via RDMA. The format is supported by WebDataset with the `.ten` filename
    extension. It is also used by Tensorcom, Tensorcom RDMA, and can be used
    for fast tensor storage with LMDB and in disk files (which can be memory
    mapped)
    
    Data is encoded as a series of chunks:
    
    - magic number (int64)
    - length in bytes (int64)
    - bytes (multiple of 64 bytes long)
    
    Arrays are a header chunk followed by a data chunk.
    Header chunks have the following structure:
    
    - dtype (int64)
    - 8 byte array name
    - ndim (int64)
    - dim[0]
    - dim[1]
    - ...

FUNCTIONS
    load(fname, infos=False, nocheck=False)
        Read a list of arrays from a file, with magics, length, and padding.
    
    read(stream, n=999999, infos=False)
        Read a list of arrays from a stream, with magics, length, and padding.
    
    save(fname, *args, infos=None, nocheck=False)
        Save a list of arrays to a file, with magics, length, and padding.
    
    sctp_recv(socket, infos=False, maxsize=100000000)
        Receive arrays as an SCTP datagram.
        
        This is just a convenience function and illustration.
        For more complex networking needs, you may want
        to call sctp_recv and decode_buffer directly.
    
    sctp_send(socket, dest, l, infos=None)
        Send arrays as an SCTP datagram.
        
        This is just a convenience function and illustration.
        For more complex networking needs, you may want
        to call encode_buffer and sctp_send directly.
    
    write(stream, l, infos=None)
        Write a list of arrays to a stream, with magics, length, and padding.
    
    zrecv_multipart(socket, infos=False)
        Receive arrays as a multipart ZMQ message.
    
    zrecv_single(socket, infos=False)
        Receive arrays as a single part ZMQ message.
    
    zsend_multipart(socket, l, infos=None)
        Send arrays as a multipart ZMQ message.
    
    zsend_single(socket, l, infos=None)
        Send arrays as a single part ZMQ message.

DATA
    __all__ = ['read', 'write', 'save', 'load', 'zsend_single', 'zrecv_sin...

FILE
    /home/tmb/proj/webdataset/webdataset/tenbin.py


```

