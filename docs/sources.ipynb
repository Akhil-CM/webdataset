{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import braceexpand\n",
    "from torch.utils.data import IterableDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local and Remote Storage URLs\n",
    "\n",
    "WebDataset refers to data sources using file paths or URLs. The following are all valid ways of referring to a data source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.WebDataset(\"dataset-000.tar\")\n",
    "dataset = wds.WebDataset(\"file:dataset-000.tar\")\n",
    "dataset = wds.WebDataset(\"http://server/dataset-000.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional way of referring to data is using the `pipe:` scheme, so the following is also equivalent to the above references:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.WebDataset(\"pipe:cat dataset-000.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the same notation for accessing data in cloud storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.WebDataset(\"pipe:gsutil cat gs://somebucket/dataset-000.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that access to standard web schemas are implemented using `curl`. That is, `http://server/dataset.tar` is internally simply treated like `pipe:curl -s -L 'http://server/dataset.tar'`. The use of `curl` to access Internet protocols actually is more efficient than using the built-in `http` library because it results in asynchronous name resolution and downloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define handlers for new schemes or override implementations for existing schemes by adding entries to `wds.gopen_schemes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gopen_gs(url, mode=\"rb\", bufsize=8192):\n",
    "    ...\n",
    "\n",
    "wds.gopen_schemes[\"gs\"] = gopen_gs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Input/Output\n",
    "\n",
    "As a special case, the string \"-\" refers to standard input (reading) or standard output (writing). This allows code using WebDataset to be used as pipes, and it permits code written over shards to be applied to individual files on disk.\n",
    "\n",
    "For example, assume that you have an image classification command line program and you want to apply it to a collection of images in a directory. You might write:\n",
    "\n",
    "```Python\n",
    "tar cf - *.jpg | shard-classifier - -o - | tar xvf - --include '.cls'\n",
    "```\n",
    "\n",
    "This is the rough equivalent of:\n",
    "\n",
    "```Python\n",
    "for image in *.jpg; do\n",
    "   image-classifier $image > $(basename $image .jpg).cls\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Shards and Mixing Datasets\n",
    "\n",
    "The `WebDataset` and `ShardList` classes take either a string or a list of strings as an argument. When given a string, the string is expanded using `braceexpand`. Therefore, the following three datasets are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.WebDataset([\"dataset-000.tar\", \"dataset-001.tar\", \"dataset-002.tar\", \"dataset-003.tar\"])\n",
    "dataset = wds.WebDataset(\"dataset-{000..003}.tar\")\n",
    "dataset = wds.WebDataset(\"file:dataset-{000..003}.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complex training problems, you may want to mix multiple datasets, where each dataset consists of multiple shards. A good way is to expand each shard spec individually using `braceexpand` and concatenate the lists. Then you can pass the result list as an argument to `WebDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1695\n"
     ]
    }
   ],
   "source": [
    "urls = (\n",
    "    list(braceexpand.braceexpand(\"imagenet-{000000..000146}.tar\")) +\n",
    "    list(braceexpand.braceexpand(\"openimages-{000000..000547}.tar\")) +\n",
    "    list(braceexpand.braceexpand(\"custom-images-{000000..000999}.tar\"))\n",
    ")\n",
    "print(len(urls))\n",
    "dataset = wds.WebDataset(urls, shardshuffle=True).shuffle(10000).decode(\"torchrgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing Datsets with a Custom `IterableDataset` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex sampling problems, you can also write sample processors. For example, to sample equally from several datasets, you could write something like this (the `Shorthands` and `Composable` base classes just add some convenience methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleEqually(IterableDataset, wds.Shorthands, wds.Composable):\n",
    "    def __init__(self, datasets):\n",
    "        super().__init__()\n",
    "        self.datasets = datasets\n",
    "    def __iter__(self):\n",
    "        sources = [iter(ds) for ds in self.datasets]\n",
    "        while True:\n",
    "            for source in sources:\n",
    "                try:\n",
    "                    yield next(source)\n",
    "                except StopIteration:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can mix samples from different sources in more complex ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = wds.WebDataset(\"imagenet-{000000..000146}.tar\", shardshuffle=True).shuffle(1000).decode(\"torchrgb\")\n",
    "dataset2 = wds.WebDataset(\"openimages-{000000..000547}.tar\", shardshuffle=True).shuffle(1000).decode(\"torchrgb\")\n",
    "dataset3 = wds.WebDataset(\"custom-images-{000000..000999}.tar\", shardshuffle=True).shuffle(1000).decode(\"torchrgb\")\n",
    "dataset = SampleEqually([dataset1, dataset2, dataset3]).shuffle(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
