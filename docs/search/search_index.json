{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WebDataset WebDataset is a PyTorch Dataset (IterableDataset) implementation providing efficient access to datasets stored in POSIX tar archives and uses only sequential/streaming data access. This brings substantial performance advantage in many compute environments, and it is essential for very large scale training. While WebDataset scales to very large problems, it also works well with smaller datasets and simplifies creation, management, and distribution of training data for deep learning. WebDataset implements standard PyTorch IterableDataset interface and works with the PyTorch DataLoader . Access to datasets is as simple as: import webdataset as wds dataset = wds.WebDataset(url).shuffle(1000).decode(\"torchrgb\").to_tuple(\"jpg;png\", \"json\") dataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=16) for inputs, outputs in dataloader: ... In that code snippet, url can refer to a local file, a local HTTP server, a cloud storage object, an object on an object store, or even the output of arbitrary command pipelines. WebDataset fulfills a similar function to Tensorflow's TFRecord/tf.Example classes, but it is much easier to adopt because it does not actually require any kind of data conversion: data is stored in exactly the same format inside tar files as it is on disk, and all preprocessing and data augmentation code remains unchanged. Installation $ pip install webdataset For the Github version: $ pip install git+https://github.com/tmbdev/webdataset.git Documentation: ReadTheDocs Introductory Videos Here are some videos talking about WebDataset and large scale deep learning: Introduction to Large Scale Deep Learning Loading Training Data with WebDataset Creating Datasets in WebDataset Format Tools for Working with Large Datasets Related Libraries and Software The AIStore server provides an efficient backend for WebDataset; it functions like a combination of web server, content distribution network, P2P network, and distributed file system. Together, AIStore and WebDataset can serve input data from rotational drives distributed across many servers at the speed of local SSDs to many GPUs, at a fraction of the cost. We can easily achieve hundreds of MBytes/s of I/O per GPU even in large, distributed training jobs. The tarproc utilities provide command line manipulation and processing of webdatasets and other tar files, including splitting, concatenation, and xargs -like functionality. The tensorcom library provides fast three-tiered I/O; it can be inserted between AIStore and WebDataset to permit distributed data augmentation and I/O. It is particularly useful when data augmentation requires more CPU than the GPU server has available. You can find the full PyTorch ImageNet sample code converted to WebDataset at tmbdev/pytorch-imagenet-wds","title":"Home"},{"location":"#webdataset","text":"WebDataset is a PyTorch Dataset (IterableDataset) implementation providing efficient access to datasets stored in POSIX tar archives and uses only sequential/streaming data access. This brings substantial performance advantage in many compute environments, and it is essential for very large scale training. While WebDataset scales to very large problems, it also works well with smaller datasets and simplifies creation, management, and distribution of training data for deep learning. WebDataset implements standard PyTorch IterableDataset interface and works with the PyTorch DataLoader . Access to datasets is as simple as: import webdataset as wds dataset = wds.WebDataset(url).shuffle(1000).decode(\"torchrgb\").to_tuple(\"jpg;png\", \"json\") dataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=16) for inputs, outputs in dataloader: ... In that code snippet, url can refer to a local file, a local HTTP server, a cloud storage object, an object on an object store, or even the output of arbitrary command pipelines. WebDataset fulfills a similar function to Tensorflow's TFRecord/tf.Example classes, but it is much easier to adopt because it does not actually require any kind of data conversion: data is stored in exactly the same format inside tar files as it is on disk, and all preprocessing and data augmentation code remains unchanged.","title":"WebDataset"},{"location":"#installation","text":"$ pip install webdataset For the Github version: $ pip install git+https://github.com/tmbdev/webdataset.git Documentation: ReadTheDocs","title":"Installation"},{"location":"#introductory-videos","text":"Here are some videos talking about WebDataset and large scale deep learning: Introduction to Large Scale Deep Learning Loading Training Data with WebDataset Creating Datasets in WebDataset Format Tools for Working with Large Datasets","title":"Introductory Videos"},{"location":"#related-libraries-and-software","text":"The AIStore server provides an efficient backend for WebDataset; it functions like a combination of web server, content distribution network, P2P network, and distributed file system. Together, AIStore and WebDataset can serve input data from rotational drives distributed across many servers at the speed of local SSDs to many GPUs, at a fraction of the cost. We can easily achieve hundreds of MBytes/s of I/O per GPU even in large, distributed training jobs. The tarproc utilities provide command line manipulation and processing of webdatasets and other tar files, including splitting, concatenation, and xargs -like functionality. The tensorcom library provides fast three-tiered I/O; it can be inserted between AIStore and WebDataset to permit distributed data augmentation and I/O. It is particularly useful when data augmentation requires more CPU than the GPU server has available. You can find the full PyTorch ImageNet sample code converted to WebDataset at tmbdev/pytorch-imagenet-wds","title":"Related Libraries and Software"},{"location":"creating/","text":"import webdataset as wds import torchvision import sys Creating a WebDataset Using tar Since WebDatasets are just regular tar files, you can usually create them by just using the tar command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with $ tar --sort=name -cf dataset.tar dataset/ If your dataset has some other directory layout, you may need a different file name in the archive from the name on disk. You can use the --transform argument to GNU tar to transform file names. You can also use the -T argument to read the files from a text file and embed other options in that text file. The tarp create Command The tarp command is a little utility for manipulating tar archives. Its create subcommand makes it particularly simple to construct tar archives from files. The tarp create command takes a recipe for building a tar archive that contains lines of the form: archive-name-1 source-name-1 archive-name-2 source-name-2 ... The source name can either be a file, \"text:something\", or \"pipe:something\". Programmatically in Python You can also create a WebDataset with library functions in this library: webdataset.TarWriter takes dictionaries containing key value pairs and writes them to disk webdataset.ShardWriter takes dictionaries containing key value pairs and writes them to disk as a series of shards Direct Conversion of Any Dataset Here is a quick way of converting an existing dataset into a WebDataset; this will store all tensors as Python pickles: dataset = torchvision.datasets.MNIST(root=\"./temp\", download=True) sink = wds.TarWriter(\"mnist.tar\") for index, (input, output) in enumerate(dataset): if index%1000==0: print(f\"{index:6d}\", end=\"\\r\", flush=True, file=sys.stderr) sink.write({ \"__key__\": \"sample%06d\" % index, \"input.pyd\": input, \"output.pyd\": output, }) sink.close() 59000 !ls -l mnist.tar !tar tvf mnist.tar | head -rw-rw-r-- 1 tmb tmb 276490240 Oct 31 14:05 mnist.tar -r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000000.input.pyd -r--r--r-- bigdata/bigdata 5 2020-10-31 14:05 sample000000.output.pyd -r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000001.input.pyd -r--r--r-- bigdata/bigdata 5 2020-10-31 14:05 sample000001.output.pyd -r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000002.input.pyd -r--r--r-- bigdata/bigdata 5 2020-10-31 14:05 sample000002.output.pyd -r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000003.input.pyd -r--r--r-- bigdata/bigdata 5 2020-10-31 14:05 sample000003.output.pyd -r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000004.input.pyd -r--r--r-- bigdata/bigdata 5 2020-10-31 14:05 sample000004.output.pyd tar: write error Storing data as Python pickles allows most common Python datatypes to be stored, it is lossless, and the format is fast to decode. However, it is uncompressed and cannot be read by non-Python programs. It's often better to choose other storage formats, e.g., taking advantage of common image compression formats. Direct Conversion of Any Dataset with Compression If you know that the input is an image and the output is an integer class, you can also write something like this: dataset = torchvision.datasets.MNIST(root=\"./temp\", download=True) sink = wds.TarWriter(\"mnist.tar\") for index, (input, output) in enumerate(dataset): if index%1000==0: print(f\"{index:6d}\", end=\"\\r\", flush=True, file=sys.stderr) sink.write({ \"__key__\": \"sample%06d\" % index, \"ppm\": input, \"cls\": output, }) sink.close() 59000 !ls -l mnist.tar !tar tvf mnist.tar | head -rw-rw-r-- 1 tmb tmb 276490240 Oct 31 14:05 mnist.tar -r--r--r-- bigdata/bigdata 1 2020-10-31 14:05 sample000000.cls -r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000000.ppm -r--r--r-- bigdata/bigdata 1 2020-10-31 14:05 sample000001.cls -r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000001.ppm -r--r--r-- bigdata/bigdata 1 2020-10-31 14:05 sample000002.cls -r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000002.ppm -r--r--r-- bigdata/bigdata 1 2020-10-31 14:05 sample000003.cls -r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000003.ppm -r--r--r-- bigdata/bigdata 1 2020-10-31 14:05 sample000004.cls -r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000004.ppm tar: write error All we needed to do was to change the key from .input.pyd to .ppm ; this will trigger using an image compressor (in this case, writing the image in PPM format). You can use different image types depending on what speed, compression, and quality tradeoffs you want to make. If you want to encode data yourself, you can simply convert it to a byte string yourself, store it under the desired key in the sample, and that binary string will get written out. Using TarWriter / ShardWriter with Binary Data (Lossless Writing) The assert statements in that loop are not necessary, but they document and illustrate the expectations for this particular dataset. Generally, the \".jpg\" encoder can actually encode a wide variety of array types as images. The \".cls\" encoder always requires an integer for encoding. Here is how you can use TarWriter for writing a dataset without using an encoder: sink = wds.TarWriter(\"dest.tar\", encoder=False) for basename in basenames: with open(f\"{basename}.png\", \"rb\") as stream): image = stream.read() cls = lookup_cls(basename) sample = { \"__key__\": basename, \"input.png\": image, \"target.cls\": cls } sink.write(sample) sink.close() Since no encoder is used, if you want to be able to read this data with the default decoder, image must contain a byte string corresponding to a PNG image (as indicated by the \".png\" extension on its dictionary key), and cls must contain an integer encoded in ASCII (as indicated by the \".cls\" extension on its dictionary key).","title":"Creating Webdatasets"},{"location":"creating/#creating-a-webdataset","text":"","title":"Creating a WebDataset"},{"location":"creating/#using-tar","text":"Since WebDatasets are just regular tar files, you can usually create them by just using the tar command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with $ tar --sort=name -cf dataset.tar dataset/ If your dataset has some other directory layout, you may need a different file name in the archive from the name on disk. You can use the --transform argument to GNU tar to transform file names. You can also use the -T argument to read the files from a text file and embed other options in that text file.","title":"Using tar"},{"location":"creating/#the-tarp-create-command","text":"The tarp command is a little utility for manipulating tar archives. Its create subcommand makes it particularly simple to construct tar archives from files. The tarp create command takes a recipe for building a tar archive that contains lines of the form: archive-name-1 source-name-1 archive-name-2 source-name-2 ... The source name can either be a file, \"text:something\", or \"pipe:something\".","title":"The tarp create Command"},{"location":"creating/#programmatically-in-python","text":"You can also create a WebDataset with library functions in this library: webdataset.TarWriter takes dictionaries containing key value pairs and writes them to disk webdataset.ShardWriter takes dictionaries containing key value pairs and writes them to disk as a series of shards","title":"Programmatically in Python"},{"location":"creating/#direct-conversion-of-any-dataset","text":"Here is a quick way of converting an existing dataset into a WebDataset; this will store all tensors as Python pickles: dataset = torchvision.datasets.MNIST(root=\"./temp\", download=True) sink = wds.TarWriter(\"mnist.tar\") for index, (input, output) in enumerate(dataset): if index%1000==0: print(f\"{index:6d}\", end=\"\\r\", flush=True, file=sys.stderr) sink.write({ \"__key__\": \"sample%06d\" % index, \"input.pyd\": input, \"output.pyd\": output, }) sink.close() 59000 !ls -l mnist.tar !tar tvf mnist.tar | head -rw-rw-r-- 1 tmb tmb 276490240 Oct 31 14:05 mnist.tar -r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000000.input.pyd -r--r--r-- bigdata/bigdata 5 2020-10-31 14:05 sample000000.output.pyd -r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000001.input.pyd -r--r--r-- bigdata/bigdata 5 2020-10-31 14:05 sample000001.output.pyd -r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000002.input.pyd -r--r--r-- bigdata/bigdata 5 2020-10-31 14:05 sample000002.output.pyd -r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000003.input.pyd -r--r--r-- bigdata/bigdata 5 2020-10-31 14:05 sample000003.output.pyd -r--r--r-- bigdata/bigdata 845 2020-10-31 14:05 sample000004.input.pyd -r--r--r-- bigdata/bigdata 5 2020-10-31 14:05 sample000004.output.pyd tar: write error Storing data as Python pickles allows most common Python datatypes to be stored, it is lossless, and the format is fast to decode. However, it is uncompressed and cannot be read by non-Python programs. It's often better to choose other storage formats, e.g., taking advantage of common image compression formats.","title":"Direct Conversion of Any Dataset"},{"location":"creating/#direct-conversion-of-any-dataset-with-compression","text":"If you know that the input is an image and the output is an integer class, you can also write something like this: dataset = torchvision.datasets.MNIST(root=\"./temp\", download=True) sink = wds.TarWriter(\"mnist.tar\") for index, (input, output) in enumerate(dataset): if index%1000==0: print(f\"{index:6d}\", end=\"\\r\", flush=True, file=sys.stderr) sink.write({ \"__key__\": \"sample%06d\" % index, \"ppm\": input, \"cls\": output, }) sink.close() 59000 !ls -l mnist.tar !tar tvf mnist.tar | head -rw-rw-r-- 1 tmb tmb 276490240 Oct 31 14:05 mnist.tar -r--r--r-- bigdata/bigdata 1 2020-10-31 14:05 sample000000.cls -r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000000.ppm -r--r--r-- bigdata/bigdata 1 2020-10-31 14:05 sample000001.cls -r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000001.ppm -r--r--r-- bigdata/bigdata 1 2020-10-31 14:05 sample000002.cls -r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000002.ppm -r--r--r-- bigdata/bigdata 1 2020-10-31 14:05 sample000003.cls -r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000003.ppm -r--r--r-- bigdata/bigdata 1 2020-10-31 14:05 sample000004.cls -r--r--r-- bigdata/bigdata 797 2020-10-31 14:05 sample000004.ppm tar: write error All we needed to do was to change the key from .input.pyd to .ppm ; this will trigger using an image compressor (in this case, writing the image in PPM format). You can use different image types depending on what speed, compression, and quality tradeoffs you want to make. If you want to encode data yourself, you can simply convert it to a byte string yourself, store it under the desired key in the sample, and that binary string will get written out.","title":"Direct Conversion of Any Dataset with Compression"},{"location":"creating/#using-tarwritershardwriter-with-binary-data-lossless-writing","text":"The assert statements in that loop are not necessary, but they document and illustrate the expectations for this particular dataset. Generally, the \".jpg\" encoder can actually encode a wide variety of array types as images. The \".cls\" encoder always requires an integer for encoding. Here is how you can use TarWriter for writing a dataset without using an encoder: sink = wds.TarWriter(\"dest.tar\", encoder=False) for basename in basenames: with open(f\"{basename}.png\", \"rb\") as stream): image = stream.read() cls = lookup_cls(basename) sample = { \"__key__\": basename, \"input.png\": image, \"target.cls\": cls } sink.write(sample) sink.close() Since no encoder is used, if you want to be able to read this data with the default decoder, image must contain a byte string corresponding to a PNG image (as indicated by the \".png\" extension on its dictionary key), and cls must contain an integer encoded in ASCII (as indicated by the \".cls\" extension on its dictionary key).","title":"Using TarWriter/ShardWriter with Binary Data (Lossless Writing)"},{"location":"decoding/","text":"%pylab inline import torch from torch.utils.data import IterableDataset from torchvision import transforms import webdataset as wds from itertools import islice Populating the interactive namespace from numpy and matplotlib Data Decoding Data decoding is a special kind of transformations of samples. You could simply write a decoding function like this: def my_sample_decoder(sample): result = dict(__key__=sample[\"__key__\"]) for key, value in sample.items(): if key == \"png\" or key.endswith(\".png\"): result[key] = mageio.imread(io.BytesIO(value)) elif ...: ... return result dataset = wds.Processor(dataset, wds.map, my_sample_decoder) This gets tedious, though, and it also unnecessarily hardcodes the sample's keys into the processing pipeline. To help with this, there is a helper class that simplifies this kind of code. The primary use of Decoder is for decoding compressed image, video, and audio formats, as well as unzipping .gz files. Here is an example of automatically decoding .png images with imread and using the default torch_video and torch_audio decoders for video and audio: def my_png_decoder(key, value): if not key.endswith(\".png\"): return None assert isinstance(value, bytes) return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(my_png_decoder, wds.torch_video, wds.torch_audio)(dataset) You can use whatever criteria you like for deciding how to decode values in samples. When used with standard WebDataset format files, the keys are the full extensions of the file names inside a .tar file. For consistency, it's recommended that you primarily rely on the extensions (e.g., .png , .mp4 ) to decide which decoders to use. There is a special helper function that simplifies this: def my_decoder(value): return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(wds.handle_extension(\".png\", my_decoder))(dataset) If you want to \"decode everyting\" automatically and even override some extensions, you can use something like: url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" def png_decoder_16bpp(key, data): ... dataset = wds.WebDataset(url).decode( wds.handle_extension(\"left.png\", png_decoder_16bpp), wds.handle_extension(\"right.png\", png_decoder_16bpp), wds.imagehandler(\"torchrgb\"), wds.torch_audio, wds.torch_video ) This code would... handle any file with a \".left.png\" or \".right.png\" extension using a special 16bpp PNG decoder function decode all other image extensions to three channel Torch tensors decode audio files using the torchaudio library decode video files using the torchvideo library In order to decode images, audio, and video, it would dynamically load the Pillow , torchaudio , and torchvideo libraries. Automatic Decompression The default decoder handles compressed files automatically. That is .json.gz is decompressed first using the gzip library and then treated as if it had been called .json . In other words, you can store compressed files directly in a WebDataset and decompression is handled for you automatically. If you want to add your own decompressors, look at the implementation of webdataset.autodecode.gzfilter .","title":"Decoding"},{"location":"decoding/#data-decoding","text":"Data decoding is a special kind of transformations of samples. You could simply write a decoding function like this: def my_sample_decoder(sample): result = dict(__key__=sample[\"__key__\"]) for key, value in sample.items(): if key == \"png\" or key.endswith(\".png\"): result[key] = mageio.imread(io.BytesIO(value)) elif ...: ... return result dataset = wds.Processor(dataset, wds.map, my_sample_decoder) This gets tedious, though, and it also unnecessarily hardcodes the sample's keys into the processing pipeline. To help with this, there is a helper class that simplifies this kind of code. The primary use of Decoder is for decoding compressed image, video, and audio formats, as well as unzipping .gz files. Here is an example of automatically decoding .png images with imread and using the default torch_video and torch_audio decoders for video and audio: def my_png_decoder(key, value): if not key.endswith(\".png\"): return None assert isinstance(value, bytes) return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(my_png_decoder, wds.torch_video, wds.torch_audio)(dataset) You can use whatever criteria you like for deciding how to decode values in samples. When used with standard WebDataset format files, the keys are the full extensions of the file names inside a .tar file. For consistency, it's recommended that you primarily rely on the extensions (e.g., .png , .mp4 ) to decide which decoders to use. There is a special helper function that simplifies this: def my_decoder(value): return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(wds.handle_extension(\".png\", my_decoder))(dataset) If you want to \"decode everyting\" automatically and even override some extensions, you can use something like: url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" def png_decoder_16bpp(key, data): ... dataset = wds.WebDataset(url).decode( wds.handle_extension(\"left.png\", png_decoder_16bpp), wds.handle_extension(\"right.png\", png_decoder_16bpp), wds.imagehandler(\"torchrgb\"), wds.torch_audio, wds.torch_video ) This code would... handle any file with a \".left.png\" or \".right.png\" extension using a special 16bpp PNG decoder function decode all other image extensions to three channel Torch tensors decode audio files using the torchaudio library decode video files using the torchvideo library In order to decode images, audio, and video, it would dynamically load the Pillow , torchaudio , and torchvideo libraries.","title":"Data Decoding"},{"location":"decoding/#automatic-decompression","text":"The default decoder handles compressed files automatically. That is .json.gz is decompressed first using the gzip library and then treated as if it had been called .json . In other words, you can store compressed files directly in a WebDataset and decompression is handled for you automatically. If you want to add your own decompressors, look at the implementation of webdataset.autodecode.gzfilter .","title":"Automatic Decompression"},{"location":"desktop/","text":"%pylab inline import torch from torch.utils.data import IterableDataset from torchvision import transforms import webdataset as wds from itertools import islice url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" Populating the interactive namespace from numpy and matplotlib Desktop Usage and Caching WebDataset is an ideal solution for training on petascale datasets kept on high performance distributed data stores like AIStore, AWS/S3, and Google Cloud. Compared to data center GPU servers, desktop machines have much slower network connections, but training jobs on desktop machines often also use much smaller datasets. WebDataset also is very useful for such smaller datasets, and it can easily be used for developing and testing on small datasets and then scaling up to large datasets by simply using more shards. Here are different usage scenarios: environment caching strategy cloud training against cloud buckets use WebDataset directly with cloud URLs on premises training with high performance store (e.g., AIStore) use WebDataset directly with storage URLs. prototyping, development, testing for large scale training copy a few shards to local disk OR use automatic shard caching OR use DBCache on premises training with slower object stores/networks use automatic shard caching or DBCache for entire dataset desktop deep learning, smaller dataset copy all shards to disk manually OR use automatic shard caching training with IterableDataset sources other than WebDataset use DBCache The upshot is: you can write a single I/O pipeline that works for both local and remote data, and for both small and large datasets, and you can fine-tune performance and take advantage of local storage by adding the cache_dir and DBCache options. Let's look at how these different methods work. Direct Copying of Shards Let's take the OpenImages dataset as an example; it's half a terabyte large. For development and testing, you may not want to download the entire dataset, but you may also not want to use the dataset remotely. With WebDataset, you can just download a small number of shards and use them during development. !test -f /tmp/openimages-train-000000.tar || curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar > /tmp/openimages-train-000000.tar dataset = wds.WebDataset(\"/tmp/openimages-train-000000.tar\") repr(next(iter(dataset)))[:200] \"{'__key__': 'e39871fd9fd74f55', 'jpg': b'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x01\\\\x01:\\\\x01:\\\\x00\\\\x00\\\\xff\\\\xdb\\\\x00C\\\\x00\\\\x06\\\\x04\\\\x05\\\\x06\\\\x05\\\\x04\\\\x06\\\\x06\\\\x05\\\\x06\\\\x07\\\\x07\\\\x06\\\\x08\\\\n\\\\x10\\\\n\\\\n\\\\t\\\\t\\\\n\\\\x14\\\\x0e\" Note that the WebDataset class works the same way on local files as it does on remote files. Furthermore, unlike other kinds of dataset formats and archive formats, downloaded datasets are immediately useful and don't need to be unpacked. Automatic Shard Caching Downloading a few shards manually is useful for development and testing. But WebDataset permits us to automate downloading and caching of shards. This is accomplished by giving a cache_dir argument to the WebDataset constructor. Note that caching happens in parallel with iterating through the dataset. This means that if you write a WebDataset-based I/O pipeline, training starts immediately; the training job does not have to wait for any shards to download first. Automatic shard caching is useful for distributing deep learning code, for academic computer labs, and for cloud computing. In this example, we make two passes through the dataset, using the cached version on the second pass. !rm -rf ./cache # just using one URL for demonstration url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" dataset = wds.WebDataset(url, cache_dir=\"./cache\") print(\"=== first pass\") for sample in dataset: pass print(\"=== second pass\") for i, sample in enumerate(dataset): for key, value in sample.items(): print(key, repr(value)[:50]) print() if i >= 3: break !ls -l ./cache [caching <webdataset.gopen.Pipe object at 0x7fe2832feaf0> at ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a.~2601956~ ] === first pass [done caching ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a ] [finished ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a] [opening cached ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a ] === second pass __key__ 'e39871fd9fd74f55' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01 json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli __key__ 'f18b91585c4d3f3e' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00 json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti __key__ 'ede6e66b2fb59aab' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00 json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti __key__ 'ed600d57fcee4f94' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01 json b'[{\"ImageID\": \"ed600d57fcee4f94\", \"Source\": \"acti total 987924 -rw-rw-r-- 1 tmb tmb 1011630080 Nov 2 09:44 9fd87fa8-d42e-3be4-a3a6-839de961b98a Using automatic shard caching, you end up with bit-identical copies of the original dataset in the local shard cache. By default, shards are named based on a MD5 checksum of their original URL. If you want to reuse the downloaded cached files, you can override the cache file naming with the cache_name= argument to WebDataset and DBCache . You can disable shard caching by setting the shard cache directory name to None . Automatic Sample Caching WebDataset also provides a way of caching training samples directly. This works with samples coming from any IterableDataset as input. The cache is stored in an SQLite3 database. Sample-based caching is implemented by the DBCache class. You specify a filename for the database and the maximum number of samples you want to cache. Samples will initially be read from the original IterableDataset, but after either the samples run out or the maximum number of samples has been reached, subsequently, samples will be served from the database cache stored on local disk. The database cache persists between invocations of the job. Automatic sample caching is useful for developing and testing deep learning jobs, as well as for caching data coming from slow IterableDataset sources, such as network-based database connections or other slower data sources. !rm -rf ./cache.db dataset = wds.WebDataset(url).compose(wds.DBCache, \"./cache.db\", 1000) print(\"=== first pass\") for sample in dataset: pass print(\"=== second pass\") for i, sample in enumerate(dataset): for key, value in sample.items(): print(key, repr(value)[:50]) print() if i >= 3: break !ls -l ./cache.db [DBCache opened ./cache.db size 1000 total 0] [DBCache total 0 size 1000 more caching] === first pass [DBCache finished caching total 1000 (size 1000)] [DBCache starting dbiter total 1000 size 1000] === second pass __key__ 'e39871fd9fd74f55' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01 json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli __key__ 'f18b91585c4d3f3e' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00 json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti __key__ 'ede6e66b2fb59aab' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00 json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti __key__ 'ed600d57fcee4f94' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01 json b'[{\"ImageID\": \"ed600d57fcee4f94\", \"Source\": \"acti -rw-r--r-- 1 tmb tmb 485199872 Nov 2 09:44 ./cache.db You can disable the cache by changing the cache file name to None . This makes it easy to enable/disable the cache for testing. Sample-based caching using DBCache gives you more flexibility than shard-based caching: you can cache before or after decoding and before or after data augmentation. However, unlike shard-based caching, the cache won't be considered \"complete\" until the number of cached samples requested have been cached. The DBCache class is primarily useful for testing, and for caching data that comes from IterableDataset sources other than WebDataset .","title":"Desktop Usage"},{"location":"desktop/#desktop-usage-and-caching","text":"WebDataset is an ideal solution for training on petascale datasets kept on high performance distributed data stores like AIStore, AWS/S3, and Google Cloud. Compared to data center GPU servers, desktop machines have much slower network connections, but training jobs on desktop machines often also use much smaller datasets. WebDataset also is very useful for such smaller datasets, and it can easily be used for developing and testing on small datasets and then scaling up to large datasets by simply using more shards. Here are different usage scenarios: environment caching strategy cloud training against cloud buckets use WebDataset directly with cloud URLs on premises training with high performance store (e.g., AIStore) use WebDataset directly with storage URLs. prototyping, development, testing for large scale training copy a few shards to local disk OR use automatic shard caching OR use DBCache on premises training with slower object stores/networks use automatic shard caching or DBCache for entire dataset desktop deep learning, smaller dataset copy all shards to disk manually OR use automatic shard caching training with IterableDataset sources other than WebDataset use DBCache The upshot is: you can write a single I/O pipeline that works for both local and remote data, and for both small and large datasets, and you can fine-tune performance and take advantage of local storage by adding the cache_dir and DBCache options. Let's look at how these different methods work.","title":"Desktop Usage and Caching"},{"location":"desktop/#direct-copying-of-shards","text":"Let's take the OpenImages dataset as an example; it's half a terabyte large. For development and testing, you may not want to download the entire dataset, but you may also not want to use the dataset remotely. With WebDataset, you can just download a small number of shards and use them during development. !test -f /tmp/openimages-train-000000.tar || curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar > /tmp/openimages-train-000000.tar dataset = wds.WebDataset(\"/tmp/openimages-train-000000.tar\") repr(next(iter(dataset)))[:200] \"{'__key__': 'e39871fd9fd74f55', 'jpg': b'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x01\\\\x01:\\\\x01:\\\\x00\\\\x00\\\\xff\\\\xdb\\\\x00C\\\\x00\\\\x06\\\\x04\\\\x05\\\\x06\\\\x05\\\\x04\\\\x06\\\\x06\\\\x05\\\\x06\\\\x07\\\\x07\\\\x06\\\\x08\\\\n\\\\x10\\\\n\\\\n\\\\t\\\\t\\\\n\\\\x14\\\\x0e\" Note that the WebDataset class works the same way on local files as it does on remote files. Furthermore, unlike other kinds of dataset formats and archive formats, downloaded datasets are immediately useful and don't need to be unpacked.","title":"Direct Copying of Shards"},{"location":"desktop/#automatic-shard-caching","text":"Downloading a few shards manually is useful for development and testing. But WebDataset permits us to automate downloading and caching of shards. This is accomplished by giving a cache_dir argument to the WebDataset constructor. Note that caching happens in parallel with iterating through the dataset. This means that if you write a WebDataset-based I/O pipeline, training starts immediately; the training job does not have to wait for any shards to download first. Automatic shard caching is useful for distributing deep learning code, for academic computer labs, and for cloud computing. In this example, we make two passes through the dataset, using the cached version on the second pass. !rm -rf ./cache # just using one URL for demonstration url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" dataset = wds.WebDataset(url, cache_dir=\"./cache\") print(\"=== first pass\") for sample in dataset: pass print(\"=== second pass\") for i, sample in enumerate(dataset): for key, value in sample.items(): print(key, repr(value)[:50]) print() if i >= 3: break !ls -l ./cache [caching <webdataset.gopen.Pipe object at 0x7fe2832feaf0> at ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a.~2601956~ ] === first pass [done caching ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a ] [finished ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a] [opening cached ./cache/9fd87fa8-d42e-3be4-a3a6-839de961b98a ] === second pass __key__ 'e39871fd9fd74f55' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01 json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli __key__ 'f18b91585c4d3f3e' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00 json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti __key__ 'ede6e66b2fb59aab' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00 json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti __key__ 'ed600d57fcee4f94' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01 json b'[{\"ImageID\": \"ed600d57fcee4f94\", \"Source\": \"acti total 987924 -rw-rw-r-- 1 tmb tmb 1011630080 Nov 2 09:44 9fd87fa8-d42e-3be4-a3a6-839de961b98a Using automatic shard caching, you end up with bit-identical copies of the original dataset in the local shard cache. By default, shards are named based on a MD5 checksum of their original URL. If you want to reuse the downloaded cached files, you can override the cache file naming with the cache_name= argument to WebDataset and DBCache . You can disable shard caching by setting the shard cache directory name to None .","title":"Automatic Shard Caching"},{"location":"desktop/#automatic-sample-caching","text":"WebDataset also provides a way of caching training samples directly. This works with samples coming from any IterableDataset as input. The cache is stored in an SQLite3 database. Sample-based caching is implemented by the DBCache class. You specify a filename for the database and the maximum number of samples you want to cache. Samples will initially be read from the original IterableDataset, but after either the samples run out or the maximum number of samples has been reached, subsequently, samples will be served from the database cache stored on local disk. The database cache persists between invocations of the job. Automatic sample caching is useful for developing and testing deep learning jobs, as well as for caching data coming from slow IterableDataset sources, such as network-based database connections or other slower data sources. !rm -rf ./cache.db dataset = wds.WebDataset(url).compose(wds.DBCache, \"./cache.db\", 1000) print(\"=== first pass\") for sample in dataset: pass print(\"=== second pass\") for i, sample in enumerate(dataset): for key, value in sample.items(): print(key, repr(value)[:50]) print() if i >= 3: break !ls -l ./cache.db [DBCache opened ./cache.db size 1000 total 0] [DBCache total 0 size 1000 more caching] === first pass [DBCache finished caching total 1000 (size 1000)] [DBCache starting dbiter total 1000 size 1000] === second pass __key__ 'e39871fd9fd74f55' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01 json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli __key__ 'f18b91585c4d3f3e' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00 json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti __key__ 'ede6e66b2fb59aab' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00 json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti __key__ 'ed600d57fcee4f94' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01 json b'[{\"ImageID\": \"ed600d57fcee4f94\", \"Source\": \"acti -rw-r--r-- 1 tmb tmb 485199872 Nov 2 09:44 ./cache.db You can disable the cache by changing the cache file name to None . This makes it easy to enable/disable the cache for testing. Sample-based caching using DBCache gives you more flexibility than shard-based caching: you can cache before or after decoding and before or after data augmentation. However, unlike shard-based caching, the cache won't be considered \"complete\" until the number of cached samples requested have been cached. The DBCache class is primarily useful for testing, and for caching data that comes from IterableDataset sources other than WebDataset .","title":"Automatic Sample Caching"},{"location":"falling-things-make-shards/","text":"%pylab inline Populating the interactive namespace from numpy and matplotlib import zipfile import tarfile import os.path import imageio import io import time import re import webdataset as wds Reading the ZipFile We're taking the data out of the original zip file without unpacking. src = zipfile.ZipFile(\"fat.zip\") files = [s for s in src.filelist if not s.is_dir()] bydir = {} for fname in files: dir = os.path.dirname(fname.filename) bydir.setdefault(dir, []).append(fname) len(bydir.keys()) 330 Creating the Tar Files The Falling Things dataset is a collection of videos of falling things. However, there are different ways in which it can be used to create training samples: each frame is a training sample, presented in random order applications: pose estimation, stereo, monocular depth a small sequence of frames makes up a training sample applications: optical flow, motion segmentation, frame interpolatino each full sequence is a training sample (only 330 training samples) applications: physical modeling, long time tracking For random access datasets, the details of how data is broken up into training samples usually is hidden in the input pipeline. When using large scale, sequential training, this happens in a separate step while we generate training data sets. Note also that videos in this dataset are not represented as video files but as sequences of frames, distinguished by filename. We are going to do the same thing in the WebDataset representation. We first generate a full sequence dataset with a special structure of one shard per video: one .tar file per directory in the ZIP file duplicate the camera and object settings for each sample (so that we can later shuffle) one sample (=basename) per frame This is a valid WebDataset, but it also still preserves the video data in sequence within each shard, giving us different processing options. !rm -rf sequences shuffled !mkdir sequences shuffled def addfile(dst, name, data): assert isinstance(name, str), type(name) info = tarfile.TarInfo(name=name) info.size = len(data) info.uname, info.gname = \"bigdata\", \"bigdata\" info.mtime = time.time() dst.addfile(info, fileobj=io.BytesIO(data)) count = 0 for dir in sorted(bydir.keys()): tname = f\"sequences/falling-things-{count:06d}.tar\" nfiles = 0 with tarfile.open(tname, \"w|\") as dst: fnames = bydir[dir] print(count, tname, dir) camera_settings = [s for s in fnames if \"_camera_settings\" in s.filename][0] camera_settings = src.open(camera_settings, \"r\").read() object_settings = [s for s in fnames if \"_object_settings\" in s.filename][0] object_settings = src.open(object_settings, \"r\").read() last_base = \"NONE\" for finfo in sorted(fnames, key=lambda x: x.filename): fname = finfo.filename if fname.startswith(\"_\"): continue base = re.sub(r\"\\..*$\", \"\", os.path.basename(fname)) if base != last_base: #print(\"base:\", base) addfile(dst, dir+\"/\"+base+\".camera.json\", camera_settings) addfile(dst, dir+\"/\"+base+\".object.json\", object_settings) last_base = base with src.open(fname, \"r\") as stream: data = stream.read() addfile(dst, fname, data) nfiles += 1 #break count += 1 0 sequences/falling-things-000000.tar fat/mixed/kitchen_0 1 sequences/falling-things-000001.tar fat/mixed/kitchen_1 2 sequences/falling-things-000002.tar fat/mixed/kitchen_2 3 sequences/falling-things-000003.tar fat/mixed/kitchen_3 4 sequences/falling-things-000004.tar fat/mixed/kitchen_4 5 sequences/falling-things-000005.tar fat/mixed/kitedemo_0 6 sequences/falling-things-000006.tar fat/mixed/kitedemo_1 7 sequences/falling-things-000007.tar fat/mixed/kitedemo_2 8 sequences/falling-things-000008.tar fat/mixed/kitedemo_3 9 sequences/falling-things-000009.tar fat/mixed/kitedemo_4 10 sequences/falling-things-000010.tar fat/mixed/temple_0 11 sequences/falling-things-000011.tar fat/mixed/temple_1 12 sequences/falling-things-000012.tar fat/mixed/temple_2 13 sequences/falling-things-000013.tar fat/mixed/temple_3 14 sequences/falling-things-000014.tar fat/mixed/temple_4 15 sequences/falling-things-000015.tar fat/single/002_master_chef_can_16k/kitchen_0 16 sequences/falling-things-000016.tar fat/single/002_master_chef_can_16k/kitchen_1 17 sequences/falling-things-000017.tar fat/single/002_master_chef_can_16k/kitchen_2 18 sequences/falling-things-000018.tar fat/single/002_master_chef_can_16k/kitchen_3 19 sequences/falling-things-000019.tar fat/single/002_master_chef_can_16k/kitchen_4 20 sequences/falling-things-000020.tar fat/single/002_master_chef_can_16k/kitedemo_0 21 sequences/falling-things-000021.tar fat/single/002_master_chef_can_16k/kitedemo_1 22 sequences/falling-things-000022.tar fat/single/002_master_chef_can_16k/kitedemo_2 23 sequences/falling-things-000023.tar fat/single/002_master_chef_can_16k/kitedemo_3 24 sequences/falling-things-000024.tar fat/single/002_master_chef_can_16k/kitedemo_4 25 sequences/falling-things-000025.tar fat/single/002_master_chef_can_16k/temple_0 26 sequences/falling-things-000026.tar fat/single/002_master_chef_can_16k/temple_1 27 sequences/falling-things-000027.tar fat/single/002_master_chef_can_16k/temple_2 28 sequences/falling-things-000028.tar fat/single/002_master_chef_can_16k/temple_3 29 sequences/falling-things-000029.tar fat/single/002_master_chef_can_16k/temple_4 30 sequences/falling-things-000030.tar fat/single/003_cracker_box_16k/kitchen_0 31 sequences/falling-things-000031.tar fat/single/003_cracker_box_16k/kitchen_1 32 sequences/falling-things-000032.tar fat/single/003_cracker_box_16k/kitchen_2 33 sequences/falling-things-000033.tar fat/single/003_cracker_box_16k/kitchen_3 34 sequences/falling-things-000034.tar fat/single/003_cracker_box_16k/kitchen_4 35 sequences/falling-things-000035.tar fat/single/003_cracker_box_16k/kitedemo_0 36 sequences/falling-things-000036.tar fat/single/003_cracker_box_16k/kitedemo_1 37 sequences/falling-things-000037.tar fat/single/003_cracker_box_16k/kitedemo_2 38 sequences/falling-things-000038.tar fat/single/003_cracker_box_16k/kitedemo_3 39 sequences/falling-things-000039.tar fat/single/003_cracker_box_16k/kitedemo_4 40 sequences/falling-things-000040.tar fat/single/003_cracker_box_16k/temple_0 41 sequences/falling-things-000041.tar fat/single/003_cracker_box_16k/temple_1 42 sequences/falling-things-000042.tar fat/single/003_cracker_box_16k/temple_2 43 sequences/falling-things-000043.tar fat/single/003_cracker_box_16k/temple_3 44 sequences/falling-things-000044.tar fat/single/003_cracker_box_16k/temple_4 45 sequences/falling-things-000045.tar fat/single/004_sugar_box_16k/kitchen_0 46 sequences/falling-things-000046.tar fat/single/004_sugar_box_16k/kitchen_1 47 sequences/falling-things-000047.tar fat/single/004_sugar_box_16k/kitchen_2 48 sequences/falling-things-000048.tar fat/single/004_sugar_box_16k/kitchen_3 49 sequences/falling-things-000049.tar fat/single/004_sugar_box_16k/kitchen_4 50 sequences/falling-things-000050.tar fat/single/004_sugar_box_16k/kitedemo_0 51 sequences/falling-things-000051.tar fat/single/004_sugar_box_16k/kitedemo_1 52 sequences/falling-things-000052.tar fat/single/004_sugar_box_16k/kitedemo_2 53 sequences/falling-things-000053.tar fat/single/004_sugar_box_16k/kitedemo_3 54 sequences/falling-things-000054.tar fat/single/004_sugar_box_16k/kitedemo_4 55 sequences/falling-things-000055.tar fat/single/004_sugar_box_16k/temple_0 56 sequences/falling-things-000056.tar fat/single/004_sugar_box_16k/temple_1 57 sequences/falling-things-000057.tar fat/single/004_sugar_box_16k/temple_2 58 sequences/falling-things-000058.tar fat/single/004_sugar_box_16k/temple_3 59 sequences/falling-things-000059.tar fat/single/004_sugar_box_16k/temple_4 60 sequences/falling-things-000060.tar fat/single/005_tomato_soup_can_16k/kitchen_0 61 sequences/falling-things-000061.tar fat/single/005_tomato_soup_can_16k/kitchen_1 62 sequences/falling-things-000062.tar fat/single/005_tomato_soup_can_16k/kitchen_2 63 sequences/falling-things-000063.tar fat/single/005_tomato_soup_can_16k/kitchen_3 64 sequences/falling-things-000064.tar fat/single/005_tomato_soup_can_16k/kitchen_4 65 sequences/falling-things-000065.tar fat/single/005_tomato_soup_can_16k/kitedemo_0 66 sequences/falling-things-000066.tar fat/single/005_tomato_soup_can_16k/kitedemo_1 67 sequences/falling-things-000067.tar fat/single/005_tomato_soup_can_16k/kitedemo_2 68 sequences/falling-things-000068.tar fat/single/005_tomato_soup_can_16k/kitedemo_3 69 sequences/falling-things-000069.tar fat/single/005_tomato_soup_can_16k/kitedemo_4 70 sequences/falling-things-000070.tar fat/single/005_tomato_soup_can_16k/temple_0 71 sequences/falling-things-000071.tar fat/single/005_tomato_soup_can_16k/temple_1 72 sequences/falling-things-000072.tar fat/single/005_tomato_soup_can_16k/temple_2 73 sequences/falling-things-000073.tar fat/single/005_tomato_soup_can_16k/temple_3 74 sequences/falling-things-000074.tar fat/single/005_tomato_soup_can_16k/temple_4 75 sequences/falling-things-000075.tar fat/single/006_mustard_bottle_16k/kitchen_0 76 sequences/falling-things-000076.tar fat/single/006_mustard_bottle_16k/kitchen_1 77 sequences/falling-things-000077.tar fat/single/006_mustard_bottle_16k/kitchen_2 78 sequences/falling-things-000078.tar fat/single/006_mustard_bottle_16k/kitchen_3 79 sequences/falling-things-000079.tar fat/single/006_mustard_bottle_16k/kitchen_4 80 sequences/falling-things-000080.tar fat/single/006_mustard_bottle_16k/kitedemo_0 81 sequences/falling-things-000081.tar fat/single/006_mustard_bottle_16k/kitedemo_1 82 sequences/falling-things-000082.tar fat/single/006_mustard_bottle_16k/kitedemo_2 83 sequences/falling-things-000083.tar fat/single/006_mustard_bottle_16k/kitedemo_3 84 sequences/falling-things-000084.tar fat/single/006_mustard_bottle_16k/kitedemo_4 85 sequences/falling-things-000085.tar fat/single/006_mustard_bottle_16k/temple_0 86 sequences/falling-things-000086.tar fat/single/006_mustard_bottle_16k/temple_1 87 sequences/falling-things-000087.tar fat/single/006_mustard_bottle_16k/temple_2 88 sequences/falling-things-000088.tar fat/single/006_mustard_bottle_16k/temple_3 89 sequences/falling-things-000089.tar fat/single/006_mustard_bottle_16k/temple_4 90 sequences/falling-things-000090.tar fat/single/007_tuna_fish_can_16k/kitchen_0 91 sequences/falling-things-000091.tar fat/single/007_tuna_fish_can_16k/kitchen_1 92 sequences/falling-things-000092.tar fat/single/007_tuna_fish_can_16k/kitchen_2 93 sequences/falling-things-000093.tar fat/single/007_tuna_fish_can_16k/kitchen_3 94 sequences/falling-things-000094.tar fat/single/007_tuna_fish_can_16k/kitchen_4 95 sequences/falling-things-000095.tar fat/single/007_tuna_fish_can_16k/kitedemo_0 96 sequences/falling-things-000096.tar fat/single/007_tuna_fish_can_16k/kitedemo_1 97 sequences/falling-things-000097.tar fat/single/007_tuna_fish_can_16k/kitedemo_2 98 sequences/falling-things-000098.tar fat/single/007_tuna_fish_can_16k/kitedemo_3 99 sequences/falling-things-000099.tar fat/single/007_tuna_fish_can_16k/kitedemo_4 100 sequences/falling-things-000100.tar fat/single/007_tuna_fish_can_16k/temple_0 101 sequences/falling-things-000101.tar fat/single/007_tuna_fish_can_16k/temple_1 102 sequences/falling-things-000102.tar fat/single/007_tuna_fish_can_16k/temple_2 103 sequences/falling-things-000103.tar fat/single/007_tuna_fish_can_16k/temple_3 104 sequences/falling-things-000104.tar fat/single/007_tuna_fish_can_16k/temple_4 105 sequences/falling-things-000105.tar fat/single/008_pudding_box_16k/kitchen_0 106 sequences/falling-things-000106.tar fat/single/008_pudding_box_16k/kitchen_1 107 sequences/falling-things-000107.tar fat/single/008_pudding_box_16k/kitchen_2 108 sequences/falling-things-000108.tar fat/single/008_pudding_box_16k/kitchen_3 109 sequences/falling-things-000109.tar fat/single/008_pudding_box_16k/kitchen_4 110 sequences/falling-things-000110.tar fat/single/008_pudding_box_16k/kitedemo_0 111 sequences/falling-things-000111.tar fat/single/008_pudding_box_16k/kitedemo_1 112 sequences/falling-things-000112.tar fat/single/008_pudding_box_16k/kitedemo_2 113 sequences/falling-things-000113.tar fat/single/008_pudding_box_16k/kitedemo_3 114 sequences/falling-things-000114.tar fat/single/008_pudding_box_16k/kitedemo_4 115 sequences/falling-things-000115.tar fat/single/008_pudding_box_16k/temple_0 116 sequences/falling-things-000116.tar fat/single/008_pudding_box_16k/temple_1 117 sequences/falling-things-000117.tar fat/single/008_pudding_box_16k/temple_2 118 sequences/falling-things-000118.tar fat/single/008_pudding_box_16k/temple_3 119 sequences/falling-things-000119.tar fat/single/008_pudding_box_16k/temple_4 120 sequences/falling-things-000120.tar fat/single/009_gelatin_box_16k/kitchen_0 121 sequences/falling-things-000121.tar fat/single/009_gelatin_box_16k/kitchen_1 122 sequences/falling-things-000122.tar fat/single/009_gelatin_box_16k/kitchen_2 123 sequences/falling-things-000123.tar fat/single/009_gelatin_box_16k/kitchen_3 124 sequences/falling-things-000124.tar fat/single/009_gelatin_box_16k/kitchen_4 125 sequences/falling-things-000125.tar fat/single/009_gelatin_box_16k/kitedemo_0 126 sequences/falling-things-000126.tar fat/single/009_gelatin_box_16k/kitedemo_1 127 sequences/falling-things-000127.tar fat/single/009_gelatin_box_16k/kitedemo_2 128 sequences/falling-things-000128.tar fat/single/009_gelatin_box_16k/kitedemo_3 129 sequences/falling-things-000129.tar fat/single/009_gelatin_box_16k/kitedemo_4 130 sequences/falling-things-000130.tar fat/single/009_gelatin_box_16k/temple_0 131 sequences/falling-things-000131.tar fat/single/009_gelatin_box_16k/temple_1 132 sequences/falling-things-000132.tar fat/single/009_gelatin_box_16k/temple_2 133 sequences/falling-things-000133.tar fat/single/009_gelatin_box_16k/temple_3 134 sequences/falling-things-000134.tar fat/single/009_gelatin_box_16k/temple_4 135 sequences/falling-things-000135.tar fat/single/010_potted_meat_can_16k/kitchen_0 136 sequences/falling-things-000136.tar fat/single/010_potted_meat_can_16k/kitchen_1 137 sequences/falling-things-000137.tar fat/single/010_potted_meat_can_16k/kitchen_2 138 sequences/falling-things-000138.tar fat/single/010_potted_meat_can_16k/kitchen_3 139 sequences/falling-things-000139.tar fat/single/010_potted_meat_can_16k/kitchen_4 140 sequences/falling-things-000140.tar fat/single/010_potted_meat_can_16k/kitedemo_0 141 sequences/falling-things-000141.tar fat/single/010_potted_meat_can_16k/kitedemo_1 142 sequences/falling-things-000142.tar fat/single/010_potted_meat_can_16k/kitedemo_2 143 sequences/falling-things-000143.tar fat/single/010_potted_meat_can_16k/kitedemo_3 144 sequences/falling-things-000144.tar fat/single/010_potted_meat_can_16k/kitedemo_4 145 sequences/falling-things-000145.tar fat/single/010_potted_meat_can_16k/temple_0 146 sequences/falling-things-000146.tar fat/single/010_potted_meat_can_16k/temple_1 147 sequences/falling-things-000147.tar fat/single/010_potted_meat_can_16k/temple_2 148 sequences/falling-things-000148.tar fat/single/010_potted_meat_can_16k/temple_3 149 sequences/falling-things-000149.tar fat/single/010_potted_meat_can_16k/temple_4 150 sequences/falling-things-000150.tar fat/single/011_banana_16k/kitchen_0 151 sequences/falling-things-000151.tar fat/single/011_banana_16k/kitchen_1 152 sequences/falling-things-000152.tar fat/single/011_banana_16k/kitchen_2 153 sequences/falling-things-000153.tar fat/single/011_banana_16k/kitchen_3 154 sequences/falling-things-000154.tar fat/single/011_banana_16k/kitchen_4 155 sequences/falling-things-000155.tar fat/single/011_banana_16k/kitedemo_0 156 sequences/falling-things-000156.tar fat/single/011_banana_16k/kitedemo_1 157 sequences/falling-things-000157.tar fat/single/011_banana_16k/kitedemo_2 158 sequences/falling-things-000158.tar fat/single/011_banana_16k/kitedemo_3 159 sequences/falling-things-000159.tar fat/single/011_banana_16k/kitedemo_4 160 sequences/falling-things-000160.tar fat/single/011_banana_16k/temple_0 161 sequences/falling-things-000161.tar fat/single/011_banana_16k/temple_1 162 sequences/falling-things-000162.tar fat/single/011_banana_16k/temple_2 163 sequences/falling-things-000163.tar fat/single/011_banana_16k/temple_3 164 sequences/falling-things-000164.tar fat/single/011_banana_16k/temple_4 165 sequences/falling-things-000165.tar fat/single/019_pitcher_base_16k/kitchen_0 166 sequences/falling-things-000166.tar fat/single/019_pitcher_base_16k/kitchen_1 167 sequences/falling-things-000167.tar fat/single/019_pitcher_base_16k/kitchen_2 168 sequences/falling-things-000168.tar fat/single/019_pitcher_base_16k/kitchen_3 169 sequences/falling-things-000169.tar fat/single/019_pitcher_base_16k/kitchen_4 170 sequences/falling-things-000170.tar fat/single/019_pitcher_base_16k/kitedemo_0 171 sequences/falling-things-000171.tar fat/single/019_pitcher_base_16k/kitedemo_1 172 sequences/falling-things-000172.tar fat/single/019_pitcher_base_16k/kitedemo_2 173 sequences/falling-things-000173.tar fat/single/019_pitcher_base_16k/kitedemo_3 174 sequences/falling-things-000174.tar fat/single/019_pitcher_base_16k/kitedemo_4 175 sequences/falling-things-000175.tar fat/single/019_pitcher_base_16k/temple_0 176 sequences/falling-things-000176.tar fat/single/019_pitcher_base_16k/temple_1 177 sequences/falling-things-000177.tar fat/single/019_pitcher_base_16k/temple_2 178 sequences/falling-things-000178.tar fat/single/019_pitcher_base_16k/temple_3 179 sequences/falling-things-000179.tar fat/single/019_pitcher_base_16k/temple_4 180 sequences/falling-things-000180.tar fat/single/021_bleach_cleanser_16k/kitchen_0 181 sequences/falling-things-000181.tar fat/single/021_bleach_cleanser_16k/kitchen_1 182 sequences/falling-things-000182.tar fat/single/021_bleach_cleanser_16k/kitchen_2 183 sequences/falling-things-000183.tar fat/single/021_bleach_cleanser_16k/kitchen_3 184 sequences/falling-things-000184.tar fat/single/021_bleach_cleanser_16k/kitchen_4 185 sequences/falling-things-000185.tar fat/single/021_bleach_cleanser_16k/kitedemo_0 186 sequences/falling-things-000186.tar fat/single/021_bleach_cleanser_16k/kitedemo_1 187 sequences/falling-things-000187.tar fat/single/021_bleach_cleanser_16k/kitedemo_2 188 sequences/falling-things-000188.tar fat/single/021_bleach_cleanser_16k/kitedemo_3 189 sequences/falling-things-000189.tar fat/single/021_bleach_cleanser_16k/kitedemo_4 190 sequences/falling-things-000190.tar fat/single/021_bleach_cleanser_16k/temple_0 191 sequences/falling-things-000191.tar fat/single/021_bleach_cleanser_16k/temple_1 192 sequences/falling-things-000192.tar fat/single/021_bleach_cleanser_16k/temple_2 193 sequences/falling-things-000193.tar fat/single/021_bleach_cleanser_16k/temple_3 194 sequences/falling-things-000194.tar fat/single/021_bleach_cleanser_16k/temple_4 195 sequences/falling-things-000195.tar fat/single/024_bowl_16k/kitchen_0 196 sequences/falling-things-000196.tar fat/single/024_bowl_16k/kitchen_1 197 sequences/falling-things-000197.tar fat/single/024_bowl_16k/kitchen_2 198 sequences/falling-things-000198.tar fat/single/024_bowl_16k/kitchen_3 199 sequences/falling-things-000199.tar fat/single/024_bowl_16k/kitchen_4 200 sequences/falling-things-000200.tar fat/single/024_bowl_16k/kitedemo_0 201 sequences/falling-things-000201.tar fat/single/024_bowl_16k/kitedemo_1 202 sequences/falling-things-000202.tar fat/single/024_bowl_16k/kitedemo_2 203 sequences/falling-things-000203.tar fat/single/024_bowl_16k/kitedemo_3 204 sequences/falling-things-000204.tar fat/single/024_bowl_16k/kitedemo_4 205 sequences/falling-things-000205.tar fat/single/024_bowl_16k/temple_0 206 sequences/falling-things-000206.tar fat/single/024_bowl_16k/temple_1 207 sequences/falling-things-000207.tar fat/single/024_bowl_16k/temple_2 208 sequences/falling-things-000208.tar fat/single/024_bowl_16k/temple_3 209 sequences/falling-things-000209.tar fat/single/024_bowl_16k/temple_4 210 sequences/falling-things-000210.tar fat/single/025_mug_16k/kitchen_0 211 sequences/falling-things-000211.tar fat/single/025_mug_16k/kitchen_1 212 sequences/falling-things-000212.tar fat/single/025_mug_16k/kitchen_2 213 sequences/falling-things-000213.tar fat/single/025_mug_16k/kitchen_3 214 sequences/falling-things-000214.tar fat/single/025_mug_16k/kitchen_4 215 sequences/falling-things-000215.tar fat/single/025_mug_16k/kitedemo_0 216 sequences/falling-things-000216.tar fat/single/025_mug_16k/kitedemo_1 217 sequences/falling-things-000217.tar fat/single/025_mug_16k/kitedemo_2 218 sequences/falling-things-000218.tar fat/single/025_mug_16k/kitedemo_3 219 sequences/falling-things-000219.tar fat/single/025_mug_16k/kitedemo_4 220 sequences/falling-things-000220.tar fat/single/025_mug_16k/temple_0 221 sequences/falling-things-000221.tar fat/single/025_mug_16k/temple_1 222 sequences/falling-things-000222.tar fat/single/025_mug_16k/temple_2 223 sequences/falling-things-000223.tar fat/single/025_mug_16k/temple_3 224 sequences/falling-things-000224.tar fat/single/025_mug_16k/temple_4 225 sequences/falling-things-000225.tar fat/single/035_power_drill_16k/kitchen_0 226 sequences/falling-things-000226.tar fat/single/035_power_drill_16k/kitchen_1 227 sequences/falling-things-000227.tar fat/single/035_power_drill_16k/kitchen_2 228 sequences/falling-things-000228.tar fat/single/035_power_drill_16k/kitchen_3 229 sequences/falling-things-000229.tar fat/single/035_power_drill_16k/kitchen_4 230 sequences/falling-things-000230.tar fat/single/035_power_drill_16k/kitedemo_0 231 sequences/falling-things-000231.tar fat/single/035_power_drill_16k/kitedemo_1 232 sequences/falling-things-000232.tar fat/single/035_power_drill_16k/kitedemo_2 233 sequences/falling-things-000233.tar fat/single/035_power_drill_16k/kitedemo_3 234 sequences/falling-things-000234.tar fat/single/035_power_drill_16k/kitedemo_4 235 sequences/falling-things-000235.tar fat/single/035_power_drill_16k/temple_0 236 sequences/falling-things-000236.tar fat/single/035_power_drill_16k/temple_1 237 sequences/falling-things-000237.tar fat/single/035_power_drill_16k/temple_2 238 sequences/falling-things-000238.tar fat/single/035_power_drill_16k/temple_3 239 sequences/falling-things-000239.tar fat/single/035_power_drill_16k/temple_4 240 sequences/falling-things-000240.tar fat/single/036_wood_block_16k/kitchen_0 241 sequences/falling-things-000241.tar fat/single/036_wood_block_16k/kitchen_1 242 sequences/falling-things-000242.tar fat/single/036_wood_block_16k/kitchen_2 243 sequences/falling-things-000243.tar fat/single/036_wood_block_16k/kitchen_3 244 sequences/falling-things-000244.tar fat/single/036_wood_block_16k/kitchen_4 245 sequences/falling-things-000245.tar fat/single/036_wood_block_16k/kitedemo_0 246 sequences/falling-things-000246.tar fat/single/036_wood_block_16k/kitedemo_1 247 sequences/falling-things-000247.tar fat/single/036_wood_block_16k/kitedemo_2 248 sequences/falling-things-000248.tar fat/single/036_wood_block_16k/kitedemo_3 249 sequences/falling-things-000249.tar fat/single/036_wood_block_16k/kitedemo_4 250 sequences/falling-things-000250.tar fat/single/036_wood_block_16k/temple_0 251 sequences/falling-things-000251.tar fat/single/036_wood_block_16k/temple_1 252 sequences/falling-things-000252.tar fat/single/036_wood_block_16k/temple_2 253 sequences/falling-things-000253.tar fat/single/036_wood_block_16k/temple_3 254 sequences/falling-things-000254.tar fat/single/036_wood_block_16k/temple_4 255 sequences/falling-things-000255.tar fat/single/037_scissors_16k/kitchen_0 256 sequences/falling-things-000256.tar fat/single/037_scissors_16k/kitchen_1 257 sequences/falling-things-000257.tar fat/single/037_scissors_16k/kitchen_2 258 sequences/falling-things-000258.tar fat/single/037_scissors_16k/kitchen_3 259 sequences/falling-things-000259.tar fat/single/037_scissors_16k/kitchen_4 260 sequences/falling-things-000260.tar fat/single/037_scissors_16k/kitedemo_0 261 sequences/falling-things-000261.tar fat/single/037_scissors_16k/kitedemo_1 262 sequences/falling-things-000262.tar fat/single/037_scissors_16k/kitedemo_2 263 sequences/falling-things-000263.tar fat/single/037_scissors_16k/kitedemo_3 264 sequences/falling-things-000264.tar fat/single/037_scissors_16k/kitedemo_4 265 sequences/falling-things-000265.tar fat/single/037_scissors_16k/temple_0 266 sequences/falling-things-000266.tar fat/single/037_scissors_16k/temple_1 267 sequences/falling-things-000267.tar fat/single/037_scissors_16k/temple_2 268 sequences/falling-things-000268.tar fat/single/037_scissors_16k/temple_3 269 sequences/falling-things-000269.tar fat/single/037_scissors_16k/temple_4 270 sequences/falling-things-000270.tar fat/single/040_large_marker_16k/kitchen_0 271 sequences/falling-things-000271.tar fat/single/040_large_marker_16k/kitchen_1 272 sequences/falling-things-000272.tar fat/single/040_large_marker_16k/kitchen_2 273 sequences/falling-things-000273.tar fat/single/040_large_marker_16k/kitchen_3 274 sequences/falling-things-000274.tar fat/single/040_large_marker_16k/kitchen_4 275 sequences/falling-things-000275.tar fat/single/040_large_marker_16k/kitedemo_0 276 sequences/falling-things-000276.tar fat/single/040_large_marker_16k/kitedemo_1 277 sequences/falling-things-000277.tar fat/single/040_large_marker_16k/kitedemo_2 278 sequences/falling-things-000278.tar fat/single/040_large_marker_16k/kitedemo_3 279 sequences/falling-things-000279.tar fat/single/040_large_marker_16k/kitedemo_4 280 sequences/falling-things-000280.tar fat/single/040_large_marker_16k/temple_0 281 sequences/falling-things-000281.tar fat/single/040_large_marker_16k/temple_1 282 sequences/falling-things-000282.tar fat/single/040_large_marker_16k/temple_2 283 sequences/falling-things-000283.tar fat/single/040_large_marker_16k/temple_3 284 sequences/falling-things-000284.tar fat/single/040_large_marker_16k/temple_4 285 sequences/falling-things-000285.tar fat/single/051_large_clamp_16k/kitchen_0 286 sequences/falling-things-000286.tar fat/single/051_large_clamp_16k/kitchen_1 287 sequences/falling-things-000287.tar fat/single/051_large_clamp_16k/kitchen_2 288 sequences/falling-things-000288.tar fat/single/051_large_clamp_16k/kitchen_3 289 sequences/falling-things-000289.tar fat/single/051_large_clamp_16k/kitchen_4 290 sequences/falling-things-000290.tar fat/single/051_large_clamp_16k/kitedemo_0 291 sequences/falling-things-000291.tar fat/single/051_large_clamp_16k/kitedemo_1 292 sequences/falling-things-000292.tar fat/single/051_large_clamp_16k/kitedemo_2 293 sequences/falling-things-000293.tar fat/single/051_large_clamp_16k/kitedemo_3 294 sequences/falling-things-000294.tar fat/single/051_large_clamp_16k/kitedemo_4 295 sequences/falling-things-000295.tar fat/single/051_large_clamp_16k/temple_0 296 sequences/falling-things-000296.tar fat/single/051_large_clamp_16k/temple_1 297 sequences/falling-things-000297.tar fat/single/051_large_clamp_16k/temple_2 298 sequences/falling-things-000298.tar fat/single/051_large_clamp_16k/temple_3 299 sequences/falling-things-000299.tar fat/single/051_large_clamp_16k/temple_4 300 sequences/falling-things-000300.tar fat/single/052_extra_large_clamp_16k/kitchen_0 301 sequences/falling-things-000301.tar fat/single/052_extra_large_clamp_16k/kitchen_1 302 sequences/falling-things-000302.tar fat/single/052_extra_large_clamp_16k/kitchen_2 303 sequences/falling-things-000303.tar fat/single/052_extra_large_clamp_16k/kitchen_3 304 sequences/falling-things-000304.tar fat/single/052_extra_large_clamp_16k/kitchen_4 305 sequences/falling-things-000305.tar fat/single/052_extra_large_clamp_16k/kitedemo_0 306 sequences/falling-things-000306.tar fat/single/052_extra_large_clamp_16k/kitedemo_1 307 sequences/falling-things-000307.tar fat/single/052_extra_large_clamp_16k/kitedemo_2 308 sequences/falling-things-000308.tar fat/single/052_extra_large_clamp_16k/kitedemo_3 309 sequences/falling-things-000309.tar fat/single/052_extra_large_clamp_16k/kitedemo_4 310 sequences/falling-things-000310.tar fat/single/052_extra_large_clamp_16k/temple_0 311 sequences/falling-things-000311.tar fat/single/052_extra_large_clamp_16k/temple_1 312 sequences/falling-things-000312.tar fat/single/052_extra_large_clamp_16k/temple_2 313 sequences/falling-things-000313.tar fat/single/052_extra_large_clamp_16k/temple_3 314 sequences/falling-things-000314.tar fat/single/052_extra_large_clamp_16k/temple_4 315 sequences/falling-things-000315.tar fat/single/061_foam_brick_16k/kitchen_0 316 sequences/falling-things-000316.tar fat/single/061_foam_brick_16k/kitchen_1 317 sequences/falling-things-000317.tar fat/single/061_foam_brick_16k/kitchen_2 318 sequences/falling-things-000318.tar fat/single/061_foam_brick_16k/kitchen_3 319 sequences/falling-things-000319.tar fat/single/061_foam_brick_16k/kitchen_4 320 sequences/falling-things-000320.tar fat/single/061_foam_brick_16k/kitedemo_0 321 sequences/falling-things-000321.tar fat/single/061_foam_brick_16k/kitedemo_1 322 sequences/falling-things-000322.tar fat/single/061_foam_brick_16k/kitedemo_2 323 sequences/falling-things-000323.tar fat/single/061_foam_brick_16k/kitedemo_3 324 sequences/falling-things-000324.tar fat/single/061_foam_brick_16k/kitedemo_4 325 sequences/falling-things-000325.tar fat/single/061_foam_brick_16k/temple_0 326 sequences/falling-things-000326.tar fat/single/061_foam_brick_16k/temple_1 327 sequences/falling-things-000327.tar fat/single/061_foam_brick_16k/temple_2 328 sequences/falling-things-000328.tar fat/single/061_foam_brick_16k/temple_3 329 sequences/falling-things-000329.tar fat/single/061_foam_brick_16k/temple_4 !ls -lth sequences/*.tar | head -rw-rw-r-- 1 tmb tmb 46M Aug 30 19:02 sequences/falling-things-000329.tar -rw-rw-r-- 1 tmb tmb 46M Aug 30 19:02 sequences/falling-things-000328.tar -rw-rw-r-- 1 tmb tmb 26M Aug 30 19:02 sequences/falling-things-000327.tar -rw-rw-r-- 1 tmb tmb 32M Aug 30 19:02 sequences/falling-things-000326.tar -rw-rw-r-- 1 tmb tmb 33M Aug 30 19:02 sequences/falling-things-000325.tar -rw-rw-r-- 1 tmb tmb 155M Aug 30 19:02 sequences/falling-things-000324.tar -rw-rw-r-- 1 tmb tmb 89M Aug 30 19:02 sequences/falling-things-000323.tar -rw-rw-r-- 1 tmb tmb 165M Aug 30 19:02 sequences/falling-things-000322.tar -rw-rw-r-- 1 tmb tmb 90M Aug 30 19:01 sequences/falling-things-000321.tar -rw-rw-r-- 1 tmb tmb 99M Aug 30 19:01 sequences/falling-things-000320.tar ls: write error: Broken pipe !tar tf sequences/falling-things-000000.tar | head fat/mixed/kitchen_0/000000.camera.json fat/mixed/kitchen_0/000000.object.json fat/mixed/kitchen_0/000000.left.depth.png fat/mixed/kitchen_0/000000.left.jpg fat/mixed/kitchen_0/000000.left.json fat/mixed/kitchen_0/000000.left.seg.png fat/mixed/kitchen_0/000000.right.depth.png fat/mixed/kitchen_0/000000.right.jpg fat/mixed/kitchen_0/000000.right.json fat/mixed/kitchen_0/000000.right.seg.png tar: write error Frame-Level Training To generate frame level training, we can simply shuffle the sequence data. For this to work, it is important that we associated the sequence level information (camera, object) with each frame, as we did in the construction of the sequence dataset. We can now shuffle with: $ tarp cat -m 5 -s 500 -o - by-dir/*.tar | tarp split - -o shuffled/temp-%06d.tar $ tarp cat -m 10 -s 1000 -o - shuffled/temp-*.tar | tarp split - -o shuffled/falling-things-shuffled-%06d.tar If your machine has more memory, you can adjust the -m and -s options. %%bash tarp cat -m 5 -s 500 -o - sequences/*.tar | tarp split - -o shuffled/temp-%06d.tar && tarp cat -m 10 -s 1000 -o - shuffled/temp-*.tar | tarp split - -o shuffled/falling-things-shuffled-%06d.tar && rm shuffled/temp-*.tar [info] # shuffle 500 [progress] # writing - [progress] # source - [progress] # shard shuffled/temp-000000.tar [progress] # shard shuffled/temp-000001.tar [progress] # shard shuffled/temp-000002.tar [progress] # shard shuffled/temp-000003.tar [progress] # shard shuffled/temp-000004.tar [progress] # shard shuffled/temp-000005.tar [progress] # shard shuffled/temp-000006.tar [progress] # shard shuffled/temp-000007.tar [progress] # shard shuffled/temp-000008.tar [progress] # shard shuffled/temp-000009.tar [progress] # shard shuffled/temp-000010.tar [progress] # shard shuffled/temp-000011.tar [progress] # shard shuffled/temp-000012.tar [progress] # shard shuffled/temp-000013.tar [progress] # shard shuffled/temp-000014.tar [progress] # shard shuffled/temp-000015.tar [progress] # shard shuffled/temp-000016.tar [progress] # shard shuffled/temp-000017.tar [progress] # shard shuffled/temp-000018.tar [progress] # shard shuffled/temp-000019.tar [progress] # shard shuffled/temp-000020.tar [progress] # shard shuffled/temp-000021.tar [progress] # shard shuffled/temp-000022.tar [progress] # shard shuffled/temp-000023.tar [progress] # shard shuffled/temp-000024.tar [progress] # shard shuffled/temp-000025.tar [progress] # shard shuffled/temp-000026.tar [progress] # shard shuffled/temp-000027.tar [progress] # shard shuffled/temp-000028.tar [progress] # shard shuffled/temp-000029.tar [progress] # shard shuffled/temp-000030.tar [progress] # shard shuffled/temp-000031.tar [progress] # shard shuffled/temp-000032.tar [progress] # shard shuffled/temp-000033.tar [progress] # shard shuffled/temp-000034.tar [progress] # shard shuffled/temp-000035.tar [progress] # shard shuffled/temp-000036.tar [progress] # shard shuffled/temp-000037.tar [progress] # shard shuffled/temp-000038.tar [progress] # shard shuffled/temp-000039.tar [progress] # shard shuffled/temp-000040.tar [progress] # shard shuffled/temp-000041.tar [progress] # shard shuffled/temp-000042.tar [progress] # shard shuffled/temp-000043.tar [progress] # shard shuffled/temp-000044.tar [progress] # shard shuffled/temp-000045.tar [progress] # shard shuffled/temp-000046.tar [progress] # source - [info] # shuffle 1000 [progress] # writing - [progress] # shard shuffled/falling-things-shuffled-000000.tar [progress] # shard shuffled/falling-things-shuffled-000001.tar [progress] # shard shuffled/falling-things-shuffled-000002.tar [progress] # shard shuffled/falling-things-shuffled-000003.tar [progress] # shard shuffled/falling-things-shuffled-000004.tar [progress] # shard shuffled/falling-things-shuffled-000005.tar [progress] # shard shuffled/falling-things-shuffled-000006.tar [progress] # shard shuffled/falling-things-shuffled-000007.tar [progress] # shard shuffled/falling-things-shuffled-000008.tar [progress] # shard shuffled/falling-things-shuffled-000009.tar [progress] # shard shuffled/falling-things-shuffled-000010.tar [progress] # shard shuffled/falling-things-shuffled-000011.tar [progress] # shard shuffled/falling-things-shuffled-000012.tar [progress] # shard shuffled/falling-things-shuffled-000013.tar [progress] # shard shuffled/falling-things-shuffled-000014.tar [progress] # shard shuffled/falling-things-shuffled-000015.tar [progress] # shard shuffled/falling-things-shuffled-000016.tar [progress] # shard shuffled/falling-things-shuffled-000017.tar [progress] # shard shuffled/falling-things-shuffled-000018.tar [progress] # shard shuffled/falling-things-shuffled-000019.tar [progress] # shard shuffled/falling-things-shuffled-000020.tar [progress] # shard shuffled/falling-things-shuffled-000021.tar [progress] # shard shuffled/falling-things-shuffled-000022.tar [progress] # shard shuffled/falling-things-shuffled-000023.tar [progress] # shard shuffled/falling-things-shuffled-000024.tar [progress] # shard shuffled/falling-things-shuffled-000025.tar [progress] # shard shuffled/falling-things-shuffled-000026.tar [progress] # shard shuffled/falling-things-shuffled-000027.tar [progress] # shard shuffled/falling-things-shuffled-000028.tar [progress] # shard shuffled/falling-things-shuffled-000029.tar [progress] # shard shuffled/falling-things-shuffled-000030.tar [progress] # shard shuffled/falling-things-shuffled-000031.tar [progress] # shard shuffled/falling-things-shuffled-000032.tar [progress] # shard shuffled/falling-things-shuffled-000033.tar [progress] # shard shuffled/falling-things-shuffled-000034.tar [progress] # shard shuffled/falling-things-shuffled-000035.tar [progress] # shard shuffled/falling-things-shuffled-000036.tar [progress] # shard shuffled/falling-things-shuffled-000037.tar [progress] # shard shuffled/falling-things-shuffled-000038.tar [progress] # shard shuffled/falling-things-shuffled-000039.tar [progress] # shard shuffled/falling-things-shuffled-000040.tar [progress] # shard shuffled/falling-things-shuffled-000041.tar [progress] # shard shuffled/falling-things-shuffled-000042.tar [progress] # shard shuffled/falling-things-shuffled-000043.tar [progress] # shard shuffled/falling-things-shuffled-000044.tar [progress] # shard shuffled/falling-things-shuffled-000045.tar [progress] # shard shuffled/falling-things-shuffled-000046.tar Loading the Frame-Level Data ds = wds.Dataset(\"shuffled/falling-things-shuffled-000033.tar\").decode() for sample in ds: break sample.keys() dict_keys(['__key__', 'object.json', 'right.json', 'left.json', 'left.jpg', 'right.seg.png', 'right.depth.png', 'camera.json', 'right.jpg', 'left.depth.png', 'left.seg.png']) figsize(12, 8) subplot(221); imshow(sample[\"left.jpg\"]) subplot(222); imshow(sample[\"right.jpg\"]) subplot(223); imshow(sample[\"left.seg.png\"]) subplot(224); imshow(sample[\"right.seg.png\"]) <matplotlib.image.AxesImage at 0x7f77e56e7f60> sample[\"camera.json\"] {'camera_settings': [{'name': 'left', 'horizontal_fov': 64, 'intrinsic_settings': {'fx': 768.1605834960938, 'fy': 768.1605834960938, 'cx': 480, 'cy': 270, 's': 0}, 'captured_image_size': {'width': 960, 'height': 540}}, {'name': 'right', 'horizontal_fov': 64, 'intrinsic_settings': {'fx': 768.1605834960938, 'fy': 768.1605834960938, 'cx': 480, 'cy': 270, 's': 0}, 'captured_image_size': {'width': 960, 'height': 540}}]} sample[\"object.json\"] {'exported_object_classes': ['011_banana_16k'], 'exported_objects': [{'class': '011_banana_16k', 'segmentation_class_id': 255, 'fixed_model_transform': [[-36.39540100097656, -17.36479949951172, 91.50869750976562, 0], [-93.08769989013672, 3.4368999004364014, -36.37120056152344, 0], [3.1707000732421875, -98.4207992553711, -17.4153995513916, 0], [0.03660000115633011, 1.497499942779541, 0.44449999928474426, 1]], 'cuboid_dimensions': [19.71739959716797, 3.8649001121520996, 7.406599998474121]}]} sample[\"left.json\"] {'camera_data': {'location_worldframe': [-487.3075866699219, -429.8739929199219, 208.91009521484375], 'quaternion_xyzw_worldframe': [0.33379998803138733, 0.3984000086784363, -0.5486999750137329, 0.6547999978065491]}, 'objects': [{'class': '011_banana_16k', 'visibility': 0.75, 'location': [10.001999855041504, 11.863900184631348, 97.93609619140625], 'quaternion_xyzw': [-0.982200026512146, 0.12720000743865967, -0.04879999905824661, -0.1290999948978424], 'pose_transform_permuted': [[0.06289999932050705, -0.2660999894142151, -0.961899995803833, 0], [0.9628999829292297, -0.23739999532699585, 0.12870000302791595, 0], [0.26249998807907104, 0.9343000054359436, -0.24130000174045563, 0], [10.001999855041504, 11.863900184631348, 97.93609619140625, 1]], 'cuboid_centroid': [10.001999855041504, 11.863900184631348, 97.93609619140625], 'projected_cuboid_centroid': [558.450927734375, 363.05450439453125], 'bounding_box': {'top_left': [327.92730712890625, 482.7919921875], 'bottom_right': [396.19769287109375, 636.2205810546875]}, 'cuboid': [[20.235000610351562, 10.343899726867676, 95.17620086669922], [1.249899983406067, 15.023799896240234, 92.63909912109375], [0.23520000278949738, 11.4128999710083, 93.57170104980469], [19.220300674438477, 6.732999801635742, 96.10880279541016], [19.768800735473633, 12.314800262451172, 102.30059814453125], [0.7835999727249146, 16.994800567626953, 99.76339721679688], [-0.23109999299049377, 13.383999824523926, 100.69599914550781], [18.754100799560547, 8.704000473022461, 103.23310089111328]], 'projected_cuboid': [[643.3162231445312, 353.4844970703125], [490.36468505859375, 394.5769958496094], [481.930908203125, 363.6925048828125], [633.6212158203125, 323.8143005371094], [628.44189453125, 362.4703063964844], [486.0342102050781, 400.8570861816406], [478.23748779296875, 372.099609375], [619.5501098632812, 334.76629638671875]]}]} sample[\"right.json\"] {'camera_data': {'location_worldframe': [-481.3999938964844, -428.82489013671875, 208.91009521484375], 'quaternion_xyzw_worldframe': [0.33379998803138733, 0.3984000086784363, -0.5486999750137329, 0.6547999978065491]}, 'objects': [{'class': '011_banana_16k', 'visibility': 0.75, 'location': [4.001999855041504, 11.86400032043457, 97.93599700927734], 'quaternion_xyzw': [-0.982200026512146, 0.12720000743865967, -0.04879999905824661, -0.1290999948978424], 'pose_transform_permuted': [[0.06289999932050705, -0.2660999894142151, -0.961899995803833, 0], [0.9628999829292297, -0.23739999532699585, 0.12870000302791595, 0], [0.26249998807907104, 0.9343000054359436, -0.24130000174045563, 0], [4.001999855041504, 11.86400032043457, 97.93599700927734, 1]], 'cuboid_centroid': [4.001999855041504, 11.86400032043457, 97.93599700927734], 'projected_cuboid_centroid': [511.389404296875, 363.05450439453125], 'bounding_box': {'top_left': [327.92730712890625, 434.88238525390625], 'bottom_right': [396.19769287109375, 588.8137817382812]}, 'cuboid': [[14.234999656677246, 10.343899726867676, 95.17620086669922], [-4.750100135803223, 15.023900032043457, 92.63899993896484], [-5.764800071716309, 11.413000106811523, 93.57160186767578], [13.22029972076416, 6.732999801635742, 96.10880279541016], [13.768799781799316, 12.314900398254395, 102.30049896240234], [-5.216400146484375, 16.99489974975586, 99.76339721679688], [-6.231100082397461, 13.383999824523926, 100.6958999633789], [12.75409984588623, 8.704000473022461, 103.23310089111328]], 'projected_cuboid': [[594.8900146484375, 353.4844970703125], [440.6123046875, 394.5769958496094], [432.6742858886719, 363.6925048828125], [585.6649169921875, 323.8143005371094], [583.38818359375, 362.4703063964844], [439.8346862792969, 400.8570861816406], [432.4659118652344, 372.099609375], [574.9033813476562, 334.76629638671875]]}]}","title":"Falling Things"},{"location":"falling-things-make-shards/#reading-the-zipfile","text":"We're taking the data out of the original zip file without unpacking. src = zipfile.ZipFile(\"fat.zip\") files = [s for s in src.filelist if not s.is_dir()] bydir = {} for fname in files: dir = os.path.dirname(fname.filename) bydir.setdefault(dir, []).append(fname) len(bydir.keys()) 330","title":"Reading the ZipFile"},{"location":"falling-things-make-shards/#creating-the-tar-files","text":"The Falling Things dataset is a collection of videos of falling things. However, there are different ways in which it can be used to create training samples: each frame is a training sample, presented in random order applications: pose estimation, stereo, monocular depth a small sequence of frames makes up a training sample applications: optical flow, motion segmentation, frame interpolatino each full sequence is a training sample (only 330 training samples) applications: physical modeling, long time tracking For random access datasets, the details of how data is broken up into training samples usually is hidden in the input pipeline. When using large scale, sequential training, this happens in a separate step while we generate training data sets. Note also that videos in this dataset are not represented as video files but as sequences of frames, distinguished by filename. We are going to do the same thing in the WebDataset representation. We first generate a full sequence dataset with a special structure of one shard per video: one .tar file per directory in the ZIP file duplicate the camera and object settings for each sample (so that we can later shuffle) one sample (=basename) per frame This is a valid WebDataset, but it also still preserves the video data in sequence within each shard, giving us different processing options. !rm -rf sequences shuffled !mkdir sequences shuffled def addfile(dst, name, data): assert isinstance(name, str), type(name) info = tarfile.TarInfo(name=name) info.size = len(data) info.uname, info.gname = \"bigdata\", \"bigdata\" info.mtime = time.time() dst.addfile(info, fileobj=io.BytesIO(data)) count = 0 for dir in sorted(bydir.keys()): tname = f\"sequences/falling-things-{count:06d}.tar\" nfiles = 0 with tarfile.open(tname, \"w|\") as dst: fnames = bydir[dir] print(count, tname, dir) camera_settings = [s for s in fnames if \"_camera_settings\" in s.filename][0] camera_settings = src.open(camera_settings, \"r\").read() object_settings = [s for s in fnames if \"_object_settings\" in s.filename][0] object_settings = src.open(object_settings, \"r\").read() last_base = \"NONE\" for finfo in sorted(fnames, key=lambda x: x.filename): fname = finfo.filename if fname.startswith(\"_\"): continue base = re.sub(r\"\\..*$\", \"\", os.path.basename(fname)) if base != last_base: #print(\"base:\", base) addfile(dst, dir+\"/\"+base+\".camera.json\", camera_settings) addfile(dst, dir+\"/\"+base+\".object.json\", object_settings) last_base = base with src.open(fname, \"r\") as stream: data = stream.read() addfile(dst, fname, data) nfiles += 1 #break count += 1 0 sequences/falling-things-000000.tar fat/mixed/kitchen_0 1 sequences/falling-things-000001.tar fat/mixed/kitchen_1 2 sequences/falling-things-000002.tar fat/mixed/kitchen_2 3 sequences/falling-things-000003.tar fat/mixed/kitchen_3 4 sequences/falling-things-000004.tar fat/mixed/kitchen_4 5 sequences/falling-things-000005.tar fat/mixed/kitedemo_0 6 sequences/falling-things-000006.tar fat/mixed/kitedemo_1 7 sequences/falling-things-000007.tar fat/mixed/kitedemo_2 8 sequences/falling-things-000008.tar fat/mixed/kitedemo_3 9 sequences/falling-things-000009.tar fat/mixed/kitedemo_4 10 sequences/falling-things-000010.tar fat/mixed/temple_0 11 sequences/falling-things-000011.tar fat/mixed/temple_1 12 sequences/falling-things-000012.tar fat/mixed/temple_2 13 sequences/falling-things-000013.tar fat/mixed/temple_3 14 sequences/falling-things-000014.tar fat/mixed/temple_4 15 sequences/falling-things-000015.tar fat/single/002_master_chef_can_16k/kitchen_0 16 sequences/falling-things-000016.tar fat/single/002_master_chef_can_16k/kitchen_1 17 sequences/falling-things-000017.tar fat/single/002_master_chef_can_16k/kitchen_2 18 sequences/falling-things-000018.tar fat/single/002_master_chef_can_16k/kitchen_3 19 sequences/falling-things-000019.tar fat/single/002_master_chef_can_16k/kitchen_4 20 sequences/falling-things-000020.tar fat/single/002_master_chef_can_16k/kitedemo_0 21 sequences/falling-things-000021.tar fat/single/002_master_chef_can_16k/kitedemo_1 22 sequences/falling-things-000022.tar fat/single/002_master_chef_can_16k/kitedemo_2 23 sequences/falling-things-000023.tar fat/single/002_master_chef_can_16k/kitedemo_3 24 sequences/falling-things-000024.tar fat/single/002_master_chef_can_16k/kitedemo_4 25 sequences/falling-things-000025.tar fat/single/002_master_chef_can_16k/temple_0 26 sequences/falling-things-000026.tar fat/single/002_master_chef_can_16k/temple_1 27 sequences/falling-things-000027.tar fat/single/002_master_chef_can_16k/temple_2 28 sequences/falling-things-000028.tar fat/single/002_master_chef_can_16k/temple_3 29 sequences/falling-things-000029.tar fat/single/002_master_chef_can_16k/temple_4 30 sequences/falling-things-000030.tar fat/single/003_cracker_box_16k/kitchen_0 31 sequences/falling-things-000031.tar fat/single/003_cracker_box_16k/kitchen_1 32 sequences/falling-things-000032.tar fat/single/003_cracker_box_16k/kitchen_2 33 sequences/falling-things-000033.tar fat/single/003_cracker_box_16k/kitchen_3 34 sequences/falling-things-000034.tar fat/single/003_cracker_box_16k/kitchen_4 35 sequences/falling-things-000035.tar fat/single/003_cracker_box_16k/kitedemo_0 36 sequences/falling-things-000036.tar fat/single/003_cracker_box_16k/kitedemo_1 37 sequences/falling-things-000037.tar fat/single/003_cracker_box_16k/kitedemo_2 38 sequences/falling-things-000038.tar fat/single/003_cracker_box_16k/kitedemo_3 39 sequences/falling-things-000039.tar fat/single/003_cracker_box_16k/kitedemo_4 40 sequences/falling-things-000040.tar fat/single/003_cracker_box_16k/temple_0 41 sequences/falling-things-000041.tar fat/single/003_cracker_box_16k/temple_1 42 sequences/falling-things-000042.tar fat/single/003_cracker_box_16k/temple_2 43 sequences/falling-things-000043.tar fat/single/003_cracker_box_16k/temple_3 44 sequences/falling-things-000044.tar fat/single/003_cracker_box_16k/temple_4 45 sequences/falling-things-000045.tar fat/single/004_sugar_box_16k/kitchen_0 46 sequences/falling-things-000046.tar fat/single/004_sugar_box_16k/kitchen_1 47 sequences/falling-things-000047.tar fat/single/004_sugar_box_16k/kitchen_2 48 sequences/falling-things-000048.tar fat/single/004_sugar_box_16k/kitchen_3 49 sequences/falling-things-000049.tar fat/single/004_sugar_box_16k/kitchen_4 50 sequences/falling-things-000050.tar fat/single/004_sugar_box_16k/kitedemo_0 51 sequences/falling-things-000051.tar fat/single/004_sugar_box_16k/kitedemo_1 52 sequences/falling-things-000052.tar fat/single/004_sugar_box_16k/kitedemo_2 53 sequences/falling-things-000053.tar fat/single/004_sugar_box_16k/kitedemo_3 54 sequences/falling-things-000054.tar fat/single/004_sugar_box_16k/kitedemo_4 55 sequences/falling-things-000055.tar fat/single/004_sugar_box_16k/temple_0 56 sequences/falling-things-000056.tar fat/single/004_sugar_box_16k/temple_1 57 sequences/falling-things-000057.tar fat/single/004_sugar_box_16k/temple_2 58 sequences/falling-things-000058.tar fat/single/004_sugar_box_16k/temple_3 59 sequences/falling-things-000059.tar fat/single/004_sugar_box_16k/temple_4 60 sequences/falling-things-000060.tar fat/single/005_tomato_soup_can_16k/kitchen_0 61 sequences/falling-things-000061.tar fat/single/005_tomato_soup_can_16k/kitchen_1 62 sequences/falling-things-000062.tar fat/single/005_tomato_soup_can_16k/kitchen_2 63 sequences/falling-things-000063.tar fat/single/005_tomato_soup_can_16k/kitchen_3 64 sequences/falling-things-000064.tar fat/single/005_tomato_soup_can_16k/kitchen_4 65 sequences/falling-things-000065.tar fat/single/005_tomato_soup_can_16k/kitedemo_0 66 sequences/falling-things-000066.tar fat/single/005_tomato_soup_can_16k/kitedemo_1 67 sequences/falling-things-000067.tar fat/single/005_tomato_soup_can_16k/kitedemo_2 68 sequences/falling-things-000068.tar fat/single/005_tomato_soup_can_16k/kitedemo_3 69 sequences/falling-things-000069.tar fat/single/005_tomato_soup_can_16k/kitedemo_4 70 sequences/falling-things-000070.tar fat/single/005_tomato_soup_can_16k/temple_0 71 sequences/falling-things-000071.tar fat/single/005_tomato_soup_can_16k/temple_1 72 sequences/falling-things-000072.tar fat/single/005_tomato_soup_can_16k/temple_2 73 sequences/falling-things-000073.tar fat/single/005_tomato_soup_can_16k/temple_3 74 sequences/falling-things-000074.tar fat/single/005_tomato_soup_can_16k/temple_4 75 sequences/falling-things-000075.tar fat/single/006_mustard_bottle_16k/kitchen_0 76 sequences/falling-things-000076.tar fat/single/006_mustard_bottle_16k/kitchen_1 77 sequences/falling-things-000077.tar fat/single/006_mustard_bottle_16k/kitchen_2 78 sequences/falling-things-000078.tar fat/single/006_mustard_bottle_16k/kitchen_3 79 sequences/falling-things-000079.tar fat/single/006_mustard_bottle_16k/kitchen_4 80 sequences/falling-things-000080.tar fat/single/006_mustard_bottle_16k/kitedemo_0 81 sequences/falling-things-000081.tar fat/single/006_mustard_bottle_16k/kitedemo_1 82 sequences/falling-things-000082.tar fat/single/006_mustard_bottle_16k/kitedemo_2 83 sequences/falling-things-000083.tar fat/single/006_mustard_bottle_16k/kitedemo_3 84 sequences/falling-things-000084.tar fat/single/006_mustard_bottle_16k/kitedemo_4 85 sequences/falling-things-000085.tar fat/single/006_mustard_bottle_16k/temple_0 86 sequences/falling-things-000086.tar fat/single/006_mustard_bottle_16k/temple_1 87 sequences/falling-things-000087.tar fat/single/006_mustard_bottle_16k/temple_2 88 sequences/falling-things-000088.tar fat/single/006_mustard_bottle_16k/temple_3 89 sequences/falling-things-000089.tar fat/single/006_mustard_bottle_16k/temple_4 90 sequences/falling-things-000090.tar fat/single/007_tuna_fish_can_16k/kitchen_0 91 sequences/falling-things-000091.tar fat/single/007_tuna_fish_can_16k/kitchen_1 92 sequences/falling-things-000092.tar fat/single/007_tuna_fish_can_16k/kitchen_2 93 sequences/falling-things-000093.tar fat/single/007_tuna_fish_can_16k/kitchen_3 94 sequences/falling-things-000094.tar fat/single/007_tuna_fish_can_16k/kitchen_4 95 sequences/falling-things-000095.tar fat/single/007_tuna_fish_can_16k/kitedemo_0 96 sequences/falling-things-000096.tar fat/single/007_tuna_fish_can_16k/kitedemo_1 97 sequences/falling-things-000097.tar fat/single/007_tuna_fish_can_16k/kitedemo_2 98 sequences/falling-things-000098.tar fat/single/007_tuna_fish_can_16k/kitedemo_3 99 sequences/falling-things-000099.tar fat/single/007_tuna_fish_can_16k/kitedemo_4 100 sequences/falling-things-000100.tar fat/single/007_tuna_fish_can_16k/temple_0 101 sequences/falling-things-000101.tar fat/single/007_tuna_fish_can_16k/temple_1 102 sequences/falling-things-000102.tar fat/single/007_tuna_fish_can_16k/temple_2 103 sequences/falling-things-000103.tar fat/single/007_tuna_fish_can_16k/temple_3 104 sequences/falling-things-000104.tar fat/single/007_tuna_fish_can_16k/temple_4 105 sequences/falling-things-000105.tar fat/single/008_pudding_box_16k/kitchen_0 106 sequences/falling-things-000106.tar fat/single/008_pudding_box_16k/kitchen_1 107 sequences/falling-things-000107.tar fat/single/008_pudding_box_16k/kitchen_2 108 sequences/falling-things-000108.tar fat/single/008_pudding_box_16k/kitchen_3 109 sequences/falling-things-000109.tar fat/single/008_pudding_box_16k/kitchen_4 110 sequences/falling-things-000110.tar fat/single/008_pudding_box_16k/kitedemo_0 111 sequences/falling-things-000111.tar fat/single/008_pudding_box_16k/kitedemo_1 112 sequences/falling-things-000112.tar fat/single/008_pudding_box_16k/kitedemo_2 113 sequences/falling-things-000113.tar fat/single/008_pudding_box_16k/kitedemo_3 114 sequences/falling-things-000114.tar fat/single/008_pudding_box_16k/kitedemo_4 115 sequences/falling-things-000115.tar fat/single/008_pudding_box_16k/temple_0 116 sequences/falling-things-000116.tar fat/single/008_pudding_box_16k/temple_1 117 sequences/falling-things-000117.tar fat/single/008_pudding_box_16k/temple_2 118 sequences/falling-things-000118.tar fat/single/008_pudding_box_16k/temple_3 119 sequences/falling-things-000119.tar fat/single/008_pudding_box_16k/temple_4 120 sequences/falling-things-000120.tar fat/single/009_gelatin_box_16k/kitchen_0 121 sequences/falling-things-000121.tar fat/single/009_gelatin_box_16k/kitchen_1 122 sequences/falling-things-000122.tar fat/single/009_gelatin_box_16k/kitchen_2 123 sequences/falling-things-000123.tar fat/single/009_gelatin_box_16k/kitchen_3 124 sequences/falling-things-000124.tar fat/single/009_gelatin_box_16k/kitchen_4 125 sequences/falling-things-000125.tar fat/single/009_gelatin_box_16k/kitedemo_0 126 sequences/falling-things-000126.tar fat/single/009_gelatin_box_16k/kitedemo_1 127 sequences/falling-things-000127.tar fat/single/009_gelatin_box_16k/kitedemo_2 128 sequences/falling-things-000128.tar fat/single/009_gelatin_box_16k/kitedemo_3 129 sequences/falling-things-000129.tar fat/single/009_gelatin_box_16k/kitedemo_4 130 sequences/falling-things-000130.tar fat/single/009_gelatin_box_16k/temple_0 131 sequences/falling-things-000131.tar fat/single/009_gelatin_box_16k/temple_1 132 sequences/falling-things-000132.tar fat/single/009_gelatin_box_16k/temple_2 133 sequences/falling-things-000133.tar fat/single/009_gelatin_box_16k/temple_3 134 sequences/falling-things-000134.tar fat/single/009_gelatin_box_16k/temple_4 135 sequences/falling-things-000135.tar fat/single/010_potted_meat_can_16k/kitchen_0 136 sequences/falling-things-000136.tar fat/single/010_potted_meat_can_16k/kitchen_1 137 sequences/falling-things-000137.tar fat/single/010_potted_meat_can_16k/kitchen_2 138 sequences/falling-things-000138.tar fat/single/010_potted_meat_can_16k/kitchen_3 139 sequences/falling-things-000139.tar fat/single/010_potted_meat_can_16k/kitchen_4 140 sequences/falling-things-000140.tar fat/single/010_potted_meat_can_16k/kitedemo_0 141 sequences/falling-things-000141.tar fat/single/010_potted_meat_can_16k/kitedemo_1 142 sequences/falling-things-000142.tar fat/single/010_potted_meat_can_16k/kitedemo_2 143 sequences/falling-things-000143.tar fat/single/010_potted_meat_can_16k/kitedemo_3 144 sequences/falling-things-000144.tar fat/single/010_potted_meat_can_16k/kitedemo_4 145 sequences/falling-things-000145.tar fat/single/010_potted_meat_can_16k/temple_0 146 sequences/falling-things-000146.tar fat/single/010_potted_meat_can_16k/temple_1 147 sequences/falling-things-000147.tar fat/single/010_potted_meat_can_16k/temple_2 148 sequences/falling-things-000148.tar fat/single/010_potted_meat_can_16k/temple_3 149 sequences/falling-things-000149.tar fat/single/010_potted_meat_can_16k/temple_4 150 sequences/falling-things-000150.tar fat/single/011_banana_16k/kitchen_0 151 sequences/falling-things-000151.tar fat/single/011_banana_16k/kitchen_1 152 sequences/falling-things-000152.tar fat/single/011_banana_16k/kitchen_2 153 sequences/falling-things-000153.tar fat/single/011_banana_16k/kitchen_3 154 sequences/falling-things-000154.tar fat/single/011_banana_16k/kitchen_4 155 sequences/falling-things-000155.tar fat/single/011_banana_16k/kitedemo_0 156 sequences/falling-things-000156.tar fat/single/011_banana_16k/kitedemo_1 157 sequences/falling-things-000157.tar fat/single/011_banana_16k/kitedemo_2 158 sequences/falling-things-000158.tar fat/single/011_banana_16k/kitedemo_3 159 sequences/falling-things-000159.tar fat/single/011_banana_16k/kitedemo_4 160 sequences/falling-things-000160.tar fat/single/011_banana_16k/temple_0 161 sequences/falling-things-000161.tar fat/single/011_banana_16k/temple_1 162 sequences/falling-things-000162.tar fat/single/011_banana_16k/temple_2 163 sequences/falling-things-000163.tar fat/single/011_banana_16k/temple_3 164 sequences/falling-things-000164.tar fat/single/011_banana_16k/temple_4 165 sequences/falling-things-000165.tar fat/single/019_pitcher_base_16k/kitchen_0 166 sequences/falling-things-000166.tar fat/single/019_pitcher_base_16k/kitchen_1 167 sequences/falling-things-000167.tar fat/single/019_pitcher_base_16k/kitchen_2 168 sequences/falling-things-000168.tar fat/single/019_pitcher_base_16k/kitchen_3 169 sequences/falling-things-000169.tar fat/single/019_pitcher_base_16k/kitchen_4 170 sequences/falling-things-000170.tar fat/single/019_pitcher_base_16k/kitedemo_0 171 sequences/falling-things-000171.tar fat/single/019_pitcher_base_16k/kitedemo_1 172 sequences/falling-things-000172.tar fat/single/019_pitcher_base_16k/kitedemo_2 173 sequences/falling-things-000173.tar fat/single/019_pitcher_base_16k/kitedemo_3 174 sequences/falling-things-000174.tar fat/single/019_pitcher_base_16k/kitedemo_4 175 sequences/falling-things-000175.tar fat/single/019_pitcher_base_16k/temple_0 176 sequences/falling-things-000176.tar fat/single/019_pitcher_base_16k/temple_1 177 sequences/falling-things-000177.tar fat/single/019_pitcher_base_16k/temple_2 178 sequences/falling-things-000178.tar fat/single/019_pitcher_base_16k/temple_3 179 sequences/falling-things-000179.tar fat/single/019_pitcher_base_16k/temple_4 180 sequences/falling-things-000180.tar fat/single/021_bleach_cleanser_16k/kitchen_0 181 sequences/falling-things-000181.tar fat/single/021_bleach_cleanser_16k/kitchen_1 182 sequences/falling-things-000182.tar fat/single/021_bleach_cleanser_16k/kitchen_2 183 sequences/falling-things-000183.tar fat/single/021_bleach_cleanser_16k/kitchen_3 184 sequences/falling-things-000184.tar fat/single/021_bleach_cleanser_16k/kitchen_4 185 sequences/falling-things-000185.tar fat/single/021_bleach_cleanser_16k/kitedemo_0 186 sequences/falling-things-000186.tar fat/single/021_bleach_cleanser_16k/kitedemo_1 187 sequences/falling-things-000187.tar fat/single/021_bleach_cleanser_16k/kitedemo_2 188 sequences/falling-things-000188.tar fat/single/021_bleach_cleanser_16k/kitedemo_3 189 sequences/falling-things-000189.tar fat/single/021_bleach_cleanser_16k/kitedemo_4 190 sequences/falling-things-000190.tar fat/single/021_bleach_cleanser_16k/temple_0 191 sequences/falling-things-000191.tar fat/single/021_bleach_cleanser_16k/temple_1 192 sequences/falling-things-000192.tar fat/single/021_bleach_cleanser_16k/temple_2 193 sequences/falling-things-000193.tar fat/single/021_bleach_cleanser_16k/temple_3 194 sequences/falling-things-000194.tar fat/single/021_bleach_cleanser_16k/temple_4 195 sequences/falling-things-000195.tar fat/single/024_bowl_16k/kitchen_0 196 sequences/falling-things-000196.tar fat/single/024_bowl_16k/kitchen_1 197 sequences/falling-things-000197.tar fat/single/024_bowl_16k/kitchen_2 198 sequences/falling-things-000198.tar fat/single/024_bowl_16k/kitchen_3 199 sequences/falling-things-000199.tar fat/single/024_bowl_16k/kitchen_4 200 sequences/falling-things-000200.tar fat/single/024_bowl_16k/kitedemo_0 201 sequences/falling-things-000201.tar fat/single/024_bowl_16k/kitedemo_1 202 sequences/falling-things-000202.tar fat/single/024_bowl_16k/kitedemo_2 203 sequences/falling-things-000203.tar fat/single/024_bowl_16k/kitedemo_3 204 sequences/falling-things-000204.tar fat/single/024_bowl_16k/kitedemo_4 205 sequences/falling-things-000205.tar fat/single/024_bowl_16k/temple_0 206 sequences/falling-things-000206.tar fat/single/024_bowl_16k/temple_1 207 sequences/falling-things-000207.tar fat/single/024_bowl_16k/temple_2 208 sequences/falling-things-000208.tar fat/single/024_bowl_16k/temple_3 209 sequences/falling-things-000209.tar fat/single/024_bowl_16k/temple_4 210 sequences/falling-things-000210.tar fat/single/025_mug_16k/kitchen_0 211 sequences/falling-things-000211.tar fat/single/025_mug_16k/kitchen_1 212 sequences/falling-things-000212.tar fat/single/025_mug_16k/kitchen_2 213 sequences/falling-things-000213.tar fat/single/025_mug_16k/kitchen_3 214 sequences/falling-things-000214.tar fat/single/025_mug_16k/kitchen_4 215 sequences/falling-things-000215.tar fat/single/025_mug_16k/kitedemo_0 216 sequences/falling-things-000216.tar fat/single/025_mug_16k/kitedemo_1 217 sequences/falling-things-000217.tar fat/single/025_mug_16k/kitedemo_2 218 sequences/falling-things-000218.tar fat/single/025_mug_16k/kitedemo_3 219 sequences/falling-things-000219.tar fat/single/025_mug_16k/kitedemo_4 220 sequences/falling-things-000220.tar fat/single/025_mug_16k/temple_0 221 sequences/falling-things-000221.tar fat/single/025_mug_16k/temple_1 222 sequences/falling-things-000222.tar fat/single/025_mug_16k/temple_2 223 sequences/falling-things-000223.tar fat/single/025_mug_16k/temple_3 224 sequences/falling-things-000224.tar fat/single/025_mug_16k/temple_4 225 sequences/falling-things-000225.tar fat/single/035_power_drill_16k/kitchen_0 226 sequences/falling-things-000226.tar fat/single/035_power_drill_16k/kitchen_1 227 sequences/falling-things-000227.tar fat/single/035_power_drill_16k/kitchen_2 228 sequences/falling-things-000228.tar fat/single/035_power_drill_16k/kitchen_3 229 sequences/falling-things-000229.tar fat/single/035_power_drill_16k/kitchen_4 230 sequences/falling-things-000230.tar fat/single/035_power_drill_16k/kitedemo_0 231 sequences/falling-things-000231.tar fat/single/035_power_drill_16k/kitedemo_1 232 sequences/falling-things-000232.tar fat/single/035_power_drill_16k/kitedemo_2 233 sequences/falling-things-000233.tar fat/single/035_power_drill_16k/kitedemo_3 234 sequences/falling-things-000234.tar fat/single/035_power_drill_16k/kitedemo_4 235 sequences/falling-things-000235.tar fat/single/035_power_drill_16k/temple_0 236 sequences/falling-things-000236.tar fat/single/035_power_drill_16k/temple_1 237 sequences/falling-things-000237.tar fat/single/035_power_drill_16k/temple_2 238 sequences/falling-things-000238.tar fat/single/035_power_drill_16k/temple_3 239 sequences/falling-things-000239.tar fat/single/035_power_drill_16k/temple_4 240 sequences/falling-things-000240.tar fat/single/036_wood_block_16k/kitchen_0 241 sequences/falling-things-000241.tar fat/single/036_wood_block_16k/kitchen_1 242 sequences/falling-things-000242.tar fat/single/036_wood_block_16k/kitchen_2 243 sequences/falling-things-000243.tar fat/single/036_wood_block_16k/kitchen_3 244 sequences/falling-things-000244.tar fat/single/036_wood_block_16k/kitchen_4 245 sequences/falling-things-000245.tar fat/single/036_wood_block_16k/kitedemo_0 246 sequences/falling-things-000246.tar fat/single/036_wood_block_16k/kitedemo_1 247 sequences/falling-things-000247.tar fat/single/036_wood_block_16k/kitedemo_2 248 sequences/falling-things-000248.tar fat/single/036_wood_block_16k/kitedemo_3 249 sequences/falling-things-000249.tar fat/single/036_wood_block_16k/kitedemo_4 250 sequences/falling-things-000250.tar fat/single/036_wood_block_16k/temple_0 251 sequences/falling-things-000251.tar fat/single/036_wood_block_16k/temple_1 252 sequences/falling-things-000252.tar fat/single/036_wood_block_16k/temple_2 253 sequences/falling-things-000253.tar fat/single/036_wood_block_16k/temple_3 254 sequences/falling-things-000254.tar fat/single/036_wood_block_16k/temple_4 255 sequences/falling-things-000255.tar fat/single/037_scissors_16k/kitchen_0 256 sequences/falling-things-000256.tar fat/single/037_scissors_16k/kitchen_1 257 sequences/falling-things-000257.tar fat/single/037_scissors_16k/kitchen_2 258 sequences/falling-things-000258.tar fat/single/037_scissors_16k/kitchen_3 259 sequences/falling-things-000259.tar fat/single/037_scissors_16k/kitchen_4 260 sequences/falling-things-000260.tar fat/single/037_scissors_16k/kitedemo_0 261 sequences/falling-things-000261.tar fat/single/037_scissors_16k/kitedemo_1 262 sequences/falling-things-000262.tar fat/single/037_scissors_16k/kitedemo_2 263 sequences/falling-things-000263.tar fat/single/037_scissors_16k/kitedemo_3 264 sequences/falling-things-000264.tar fat/single/037_scissors_16k/kitedemo_4 265 sequences/falling-things-000265.tar fat/single/037_scissors_16k/temple_0 266 sequences/falling-things-000266.tar fat/single/037_scissors_16k/temple_1 267 sequences/falling-things-000267.tar fat/single/037_scissors_16k/temple_2 268 sequences/falling-things-000268.tar fat/single/037_scissors_16k/temple_3 269 sequences/falling-things-000269.tar fat/single/037_scissors_16k/temple_4 270 sequences/falling-things-000270.tar fat/single/040_large_marker_16k/kitchen_0 271 sequences/falling-things-000271.tar fat/single/040_large_marker_16k/kitchen_1 272 sequences/falling-things-000272.tar fat/single/040_large_marker_16k/kitchen_2 273 sequences/falling-things-000273.tar fat/single/040_large_marker_16k/kitchen_3 274 sequences/falling-things-000274.tar fat/single/040_large_marker_16k/kitchen_4 275 sequences/falling-things-000275.tar fat/single/040_large_marker_16k/kitedemo_0 276 sequences/falling-things-000276.tar fat/single/040_large_marker_16k/kitedemo_1 277 sequences/falling-things-000277.tar fat/single/040_large_marker_16k/kitedemo_2 278 sequences/falling-things-000278.tar fat/single/040_large_marker_16k/kitedemo_3 279 sequences/falling-things-000279.tar fat/single/040_large_marker_16k/kitedemo_4 280 sequences/falling-things-000280.tar fat/single/040_large_marker_16k/temple_0 281 sequences/falling-things-000281.tar fat/single/040_large_marker_16k/temple_1 282 sequences/falling-things-000282.tar fat/single/040_large_marker_16k/temple_2 283 sequences/falling-things-000283.tar fat/single/040_large_marker_16k/temple_3 284 sequences/falling-things-000284.tar fat/single/040_large_marker_16k/temple_4 285 sequences/falling-things-000285.tar fat/single/051_large_clamp_16k/kitchen_0 286 sequences/falling-things-000286.tar fat/single/051_large_clamp_16k/kitchen_1 287 sequences/falling-things-000287.tar fat/single/051_large_clamp_16k/kitchen_2 288 sequences/falling-things-000288.tar fat/single/051_large_clamp_16k/kitchen_3 289 sequences/falling-things-000289.tar fat/single/051_large_clamp_16k/kitchen_4 290 sequences/falling-things-000290.tar fat/single/051_large_clamp_16k/kitedemo_0 291 sequences/falling-things-000291.tar fat/single/051_large_clamp_16k/kitedemo_1 292 sequences/falling-things-000292.tar fat/single/051_large_clamp_16k/kitedemo_2 293 sequences/falling-things-000293.tar fat/single/051_large_clamp_16k/kitedemo_3 294 sequences/falling-things-000294.tar fat/single/051_large_clamp_16k/kitedemo_4 295 sequences/falling-things-000295.tar fat/single/051_large_clamp_16k/temple_0 296 sequences/falling-things-000296.tar fat/single/051_large_clamp_16k/temple_1 297 sequences/falling-things-000297.tar fat/single/051_large_clamp_16k/temple_2 298 sequences/falling-things-000298.tar fat/single/051_large_clamp_16k/temple_3 299 sequences/falling-things-000299.tar fat/single/051_large_clamp_16k/temple_4 300 sequences/falling-things-000300.tar fat/single/052_extra_large_clamp_16k/kitchen_0 301 sequences/falling-things-000301.tar fat/single/052_extra_large_clamp_16k/kitchen_1 302 sequences/falling-things-000302.tar fat/single/052_extra_large_clamp_16k/kitchen_2 303 sequences/falling-things-000303.tar fat/single/052_extra_large_clamp_16k/kitchen_3 304 sequences/falling-things-000304.tar fat/single/052_extra_large_clamp_16k/kitchen_4 305 sequences/falling-things-000305.tar fat/single/052_extra_large_clamp_16k/kitedemo_0 306 sequences/falling-things-000306.tar fat/single/052_extra_large_clamp_16k/kitedemo_1 307 sequences/falling-things-000307.tar fat/single/052_extra_large_clamp_16k/kitedemo_2 308 sequences/falling-things-000308.tar fat/single/052_extra_large_clamp_16k/kitedemo_3 309 sequences/falling-things-000309.tar fat/single/052_extra_large_clamp_16k/kitedemo_4 310 sequences/falling-things-000310.tar fat/single/052_extra_large_clamp_16k/temple_0 311 sequences/falling-things-000311.tar fat/single/052_extra_large_clamp_16k/temple_1 312 sequences/falling-things-000312.tar fat/single/052_extra_large_clamp_16k/temple_2 313 sequences/falling-things-000313.tar fat/single/052_extra_large_clamp_16k/temple_3 314 sequences/falling-things-000314.tar fat/single/052_extra_large_clamp_16k/temple_4 315 sequences/falling-things-000315.tar fat/single/061_foam_brick_16k/kitchen_0 316 sequences/falling-things-000316.tar fat/single/061_foam_brick_16k/kitchen_1 317 sequences/falling-things-000317.tar fat/single/061_foam_brick_16k/kitchen_2 318 sequences/falling-things-000318.tar fat/single/061_foam_brick_16k/kitchen_3 319 sequences/falling-things-000319.tar fat/single/061_foam_brick_16k/kitchen_4 320 sequences/falling-things-000320.tar fat/single/061_foam_brick_16k/kitedemo_0 321 sequences/falling-things-000321.tar fat/single/061_foam_brick_16k/kitedemo_1 322 sequences/falling-things-000322.tar fat/single/061_foam_brick_16k/kitedemo_2 323 sequences/falling-things-000323.tar fat/single/061_foam_brick_16k/kitedemo_3 324 sequences/falling-things-000324.tar fat/single/061_foam_brick_16k/kitedemo_4 325 sequences/falling-things-000325.tar fat/single/061_foam_brick_16k/temple_0 326 sequences/falling-things-000326.tar fat/single/061_foam_brick_16k/temple_1 327 sequences/falling-things-000327.tar fat/single/061_foam_brick_16k/temple_2 328 sequences/falling-things-000328.tar fat/single/061_foam_brick_16k/temple_3 329 sequences/falling-things-000329.tar fat/single/061_foam_brick_16k/temple_4 !ls -lth sequences/*.tar | head -rw-rw-r-- 1 tmb tmb 46M Aug 30 19:02 sequences/falling-things-000329.tar -rw-rw-r-- 1 tmb tmb 46M Aug 30 19:02 sequences/falling-things-000328.tar -rw-rw-r-- 1 tmb tmb 26M Aug 30 19:02 sequences/falling-things-000327.tar -rw-rw-r-- 1 tmb tmb 32M Aug 30 19:02 sequences/falling-things-000326.tar -rw-rw-r-- 1 tmb tmb 33M Aug 30 19:02 sequences/falling-things-000325.tar -rw-rw-r-- 1 tmb tmb 155M Aug 30 19:02 sequences/falling-things-000324.tar -rw-rw-r-- 1 tmb tmb 89M Aug 30 19:02 sequences/falling-things-000323.tar -rw-rw-r-- 1 tmb tmb 165M Aug 30 19:02 sequences/falling-things-000322.tar -rw-rw-r-- 1 tmb tmb 90M Aug 30 19:01 sequences/falling-things-000321.tar -rw-rw-r-- 1 tmb tmb 99M Aug 30 19:01 sequences/falling-things-000320.tar ls: write error: Broken pipe !tar tf sequences/falling-things-000000.tar | head fat/mixed/kitchen_0/000000.camera.json fat/mixed/kitchen_0/000000.object.json fat/mixed/kitchen_0/000000.left.depth.png fat/mixed/kitchen_0/000000.left.jpg fat/mixed/kitchen_0/000000.left.json fat/mixed/kitchen_0/000000.left.seg.png fat/mixed/kitchen_0/000000.right.depth.png fat/mixed/kitchen_0/000000.right.jpg fat/mixed/kitchen_0/000000.right.json fat/mixed/kitchen_0/000000.right.seg.png tar: write error","title":"Creating the Tar Files"},{"location":"falling-things-make-shards/#frame-level-training","text":"To generate frame level training, we can simply shuffle the sequence data. For this to work, it is important that we associated the sequence level information (camera, object) with each frame, as we did in the construction of the sequence dataset. We can now shuffle with: $ tarp cat -m 5 -s 500 -o - by-dir/*.tar | tarp split - -o shuffled/temp-%06d.tar $ tarp cat -m 10 -s 1000 -o - shuffled/temp-*.tar | tarp split - -o shuffled/falling-things-shuffled-%06d.tar If your machine has more memory, you can adjust the -m and -s options. %%bash tarp cat -m 5 -s 500 -o - sequences/*.tar | tarp split - -o shuffled/temp-%06d.tar && tarp cat -m 10 -s 1000 -o - shuffled/temp-*.tar | tarp split - -o shuffled/falling-things-shuffled-%06d.tar && rm shuffled/temp-*.tar [info] # shuffle 500 [progress] # writing - [progress] # source - [progress] # shard shuffled/temp-000000.tar [progress] # shard shuffled/temp-000001.tar [progress] # shard shuffled/temp-000002.tar [progress] # shard shuffled/temp-000003.tar [progress] # shard shuffled/temp-000004.tar [progress] # shard shuffled/temp-000005.tar [progress] # shard shuffled/temp-000006.tar [progress] # shard shuffled/temp-000007.tar [progress] # shard shuffled/temp-000008.tar [progress] # shard shuffled/temp-000009.tar [progress] # shard shuffled/temp-000010.tar [progress] # shard shuffled/temp-000011.tar [progress] # shard shuffled/temp-000012.tar [progress] # shard shuffled/temp-000013.tar [progress] # shard shuffled/temp-000014.tar [progress] # shard shuffled/temp-000015.tar [progress] # shard shuffled/temp-000016.tar [progress] # shard shuffled/temp-000017.tar [progress] # shard shuffled/temp-000018.tar [progress] # shard shuffled/temp-000019.tar [progress] # shard shuffled/temp-000020.tar [progress] # shard shuffled/temp-000021.tar [progress] # shard shuffled/temp-000022.tar [progress] # shard shuffled/temp-000023.tar [progress] # shard shuffled/temp-000024.tar [progress] # shard shuffled/temp-000025.tar [progress] # shard shuffled/temp-000026.tar [progress] # shard shuffled/temp-000027.tar [progress] # shard shuffled/temp-000028.tar [progress] # shard shuffled/temp-000029.tar [progress] # shard shuffled/temp-000030.tar [progress] # shard shuffled/temp-000031.tar [progress] # shard shuffled/temp-000032.tar [progress] # shard shuffled/temp-000033.tar [progress] # shard shuffled/temp-000034.tar [progress] # shard shuffled/temp-000035.tar [progress] # shard shuffled/temp-000036.tar [progress] # shard shuffled/temp-000037.tar [progress] # shard shuffled/temp-000038.tar [progress] # shard shuffled/temp-000039.tar [progress] # shard shuffled/temp-000040.tar [progress] # shard shuffled/temp-000041.tar [progress] # shard shuffled/temp-000042.tar [progress] # shard shuffled/temp-000043.tar [progress] # shard shuffled/temp-000044.tar [progress] # shard shuffled/temp-000045.tar [progress] # shard shuffled/temp-000046.tar [progress] # source - [info] # shuffle 1000 [progress] # writing - [progress] # shard shuffled/falling-things-shuffled-000000.tar [progress] # shard shuffled/falling-things-shuffled-000001.tar [progress] # shard shuffled/falling-things-shuffled-000002.tar [progress] # shard shuffled/falling-things-shuffled-000003.tar [progress] # shard shuffled/falling-things-shuffled-000004.tar [progress] # shard shuffled/falling-things-shuffled-000005.tar [progress] # shard shuffled/falling-things-shuffled-000006.tar [progress] # shard shuffled/falling-things-shuffled-000007.tar [progress] # shard shuffled/falling-things-shuffled-000008.tar [progress] # shard shuffled/falling-things-shuffled-000009.tar [progress] # shard shuffled/falling-things-shuffled-000010.tar [progress] # shard shuffled/falling-things-shuffled-000011.tar [progress] # shard shuffled/falling-things-shuffled-000012.tar [progress] # shard shuffled/falling-things-shuffled-000013.tar [progress] # shard shuffled/falling-things-shuffled-000014.tar [progress] # shard shuffled/falling-things-shuffled-000015.tar [progress] # shard shuffled/falling-things-shuffled-000016.tar [progress] # shard shuffled/falling-things-shuffled-000017.tar [progress] # shard shuffled/falling-things-shuffled-000018.tar [progress] # shard shuffled/falling-things-shuffled-000019.tar [progress] # shard shuffled/falling-things-shuffled-000020.tar [progress] # shard shuffled/falling-things-shuffled-000021.tar [progress] # shard shuffled/falling-things-shuffled-000022.tar [progress] # shard shuffled/falling-things-shuffled-000023.tar [progress] # shard shuffled/falling-things-shuffled-000024.tar [progress] # shard shuffled/falling-things-shuffled-000025.tar [progress] # shard shuffled/falling-things-shuffled-000026.tar [progress] # shard shuffled/falling-things-shuffled-000027.tar [progress] # shard shuffled/falling-things-shuffled-000028.tar [progress] # shard shuffled/falling-things-shuffled-000029.tar [progress] # shard shuffled/falling-things-shuffled-000030.tar [progress] # shard shuffled/falling-things-shuffled-000031.tar [progress] # shard shuffled/falling-things-shuffled-000032.tar [progress] # shard shuffled/falling-things-shuffled-000033.tar [progress] # shard shuffled/falling-things-shuffled-000034.tar [progress] # shard shuffled/falling-things-shuffled-000035.tar [progress] # shard shuffled/falling-things-shuffled-000036.tar [progress] # shard shuffled/falling-things-shuffled-000037.tar [progress] # shard shuffled/falling-things-shuffled-000038.tar [progress] # shard shuffled/falling-things-shuffled-000039.tar [progress] # shard shuffled/falling-things-shuffled-000040.tar [progress] # shard shuffled/falling-things-shuffled-000041.tar [progress] # shard shuffled/falling-things-shuffled-000042.tar [progress] # shard shuffled/falling-things-shuffled-000043.tar [progress] # shard shuffled/falling-things-shuffled-000044.tar [progress] # shard shuffled/falling-things-shuffled-000045.tar [progress] # shard shuffled/falling-things-shuffled-000046.tar","title":"Frame-Level Training"},{"location":"falling-things-make-shards/#loading-the-frame-level-data","text":"ds = wds.Dataset(\"shuffled/falling-things-shuffled-000033.tar\").decode() for sample in ds: break sample.keys() dict_keys(['__key__', 'object.json', 'right.json', 'left.json', 'left.jpg', 'right.seg.png', 'right.depth.png', 'camera.json', 'right.jpg', 'left.depth.png', 'left.seg.png']) figsize(12, 8) subplot(221); imshow(sample[\"left.jpg\"]) subplot(222); imshow(sample[\"right.jpg\"]) subplot(223); imshow(sample[\"left.seg.png\"]) subplot(224); imshow(sample[\"right.seg.png\"]) <matplotlib.image.AxesImage at 0x7f77e56e7f60> sample[\"camera.json\"] {'camera_settings': [{'name': 'left', 'horizontal_fov': 64, 'intrinsic_settings': {'fx': 768.1605834960938, 'fy': 768.1605834960938, 'cx': 480, 'cy': 270, 's': 0}, 'captured_image_size': {'width': 960, 'height': 540}}, {'name': 'right', 'horizontal_fov': 64, 'intrinsic_settings': {'fx': 768.1605834960938, 'fy': 768.1605834960938, 'cx': 480, 'cy': 270, 's': 0}, 'captured_image_size': {'width': 960, 'height': 540}}]} sample[\"object.json\"] {'exported_object_classes': ['011_banana_16k'], 'exported_objects': [{'class': '011_banana_16k', 'segmentation_class_id': 255, 'fixed_model_transform': [[-36.39540100097656, -17.36479949951172, 91.50869750976562, 0], [-93.08769989013672, 3.4368999004364014, -36.37120056152344, 0], [3.1707000732421875, -98.4207992553711, -17.4153995513916, 0], [0.03660000115633011, 1.497499942779541, 0.44449999928474426, 1]], 'cuboid_dimensions': [19.71739959716797, 3.8649001121520996, 7.406599998474121]}]} sample[\"left.json\"] {'camera_data': {'location_worldframe': [-487.3075866699219, -429.8739929199219, 208.91009521484375], 'quaternion_xyzw_worldframe': [0.33379998803138733, 0.3984000086784363, -0.5486999750137329, 0.6547999978065491]}, 'objects': [{'class': '011_banana_16k', 'visibility': 0.75, 'location': [10.001999855041504, 11.863900184631348, 97.93609619140625], 'quaternion_xyzw': [-0.982200026512146, 0.12720000743865967, -0.04879999905824661, -0.1290999948978424], 'pose_transform_permuted': [[0.06289999932050705, -0.2660999894142151, -0.961899995803833, 0], [0.9628999829292297, -0.23739999532699585, 0.12870000302791595, 0], [0.26249998807907104, 0.9343000054359436, -0.24130000174045563, 0], [10.001999855041504, 11.863900184631348, 97.93609619140625, 1]], 'cuboid_centroid': [10.001999855041504, 11.863900184631348, 97.93609619140625], 'projected_cuboid_centroid': [558.450927734375, 363.05450439453125], 'bounding_box': {'top_left': [327.92730712890625, 482.7919921875], 'bottom_right': [396.19769287109375, 636.2205810546875]}, 'cuboid': [[20.235000610351562, 10.343899726867676, 95.17620086669922], [1.249899983406067, 15.023799896240234, 92.63909912109375], [0.23520000278949738, 11.4128999710083, 93.57170104980469], [19.220300674438477, 6.732999801635742, 96.10880279541016], [19.768800735473633, 12.314800262451172, 102.30059814453125], [0.7835999727249146, 16.994800567626953, 99.76339721679688], [-0.23109999299049377, 13.383999824523926, 100.69599914550781], [18.754100799560547, 8.704000473022461, 103.23310089111328]], 'projected_cuboid': [[643.3162231445312, 353.4844970703125], [490.36468505859375, 394.5769958496094], [481.930908203125, 363.6925048828125], [633.6212158203125, 323.8143005371094], [628.44189453125, 362.4703063964844], [486.0342102050781, 400.8570861816406], [478.23748779296875, 372.099609375], [619.5501098632812, 334.76629638671875]]}]} sample[\"right.json\"] {'camera_data': {'location_worldframe': [-481.3999938964844, -428.82489013671875, 208.91009521484375], 'quaternion_xyzw_worldframe': [0.33379998803138733, 0.3984000086784363, -0.5486999750137329, 0.6547999978065491]}, 'objects': [{'class': '011_banana_16k', 'visibility': 0.75, 'location': [4.001999855041504, 11.86400032043457, 97.93599700927734], 'quaternion_xyzw': [-0.982200026512146, 0.12720000743865967, -0.04879999905824661, -0.1290999948978424], 'pose_transform_permuted': [[0.06289999932050705, -0.2660999894142151, -0.961899995803833, 0], [0.9628999829292297, -0.23739999532699585, 0.12870000302791595, 0], [0.26249998807907104, 0.9343000054359436, -0.24130000174045563, 0], [4.001999855041504, 11.86400032043457, 97.93599700927734, 1]], 'cuboid_centroid': [4.001999855041504, 11.86400032043457, 97.93599700927734], 'projected_cuboid_centroid': [511.389404296875, 363.05450439453125], 'bounding_box': {'top_left': [327.92730712890625, 434.88238525390625], 'bottom_right': [396.19769287109375, 588.8137817382812]}, 'cuboid': [[14.234999656677246, 10.343899726867676, 95.17620086669922], [-4.750100135803223, 15.023900032043457, 92.63899993896484], [-5.764800071716309, 11.413000106811523, 93.57160186767578], [13.22029972076416, 6.732999801635742, 96.10880279541016], [13.768799781799316, 12.314900398254395, 102.30049896240234], [-5.216400146484375, 16.99489974975586, 99.76339721679688], [-6.231100082397461, 13.383999824523926, 100.6958999633789], [12.75409984588623, 8.704000473022461, 103.23310089111328]], 'projected_cuboid': [[594.8900146484375, 353.4844970703125], [440.6123046875, 394.5769958496094], [432.6742858886719, 363.6925048828125], [585.6649169921875, 323.8143005371094], [583.38818359375, 362.4703063964844], [439.8346862792969, 400.8570861816406], [432.4659118652344, 372.099609375], [574.9033813476562, 334.76629638671875]]}]}","title":"Loading the Frame-Level Data"},{"location":"gettingstarted/","text":"%pylab inline import torch from torch.utils.data import IterableDataset from torchvision import transforms import webdataset as wds from itertools import islice Populating the interactive namespace from numpy and matplotlib Getting Started WebDataset reads dataset that are stored as tar files, with the simple convention that files that belong together and make up a training sample share the same basename. WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores. %%bash curl -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar | tar tf - | sed 10q e39871fd9fd74f55.jpg e39871fd9fd74f55.json f18b91585c4d3f3e.jpg f18b91585c4d3f3e.json ede6e66b2fb59aab.jpg ede6e66b2fb59aab.json ed600d57fcee4f94.jpg ed600d57fcee4f94.json ff47e649b23f446d.jpg ff47e649b23f446d.json url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" For starters, let's use the webdataset.Dataset class to illustrate how the webdataset library works. dataset = wds.WebDataset(url) for sample in islice(dataset, 0, 3): for key, value in sample.items(): print(key, repr(value)[:50]) print() __key__ 'e39871fd9fd74f55' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01 json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli __key__ 'f18b91585c4d3f3e' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00 json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti __key__ 'ede6e66b2fb59aab' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00 json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti There are common processing stages you can add to a dataset to make it a drop-in replacement for any existing dataset. For convenience, common operations are available through a \"fluent\" interface (as chained method calls). dataset = ( wds.WebDataset(url) .shuffle(100) .decode(\"rgb\") .to_tuple(\"jpg;png\", \"json\") ) for image, data in islice(dataset, 0, 3): print(image.shape, image.dtype, type(data)) (1024, 683, 3) float32 <class 'list'> (660, 1024, 3) float32 <class 'list'> (701, 1024, 3) float32 <class 'list'> The webdataset.Dataset class has some common operations: shuffle(n) : shuffle the dataset with a buffer of size n ; also shuffles shards (see below) decode(decoder, ...) : automatically decode files (most commonly, you can just specify \"pil\" , \"rgb\" , \"rgb8\" , \"rgbtorch\" , etc.) rename(new=\"old1;old2\", ...) : rename fields map(f) : apply f to each sample map_dict(key=f, ...) : apply f to its corresponding key map_tuple(f, g, ...) : apply f , g , etc. to their corresponding values in the tuple pipe(f) : f should be a function that takes an iterator and returns a new iterator Stages commonly take a handler= argument, which is a function that gets called when there is an exception; you can write whatever function you want, but common functions are: webdataset.ignore_and_stop webdataset.ignore_and_continue webdataset.warn_and_stop webdataset.warn_and_continue webdataset.reraise_exception Data Augmentation Here is an example that uses torchvision data augmentation the same way you might use it with a FileDataset . def identity(x): return x normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) preproc = transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize, ]) dataset = ( wds.WebDataset(url) .shuffle(100) .decode(\"pil\") .to_tuple(\"jpg;png\", \"json\") .map_tuple(preproc, identity) ) for image, data in islice(dataset, 0, 3): print(image.shape, image.dtype, type(data)) torch.Size([3, 224, 224]) torch.float32 <class 'list'> torch.Size([3, 224, 224]) torch.float32 <class 'list'> torch.Size([3, 224, 224]) torch.float32 <class 'list'> WebDataset and DataLoader When used with a standard Torch DataLoader, this will would perform parallel I/O and preprocessing. However, the recommended way of using IterableDataset with DataLoader is to do the batching explicitly in the Dataset: batch_size = 20 dataloader = torch.utils.data.DataLoader(dataset.batched(batch_size), num_workers=4, batch_size=None) images, targets = next(iter(dataloader)) images.shape /home/tmb/proj/webdataset/docs/webdataset/dataset.py:85: UserWarning: num_workers 4 > num_shards 1 warnings.warn(f\"num_workers {num_workers} > num_shards {len(urls)}\") torch.Size([20, 3, 224, 224]) You can find the full PyTorch ImageNet sample code converted to WebDataset at tmbdev/pytorch-imagenet-wds","title":"Getting Started"},{"location":"gettingstarted/#getting-started","text":"WebDataset reads dataset that are stored as tar files, with the simple convention that files that belong together and make up a training sample share the same basename. WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores. %%bash curl -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar | tar tf - | sed 10q e39871fd9fd74f55.jpg e39871fd9fd74f55.json f18b91585c4d3f3e.jpg f18b91585c4d3f3e.json ede6e66b2fb59aab.jpg ede6e66b2fb59aab.json ed600d57fcee4f94.jpg ed600d57fcee4f94.json ff47e649b23f446d.jpg ff47e649b23f446d.json url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" For starters, let's use the webdataset.Dataset class to illustrate how the webdataset library works. dataset = wds.WebDataset(url) for sample in islice(dataset, 0, 3): for key, value in sample.items(): print(key, repr(value)[:50]) print() __key__ 'e39871fd9fd74f55' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01 json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli __key__ 'f18b91585c4d3f3e' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00 json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti __key__ 'ede6e66b2fb59aab' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00 json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti There are common processing stages you can add to a dataset to make it a drop-in replacement for any existing dataset. For convenience, common operations are available through a \"fluent\" interface (as chained method calls). dataset = ( wds.WebDataset(url) .shuffle(100) .decode(\"rgb\") .to_tuple(\"jpg;png\", \"json\") ) for image, data in islice(dataset, 0, 3): print(image.shape, image.dtype, type(data)) (1024, 683, 3) float32 <class 'list'> (660, 1024, 3) float32 <class 'list'> (701, 1024, 3) float32 <class 'list'> The webdataset.Dataset class has some common operations: shuffle(n) : shuffle the dataset with a buffer of size n ; also shuffles shards (see below) decode(decoder, ...) : automatically decode files (most commonly, you can just specify \"pil\" , \"rgb\" , \"rgb8\" , \"rgbtorch\" , etc.) rename(new=\"old1;old2\", ...) : rename fields map(f) : apply f to each sample map_dict(key=f, ...) : apply f to its corresponding key map_tuple(f, g, ...) : apply f , g , etc. to their corresponding values in the tuple pipe(f) : f should be a function that takes an iterator and returns a new iterator Stages commonly take a handler= argument, which is a function that gets called when there is an exception; you can write whatever function you want, but common functions are: webdataset.ignore_and_stop webdataset.ignore_and_continue webdataset.warn_and_stop webdataset.warn_and_continue webdataset.reraise_exception","title":"Getting Started"},{"location":"gettingstarted/#data-augmentation","text":"Here is an example that uses torchvision data augmentation the same way you might use it with a FileDataset . def identity(x): return x normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) preproc = transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize, ]) dataset = ( wds.WebDataset(url) .shuffle(100) .decode(\"pil\") .to_tuple(\"jpg;png\", \"json\") .map_tuple(preproc, identity) ) for image, data in islice(dataset, 0, 3): print(image.shape, image.dtype, type(data)) torch.Size([3, 224, 224]) torch.float32 <class 'list'> torch.Size([3, 224, 224]) torch.float32 <class 'list'> torch.Size([3, 224, 224]) torch.float32 <class 'list'>","title":"Data Augmentation"},{"location":"gettingstarted/#webdataset-and-dataloader","text":"When used with a standard Torch DataLoader, this will would perform parallel I/O and preprocessing. However, the recommended way of using IterableDataset with DataLoader is to do the batching explicitly in the Dataset: batch_size = 20 dataloader = torch.utils.data.DataLoader(dataset.batched(batch_size), num_workers=4, batch_size=None) images, targets = next(iter(dataloader)) images.shape /home/tmb/proj/webdataset/docs/webdataset/dataset.py:85: UserWarning: num_workers 4 > num_shards 1 warnings.warn(f\"num_workers {num_workers} > num_shards {len(urls)}\") torch.Size([20, 3, 224, 224]) You can find the full PyTorch ImageNet sample code converted to WebDataset at tmbdev/pytorch-imagenet-wds","title":"WebDataset and DataLoader"},{"location":"howitworks/","text":"%pylab inline import torch from torch.utils.data import IterableDataset from torchvision import transforms import webdataset as wds from itertools import islice Populating the interactive namespace from numpy and matplotlib How it Works WebDataset is powerful and it may look complex from the outside, but its structure is quite simple: most of the code consists of functions mapping an input iterator to an output iterator: def add_noise(source, noise=0.01): for inputs, targets in source: inputs = inputs + noise * torch.randn_like(inputs) yield inputs, targets To write new processing stages, a function like this is all you ever have to write. The rest is really bookkeeping: we need to be able to repeatedly invoke functions like this for every epoch, and we need to chain them together. To turn a function like that into an IterableDataset , and chain it with an existing dataset, you can use the webdataset.Processor class: dataset = ... noisy_dataset = wds.Processor(dataset, add_noise, noise=0.02) The webdataset.WebDataset class is just a wrapper for Processor with a default initial processing pipeline and some convenience methods. Full expanded, the above pipeline can be written as: url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" dataset = wds.ShardList(url) dataset = wds.Processor(dataset, wds.url_opener) dataset = wds.Processor(dataset, wds.tar_file_expander) dataset = wds.Processor(dataset, wds.group_by_keys) dataset = wds.Processor(dataset, wds.shuffle, 100) dataset = wds.Processor(dataset, wds.decode, wds.imagehandler(\"torchrgb\")) dataset = wds.Processor(dataset, wds.to_tuple, \"png;jpg;jpeg\", \"json\") noisy_dataset = wds.Processor(dataset, add_noise, noise=0.02) next(iter(noisy_dataset))[0].shape torch.Size([3, 683, 1024]) You can mix the shorthands with explicit constructions of processors: dataset = wds.WebDataset(url).shuffle(100).decode(\"torchrgb\").to_tuple(\"png;jpg;jpeg\", \"json\") noisy_dataset = wds.Processor(dataset, add_noise, noise=0.02) next(iter(noisy_dataset))[0].shape torch.Size([3, 768, 1024]) wds.Processor is just an IterableDataset instance; you can use it wherever you might use an IterableDataset . That means that all the functionality from the WebDataset library is available with other iterable sources. Let's start by defining a simple SQL-based IterableDataset . import sqlite3 import pickle import io import torch from torch.utils.data import IterableDataset class SqlDataset(IterableDataset): def __init__(self, dbname): self.db = sqlite3.connect(dbname) self.db.execute(\"create table if not exists data (inputs blob, targets blob)\") def add(self, inputs, targets): self.db.execute(\"insert into data (inputs, targets) values (?, ?)\", (wds.torch_dumps(inputs), wds.torch_dumps(targets))) def __iter__(self): query = \"select inputs, targets from data\" cursor = self.db.execute(query) for inputs, targets in cursor: yield wds.torch_loads(inputs), wds.torch_loads(targets) def __len__(self): return self.db.execute(\"select count(*) from data\").fetchone()[0] !rm -f test.db dataset = SqlDataset(\"test.db\") size=32 for i in range(1000): dataset.add(torch.randn(3, size, size), torch.randn(3, size, size)) print(len(dataset), next(iter(dataset))[0].shape) 1000 torch.Size([3, 32, 32]) Now we can chain this IterableDataset implementation with webdataset.Processor : dataset = wds.Processor(dataset, wds.shuffle, 100) dataset = wds.Processor(dataset, wds.batched, 16) noisy_dataset = wds.Processor(dataset, add_noise, noise=0.02) print(next(iter(noisy_dataset))[0].shape) torch.Size([16, 3, 32, 32])","title":"How It Works"},{"location":"howitworks/#how-it-works","text":"WebDataset is powerful and it may look complex from the outside, but its structure is quite simple: most of the code consists of functions mapping an input iterator to an output iterator: def add_noise(source, noise=0.01): for inputs, targets in source: inputs = inputs + noise * torch.randn_like(inputs) yield inputs, targets To write new processing stages, a function like this is all you ever have to write. The rest is really bookkeeping: we need to be able to repeatedly invoke functions like this for every epoch, and we need to chain them together. To turn a function like that into an IterableDataset , and chain it with an existing dataset, you can use the webdataset.Processor class: dataset = ... noisy_dataset = wds.Processor(dataset, add_noise, noise=0.02) The webdataset.WebDataset class is just a wrapper for Processor with a default initial processing pipeline and some convenience methods. Full expanded, the above pipeline can be written as: url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" dataset = wds.ShardList(url) dataset = wds.Processor(dataset, wds.url_opener) dataset = wds.Processor(dataset, wds.tar_file_expander) dataset = wds.Processor(dataset, wds.group_by_keys) dataset = wds.Processor(dataset, wds.shuffle, 100) dataset = wds.Processor(dataset, wds.decode, wds.imagehandler(\"torchrgb\")) dataset = wds.Processor(dataset, wds.to_tuple, \"png;jpg;jpeg\", \"json\") noisy_dataset = wds.Processor(dataset, add_noise, noise=0.02) next(iter(noisy_dataset))[0].shape torch.Size([3, 683, 1024]) You can mix the shorthands with explicit constructions of processors: dataset = wds.WebDataset(url).shuffle(100).decode(\"torchrgb\").to_tuple(\"png;jpg;jpeg\", \"json\") noisy_dataset = wds.Processor(dataset, add_noise, noise=0.02) next(iter(noisy_dataset))[0].shape torch.Size([3, 768, 1024]) wds.Processor is just an IterableDataset instance; you can use it wherever you might use an IterableDataset . That means that all the functionality from the WebDataset library is available with other iterable sources. Let's start by defining a simple SQL-based IterableDataset . import sqlite3 import pickle import io import torch from torch.utils.data import IterableDataset class SqlDataset(IterableDataset): def __init__(self, dbname): self.db = sqlite3.connect(dbname) self.db.execute(\"create table if not exists data (inputs blob, targets blob)\") def add(self, inputs, targets): self.db.execute(\"insert into data (inputs, targets) values (?, ?)\", (wds.torch_dumps(inputs), wds.torch_dumps(targets))) def __iter__(self): query = \"select inputs, targets from data\" cursor = self.db.execute(query) for inputs, targets in cursor: yield wds.torch_loads(inputs), wds.torch_loads(targets) def __len__(self): return self.db.execute(\"select count(*) from data\").fetchone()[0] !rm -f test.db dataset = SqlDataset(\"test.db\") size=32 for i in range(1000): dataset.add(torch.randn(3, size, size), torch.randn(3, size, size)) print(len(dataset), next(iter(dataset))[0].shape) 1000 torch.Size([3, 32, 32]) Now we can chain this IterableDataset implementation with webdataset.Processor : dataset = wds.Processor(dataset, wds.shuffle, 100) dataset = wds.Processor(dataset, wds.batched, 16) noisy_dataset = wds.Processor(dataset, add_noise, noise=0.02) print(next(iter(noisy_dataset))[0].shape) torch.Size([16, 3, 32, 32])","title":"How it Works"},{"location":"multinode/","text":"%pylab inline import torch import webdataset as wds import braceexpand Populating the interactive namespace from numpy and matplotlib Splitting Shards across Nodes and Workers Unlike traditional PyTorch Dataset instances, WebDataset splits data across nodes at the shard level, not at the sample level. This functionality is handled inside the ShardList class. Recall that dataset = webdataset.Webdataset(urls) is just a shorthand for: urls = list(braceexpand.braceexpand(\"dataset-{000000..000999}.tar\")) dataset = wds.ShardList(urls, splitter=wds.split_by_worker, nodesplitter=wds.split_by_node, shuffle=False) dataset = wds.Processor(dataset, wds.url_opener) dataset = wds.Processor(dataset, wds.tar_file_expander) dataset = wds.Processor(dataset, wds.group_by_keys) Here, nodesplitter and splitter are functions that are called inside ShardList to split up the URLs in urls by node and worker. You can use any functions you like there, all they need to do is take a list of URLs and return a subset of those URLs as a result. The default split_by_worker looks roughly like: def my_split_by_worker(urls): wi = torch.utils.data.get_worker_info() if wi is None: return urls else: return urls[wi.id::wi.num_workers] The same approach works for multiple worker nodes: def my_split_by_node(urls): node_id, node_count = torch.distributed.get_rank(), torch.distributed.get_world_size() return urls[node_id::node_count] dataset = wds.WebDataset(urls, splitter=my_split_by_worker, nodesplitter=my_split_by_node) Of course, you can also create more complex splitting strategies if necessary. DistributedDataParallel DistributedDataParallel training requires that each participating node receive exactly the same number of training batches as all others. The ddp_equalize method ensures this: urls = \"./shards/imagenet-train-{000000..001281}.tar\" dataset_size, batch_size = 1282000, 64 dataset = wds.WebDataset(urls).decode(\"pil\").shuffle(5000).batched(batch_size, partial=False) loader = wds.WebLoader(dataset, num_workers=4) loader = loader.ddp_equalize(dataset_size // batch_size) You need to give the total number of batches in your dataset to ddp_equalize ; it will compute the batches per node from this and equalize batches accordingly. You need to apply ddp_equalize to the WebLoader rather than the Dataset .","title":"Multinode"},{"location":"multinode/#splitting-shards-across-nodes-and-workers","text":"Unlike traditional PyTorch Dataset instances, WebDataset splits data across nodes at the shard level, not at the sample level. This functionality is handled inside the ShardList class. Recall that dataset = webdataset.Webdataset(urls) is just a shorthand for: urls = list(braceexpand.braceexpand(\"dataset-{000000..000999}.tar\")) dataset = wds.ShardList(urls, splitter=wds.split_by_worker, nodesplitter=wds.split_by_node, shuffle=False) dataset = wds.Processor(dataset, wds.url_opener) dataset = wds.Processor(dataset, wds.tar_file_expander) dataset = wds.Processor(dataset, wds.group_by_keys) Here, nodesplitter and splitter are functions that are called inside ShardList to split up the URLs in urls by node and worker. You can use any functions you like there, all they need to do is take a list of URLs and return a subset of those URLs as a result. The default split_by_worker looks roughly like: def my_split_by_worker(urls): wi = torch.utils.data.get_worker_info() if wi is None: return urls else: return urls[wi.id::wi.num_workers] The same approach works for multiple worker nodes: def my_split_by_node(urls): node_id, node_count = torch.distributed.get_rank(), torch.distributed.get_world_size() return urls[node_id::node_count] dataset = wds.WebDataset(urls, splitter=my_split_by_worker, nodesplitter=my_split_by_node) Of course, you can also create more complex splitting strategies if necessary.","title":"Splitting Shards across Nodes and Workers"},{"location":"multinode/#distributeddataparallel","text":"DistributedDataParallel training requires that each participating node receive exactly the same number of training batches as all others. The ddp_equalize method ensures this: urls = \"./shards/imagenet-train-{000000..001281}.tar\" dataset_size, batch_size = 1282000, 64 dataset = wds.WebDataset(urls).decode(\"pil\").shuffle(5000).batched(batch_size, partial=False) loader = wds.WebLoader(dataset, num_workers=4) loader = loader.ddp_equalize(dataset_size // batch_size) You need to give the total number of batches in your dataset to ddp_equalize ; it will compute the batches per node from this and equalize batches accordingly. You need to apply ddp_equalize to the WebLoader rather than the Dataset .","title":"DistributedDataParallel"},{"location":"objectron-conversion/","text":"Objectron conversion to WebDataset Format import re import os import os.path import json assert os.path.exists(\"objectron-files.txt\") # create with: # !gsutil ls -r gs://objectron > objectron-files.txt Parameters shard_size = 3 # samples bucket = \"tmbdev-objectron\" only_with_anno = False # only keep samples with annotation max_shards = 5 # for testing; for production, set to 99999 Creating the File Lists Read the complete file list and find the video files. files = set(x.strip() for x in open(\"objectron-files.txt\").readlines()) movs = set(x for x in files if \"video.MOV\" in x) len(files), len(movs) (210448, 20088) Assemble a list of samples, each sample comprising the video file, its corresponding geometry, and the annotation. def cleanpath(s): return re.sub(\"gs://objectron/videos/\", \"\", s).lower() samples = [] for mov in movs: base = re.sub(\"/video.MOV\", \"\", mov) geo = base + \"/geometry.pbdata\" anno = re.sub(\"/videos/\", \"/annotations/\", base) + \".pbdata\" sample = [f\"\"\"{cleanpath(mov)} pipe:gsutil cat {mov}\"\"\"] sample += [f\"\"\"{cleanpath(geo)} pipe:gsutil cat {geo}\"\"\"] if anno in files and not only_with_anno: # fix up the path to be in the same directory sample += [f\"\"\"{cleanpath(base+\"/anno.pbdata\")} pipe:gsutil cat {anno}\"\"\"] samples.append(sample) print(samples[0]) len(samples) ['bottle/batch-27/43/video.mov pipe:gsutil cat gs://objectron/videos/bottle/batch-27/43/video.MOV', 'bottle/batch-27/43/geometry.pbdata pipe:gsutil cat gs://objectron/videos/bottle/batch-27/43/geometry.pbdata', 'bottle/batch-27/43/anno.pbdata pipe:gsutil cat gs://objectron/annotations/bottle/batch-27/43.pbdata'] 20088 Split up the complete list of samples into shards of size shard_size . shards = [] for i in range(0, len(samples), shard_size): shards.append(samples[i:i+shard_size]) shards = [[x for l in shard for x in l] for shard in shards] shards = shards[:max_shards] print(shards[0][:10]) print(len(shards)) ['bottle/batch-27/43/video.mov pipe:gsutil cat gs://objectron/videos/bottle/batch-27/43/video.MOV', 'bottle/batch-27/43/geometry.pbdata pipe:gsutil cat gs://objectron/videos/bottle/batch-27/43/geometry.pbdata', 'bottle/batch-27/43/anno.pbdata pipe:gsutil cat gs://objectron/annotations/bottle/batch-27/43.pbdata', 'laptop/batch-3/16/video.mov pipe:gsutil cat gs://objectron/videos/laptop/batch-3/16/video.MOV', 'laptop/batch-3/16/geometry.pbdata pipe:gsutil cat gs://objectron/videos/laptop/batch-3/16/geometry.pbdata', 'laptop/batch-3/16/anno.pbdata pipe:gsutil cat gs://objectron/annotations/laptop/batch-3/16.pbdata', 'shoe/batch-34/7/video.mov pipe:gsutil cat gs://objectron/videos/shoe/batch-34/7/video.MOV', 'shoe/batch-34/7/geometry.pbdata pipe:gsutil cat gs://objectron/videos/shoe/batch-34/7/geometry.pbdata', 'shoe/batch-34/7/anno.pbdata pipe:gsutil cat gs://objectron/annotations/shoe/batch-34/7.pbdata'] 5 os.system(\"gsutil rm \") for i, f in enumerate(shards): print(i, end=\" \", flush=True) with os.popen(f\"gsutil cp - gs://{bucket}/objectron-{i:04d}.txt\", \"w\") as stream: stream.write(\"\\n\".join(f) + \"\\n\") 0 1 2 3 4 Creating the Shards First, a simple function that takes a \".txt\" file and creates the corresponding shard. The core of the task is just handled by a simple shell command. import os def makeshard(src): output = re.sub(\".txt$\", \".tar\", src) assert output != src # output creation on GCS is atomic, so if the file exists, we're done if os.system(f\"gsutil stat {output}\") == 0: return f\"{output}: already exists\" # create the .tar shard in a fully streaming mode cmd = f\"gsutil cat {src} | tarp create - -o - | gsutil cp - {output}\" print(cmd) assert 0 == os.system(cmd) return f\"{output}: OK\" makeshard(\"gs://tmbdev-objectron/objectron-0000.txt\") 'gs://tmbdev-objectron/objectron-0000.tar: already exists' Parallel Execution Next, let's parallelize that with Dask. from dask.distributed import Client from dask import delayed import dask import dask.bag as db client = Client(n_workers=4) npartitions = 4 # used below client /home/tmb/proj/webdataset/venv/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use. Perhaps you already have a cluster running? Hosting the HTTP server on port 46693 instead warnings.warn( Client Scheduler: tcp://127.0.0.1:44381 Dashboard: http://127.0.0.1:46693/status Cluster Workers: 4 Cores: 24 Memory: 67.46 GB sources = [s.strip() for s in os.popen(f\"gsutil ls gs://{bucket}/objectron-*.txt\").readlines()] sources = db.from_sequence(sources, npartitions=npartitions) results = sources.map(makeshard) results.compute() ['gs://tmbdev-objectron/objectron-0000.tar: already exists', 'gs://tmbdev-objectron/objectron-0001.tar: already exists', 'gs://tmbdev-objectron/objectron-0002.tar: already exists', 'gs://tmbdev-objectron/objectron-0003.tar: already exists', 'gs://tmbdev-objectron/objectron-0004.tar: already exists'] Running It for Real Note that if you want to run this for real, you need to: change shard_size to something like 50-100 change the bucket change max_shards to 999999 set up dask to run actually distributed","title":"Objectron"},{"location":"objectron-conversion/#objectron-conversion-to-webdataset-format","text":"import re import os import os.path import json assert os.path.exists(\"objectron-files.txt\") # create with: # !gsutil ls -r gs://objectron > objectron-files.txt","title":"Objectron conversion to WebDataset Format"},{"location":"objectron-conversion/#parameters","text":"shard_size = 3 # samples bucket = \"tmbdev-objectron\" only_with_anno = False # only keep samples with annotation max_shards = 5 # for testing; for production, set to 99999","title":"Parameters"},{"location":"objectron-conversion/#creating-the-file-lists","text":"Read the complete file list and find the video files. files = set(x.strip() for x in open(\"objectron-files.txt\").readlines()) movs = set(x for x in files if \"video.MOV\" in x) len(files), len(movs) (210448, 20088) Assemble a list of samples, each sample comprising the video file, its corresponding geometry, and the annotation. def cleanpath(s): return re.sub(\"gs://objectron/videos/\", \"\", s).lower() samples = [] for mov in movs: base = re.sub(\"/video.MOV\", \"\", mov) geo = base + \"/geometry.pbdata\" anno = re.sub(\"/videos/\", \"/annotations/\", base) + \".pbdata\" sample = [f\"\"\"{cleanpath(mov)} pipe:gsutil cat {mov}\"\"\"] sample += [f\"\"\"{cleanpath(geo)} pipe:gsutil cat {geo}\"\"\"] if anno in files and not only_with_anno: # fix up the path to be in the same directory sample += [f\"\"\"{cleanpath(base+\"/anno.pbdata\")} pipe:gsutil cat {anno}\"\"\"] samples.append(sample) print(samples[0]) len(samples) ['bottle/batch-27/43/video.mov pipe:gsutil cat gs://objectron/videos/bottle/batch-27/43/video.MOV', 'bottle/batch-27/43/geometry.pbdata pipe:gsutil cat gs://objectron/videos/bottle/batch-27/43/geometry.pbdata', 'bottle/batch-27/43/anno.pbdata pipe:gsutil cat gs://objectron/annotations/bottle/batch-27/43.pbdata'] 20088 Split up the complete list of samples into shards of size shard_size . shards = [] for i in range(0, len(samples), shard_size): shards.append(samples[i:i+shard_size]) shards = [[x for l in shard for x in l] for shard in shards] shards = shards[:max_shards] print(shards[0][:10]) print(len(shards)) ['bottle/batch-27/43/video.mov pipe:gsutil cat gs://objectron/videos/bottle/batch-27/43/video.MOV', 'bottle/batch-27/43/geometry.pbdata pipe:gsutil cat gs://objectron/videos/bottle/batch-27/43/geometry.pbdata', 'bottle/batch-27/43/anno.pbdata pipe:gsutil cat gs://objectron/annotations/bottle/batch-27/43.pbdata', 'laptop/batch-3/16/video.mov pipe:gsutil cat gs://objectron/videos/laptop/batch-3/16/video.MOV', 'laptop/batch-3/16/geometry.pbdata pipe:gsutil cat gs://objectron/videos/laptop/batch-3/16/geometry.pbdata', 'laptop/batch-3/16/anno.pbdata pipe:gsutil cat gs://objectron/annotations/laptop/batch-3/16.pbdata', 'shoe/batch-34/7/video.mov pipe:gsutil cat gs://objectron/videos/shoe/batch-34/7/video.MOV', 'shoe/batch-34/7/geometry.pbdata pipe:gsutil cat gs://objectron/videos/shoe/batch-34/7/geometry.pbdata', 'shoe/batch-34/7/anno.pbdata pipe:gsutil cat gs://objectron/annotations/shoe/batch-34/7.pbdata'] 5 os.system(\"gsutil rm \") for i, f in enumerate(shards): print(i, end=\" \", flush=True) with os.popen(f\"gsutil cp - gs://{bucket}/objectron-{i:04d}.txt\", \"w\") as stream: stream.write(\"\\n\".join(f) + \"\\n\") 0 1 2 3 4","title":"Creating the File Lists"},{"location":"objectron-conversion/#creating-the-shards","text":"First, a simple function that takes a \".txt\" file and creates the corresponding shard. The core of the task is just handled by a simple shell command. import os def makeshard(src): output = re.sub(\".txt$\", \".tar\", src) assert output != src # output creation on GCS is atomic, so if the file exists, we're done if os.system(f\"gsutil stat {output}\") == 0: return f\"{output}: already exists\" # create the .tar shard in a fully streaming mode cmd = f\"gsutil cat {src} | tarp create - -o - | gsutil cp - {output}\" print(cmd) assert 0 == os.system(cmd) return f\"{output}: OK\" makeshard(\"gs://tmbdev-objectron/objectron-0000.txt\") 'gs://tmbdev-objectron/objectron-0000.tar: already exists'","title":"Creating the Shards"},{"location":"objectron-conversion/#parallel-execution","text":"Next, let's parallelize that with Dask. from dask.distributed import Client from dask import delayed import dask import dask.bag as db client = Client(n_workers=4) npartitions = 4 # used below client /home/tmb/proj/webdataset/venv/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use. Perhaps you already have a cluster running? Hosting the HTTP server on port 46693 instead warnings.warn(","title":"Parallel Execution"},{"location":"objectron-conversion/#running-it-for-real","text":"Note that if you want to run this for real, you need to: change shard_size to something like 50-100 change the bucket change max_shards to 999999 set up dask to run actually distributed","title":"Running It for Real"},{"location":"sharding/","text":"Sharding, Parallel I/O, and DataLoader WebDataset datasets are usually split into many shards; this is both to achieve parallel I/O and to shuffle data. %pylab inline import torch from torch.utils.data import IterableDataset from torchvision import transforms import webdataset as wds from itertools import islice Populating the interactive namespace from numpy and matplotlib Sets of shards can be given as a list of files, or they can be written using the brace notation, as in openimages-train-{000000..000554}.tar . For example, the OpenImages dataset consists of 554 shards, each containing about 1 Gbyte of images. You can open the entire dataset as follows (note the explicit use of both shardshuffle=True (for shuffling the shards and the .shuffle processor for shuffling samples inline). url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\" url = f\"pipe:curl -L -s {url} || true\" normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) preproc = transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize, ]) dataset = ( wds.WebDataset(url, shardshuffle=True) .shuffle(100) .decode(\"pil\") .to_tuple(\"jpg;png\", \"json\") .map_tuple(preproc) ) x, y = next(iter(dataset)) print(x.shape, str(y)[:50]) torch.Size([3, 224, 224]) [{'ImageID': '19a7594f418fe39e', 'Source': 'xclick When used with a standard Torch DataLoader , this will would perform parallel I/O and preprocessing. However, the recommended way of using IterableDataset with DataLoader is to do the batching explicitly in the Dataset : batch_size = 20 dataloader = torch.utils.data.DataLoader(dataset.batched(batch_size), num_workers=4, batch_size=None) images, targets = next(iter(dataloader)) images.shape torch.Size([20, 3, 224, 224]) Explicit Dataset Sizes Ideally, you shouldn't use len(dataset) or len(loader) at all in your training loop. However, some code may use calls to the len(.) function. WebDataset generally propagates such calls back through the chain of dataset processors. Generally, IterableDataset implementations don't have a size, but you can specify an explicit size using the length= argument to WebDataset . You can also use the ResizedDataset class to force an IterableDataset to have a specific epoch length and (if desired) set a separate nominal epoch length.","title":"Sharding"},{"location":"sharding/#sharding-parallel-io-and-dataloader","text":"WebDataset datasets are usually split into many shards; this is both to achieve parallel I/O and to shuffle data. %pylab inline import torch from torch.utils.data import IterableDataset from torchvision import transforms import webdataset as wds from itertools import islice Populating the interactive namespace from numpy and matplotlib Sets of shards can be given as a list of files, or they can be written using the brace notation, as in openimages-train-{000000..000554}.tar . For example, the OpenImages dataset consists of 554 shards, each containing about 1 Gbyte of images. You can open the entire dataset as follows (note the explicit use of both shardshuffle=True (for shuffling the shards and the .shuffle processor for shuffling samples inline). url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\" url = f\"pipe:curl -L -s {url} || true\" normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) preproc = transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize, ]) dataset = ( wds.WebDataset(url, shardshuffle=True) .shuffle(100) .decode(\"pil\") .to_tuple(\"jpg;png\", \"json\") .map_tuple(preproc) ) x, y = next(iter(dataset)) print(x.shape, str(y)[:50]) torch.Size([3, 224, 224]) [{'ImageID': '19a7594f418fe39e', 'Source': 'xclick When used with a standard Torch DataLoader , this will would perform parallel I/O and preprocessing. However, the recommended way of using IterableDataset with DataLoader is to do the batching explicitly in the Dataset : batch_size = 20 dataloader = torch.utils.data.DataLoader(dataset.batched(batch_size), num_workers=4, batch_size=None) images, targets = next(iter(dataloader)) images.shape torch.Size([20, 3, 224, 224])","title":"Sharding, Parallel I/O, and DataLoader"},{"location":"sharding/#explicit-dataset-sizes","text":"Ideally, you shouldn't use len(dataset) or len(loader) at all in your training loop. However, some code may use calls to the len(.) function. WebDataset generally propagates such calls back through the chain of dataset processors. Generally, IterableDataset implementations don't have a size, but you can specify an explicit size using the length= argument to WebDataset . You can also use the ResizedDataset class to force an IterableDataset to have a specific epoch length and (if desired) set a separate nominal epoch length.","title":"Explicit Dataset Sizes"},{"location":"sources/","text":"import webdataset as wds import braceexpand from torch.utils.data import IterableDataset from webdataset import gopen Local and Remote Storage URLs WebDataset refers to data sources using file paths or URLs. The following are all valid ways of referring to a data source: dataset = wds.WebDataset(\"dataset-000.tar\") dataset = wds.WebDataset(\"file:dataset-000.tar\") dataset = wds.WebDataset(\"http://server/dataset-000.tar\") An additional way of referring to data is using the pipe: scheme, so the following is also equivalent to the above references: dataset = wds.WebDataset(\"pipe:cat dataset-000.tar\") You can use the same notation for accessing data in cloud storage: dataset = wds.WebDataset(\"pipe:gsutil cat gs://somebucket/dataset-000.tar\") Note that access to standard web schemas are implemented using curl . That is, http://server/dataset.tar is internally simply treated like pipe:curl -s -L 'http://server/dataset.tar' . The use of curl to access Internet protocols actually is more efficient than using the built-in http library because it results in asynchronous name resolution and downloads. File opening is handled by webdataset.gopen.gopen . This is a small function that just wraps standard Python file I/O and pipe capabilities. You can define handlers for new schemes or override implementations for existing schemes by adding entries to wds.gopen_schemes : def gopen_gs(url, mode=\"rb\", bufsize=8192): ... gopen.gopen_schemes[\"gs\"] = gopen_gs Standard Input/Output For the following examples, assume that we have a program called image-classifier that takes a WebDataset containing just JPEG files as input and produces a WebDataset containing JPEG files and their corresponding classifications in JSON format: image-classifier input-shard.tar --output=output-shard.tar --model=some-model.pth As a special case, the string \"-\" refers to standard input (reading) or standard output (writing). This allows code using WebDataset to be used as part of pipes. This is useful, for example, inside Kubernetes containers with limited local storage. Assume that you store shards in Google Cloud and access it with gsutil . Using \"-\", you can simply write: gsutil cat gs://input-bucket/data-000174.tar | image-classifer - -o - | gsutil cp - gs://output-bucket/output-000174.tar It's also useful to create shards on the fly using tar and extract the result immediately; this lets you use shard based programs directly for operating on individual files. For example, for the image-classifier program above, you can write: tar cf - *.jpg | shard-classifier - -o - | tar xvf - --include '.json' This is the rough equivalent of: for fname in *.jpg; do image-classifier $fname > $(basename $fname .jpg).cls done Multiple Shards and Mixing Datasets The WebDataset and ShardList classes take either a string or a list of strings as an argument. When given a string, the string is expanded using braceexpand . Therefore, the following three datasets are equivalent: dataset = wds.WebDataset([\"dataset-000.tar\", \"dataset-001.tar\", \"dataset-002.tar\", \"dataset-003.tar\"]) dataset = wds.WebDataset(\"dataset-{000..003}.tar\") dataset = wds.WebDataset(\"file:dataset-{000..003}.tar\") For complex training problems, you may want to mix multiple datasets, where each dataset consists of multiple shards. A good way is to expand each shard spec individually using braceexpand and concatenate the lists. Then you can pass the result list as an argument to WebDataset . urls = ( list(braceexpand.braceexpand(\"imagenet-{000000..000146}.tar\")) + list(braceexpand.braceexpand(\"openimages-{000000..000547}.tar\")) + list(braceexpand.braceexpand(\"custom-images-{000000..000999}.tar\")) ) print(len(urls)) dataset = wds.WebDataset(urls, shardshuffle=True).shuffle(10000).decode(\"torchrgb\") 1695 Mixing Datsets with a Custom IterableDataset Class For more complex sampling problems, you can also write sample processors. For example, to sample equally from several datasets, you could write something like this (the Shorthands and Composable base classes just add some convenience methods): class SampleEqually(IterableDataset, wds.Shorthands, wds.Composable): def __init__(self, datasets): super().__init__() self.datasets = datasets def __iter__(self): sources = [iter(ds) for ds in self.datasets] while True: for source in sources: try: yield next(source) except StopIteration: return Now we can mix samples from different sources in more complex ways: dataset1 = wds.WebDataset(\"imagenet-{000000..000146}.tar\", shardshuffle=True).shuffle(1000).decode(\"torchrgb\") dataset2 = wds.WebDataset(\"openimages-{000000..000547}.tar\", shardshuffle=True).shuffle(1000).decode(\"torchrgb\") dataset3 = wds.WebDataset(\"custom-images-{000000..000999}.tar\", shardshuffle=True).shuffle(1000).decode(\"torchrgb\") dataset = SampleEqually([dataset1, dataset2, dataset3]).shuffle(1000)","title":"Sources"},{"location":"sources/#local-and-remote-storage-urls","text":"WebDataset refers to data sources using file paths or URLs. The following are all valid ways of referring to a data source: dataset = wds.WebDataset(\"dataset-000.tar\") dataset = wds.WebDataset(\"file:dataset-000.tar\") dataset = wds.WebDataset(\"http://server/dataset-000.tar\") An additional way of referring to data is using the pipe: scheme, so the following is also equivalent to the above references: dataset = wds.WebDataset(\"pipe:cat dataset-000.tar\") You can use the same notation for accessing data in cloud storage: dataset = wds.WebDataset(\"pipe:gsutil cat gs://somebucket/dataset-000.tar\") Note that access to standard web schemas are implemented using curl . That is, http://server/dataset.tar is internally simply treated like pipe:curl -s -L 'http://server/dataset.tar' . The use of curl to access Internet protocols actually is more efficient than using the built-in http library because it results in asynchronous name resolution and downloads. File opening is handled by webdataset.gopen.gopen . This is a small function that just wraps standard Python file I/O and pipe capabilities. You can define handlers for new schemes or override implementations for existing schemes by adding entries to wds.gopen_schemes : def gopen_gs(url, mode=\"rb\", bufsize=8192): ... gopen.gopen_schemes[\"gs\"] = gopen_gs","title":"Local and Remote Storage URLs"},{"location":"sources/#standard-inputoutput","text":"For the following examples, assume that we have a program called image-classifier that takes a WebDataset containing just JPEG files as input and produces a WebDataset containing JPEG files and their corresponding classifications in JSON format: image-classifier input-shard.tar --output=output-shard.tar --model=some-model.pth As a special case, the string \"-\" refers to standard input (reading) or standard output (writing). This allows code using WebDataset to be used as part of pipes. This is useful, for example, inside Kubernetes containers with limited local storage. Assume that you store shards in Google Cloud and access it with gsutil . Using \"-\", you can simply write: gsutil cat gs://input-bucket/data-000174.tar | image-classifer - -o - | gsutil cp - gs://output-bucket/output-000174.tar It's also useful to create shards on the fly using tar and extract the result immediately; this lets you use shard based programs directly for operating on individual files. For example, for the image-classifier program above, you can write: tar cf - *.jpg | shard-classifier - -o - | tar xvf - --include '.json' This is the rough equivalent of: for fname in *.jpg; do image-classifier $fname > $(basename $fname .jpg).cls done","title":"Standard Input/Output"},{"location":"sources/#multiple-shards-and-mixing-datasets","text":"The WebDataset and ShardList classes take either a string or a list of strings as an argument. When given a string, the string is expanded using braceexpand . Therefore, the following three datasets are equivalent: dataset = wds.WebDataset([\"dataset-000.tar\", \"dataset-001.tar\", \"dataset-002.tar\", \"dataset-003.tar\"]) dataset = wds.WebDataset(\"dataset-{000..003}.tar\") dataset = wds.WebDataset(\"file:dataset-{000..003}.tar\") For complex training problems, you may want to mix multiple datasets, where each dataset consists of multiple shards. A good way is to expand each shard spec individually using braceexpand and concatenate the lists. Then you can pass the result list as an argument to WebDataset . urls = ( list(braceexpand.braceexpand(\"imagenet-{000000..000146}.tar\")) + list(braceexpand.braceexpand(\"openimages-{000000..000547}.tar\")) + list(braceexpand.braceexpand(\"custom-images-{000000..000999}.tar\")) ) print(len(urls)) dataset = wds.WebDataset(urls, shardshuffle=True).shuffle(10000).decode(\"torchrgb\") 1695","title":"Multiple Shards and Mixing Datasets"},{"location":"sources/#mixing-datsets-with-a-custom-iterabledataset-class","text":"For more complex sampling problems, you can also write sample processors. For example, to sample equally from several datasets, you could write something like this (the Shorthands and Composable base classes just add some convenience methods): class SampleEqually(IterableDataset, wds.Shorthands, wds.Composable): def __init__(self, datasets): super().__init__() self.datasets = datasets def __iter__(self): sources = [iter(ds) for ds in self.datasets] while True: for source in sources: try: yield next(source) except StopIteration: return Now we can mix samples from different sources in more complex ways: dataset1 = wds.WebDataset(\"imagenet-{000000..000146}.tar\", shardshuffle=True).shuffle(1000).decode(\"torchrgb\") dataset2 = wds.WebDataset(\"openimages-{000000..000547}.tar\", shardshuffle=True).shuffle(1000).decode(\"torchrgb\") dataset3 = wds.WebDataset(\"custom-images-{000000..000999}.tar\", shardshuffle=True).shuffle(1000).decode(\"torchrgb\") dataset = SampleEqually([dataset1, dataset2, dataset3]).shuffle(1000)","title":"Mixing Datsets with a Custom IterableDataset Class"},{"location":"video-loading-example/","text":"%pylab inline Populating the interactive namespace from numpy and matplotlib import webdataset as wds Simple Examples of Reading Video with WebDataset There are many different ways in which video can be used in deep learning. One of the more common ones is to break up large video sequences into fixed sized clips and load those clips into a time x channel x height x width tensor. Breaking up large sequences into clips can be done on-the-fly or it can be done ahead of time. On-the-fly splitting of video is common but can be resource intensive, difficult to debug, and result in poor shuffling. WebDataset can be used with on-the-fly splitting, but when training on video clips with WebDataset, we generally prefer to split videos into clips ahead of time, shuffle the clips, and then just treat them as if they were regular samples. Splitting and shuffling of videos for WebDataset can be done with a small shell script and using FFMPEG; here is a notebook that illustrates this. We're going to look at two common sets of representing videos for deep learning: each video clip is stored in a video format like .mp4 each video clip is represented as a sequence of frames, stored as separate images There are a couple of video datasets you can experiment with, stored in Google Cloud: gs://nvdata-fallingthings contains the Falling Things dataset from NVIDIA; this uses separate image storage; you can find here... the original dataset ( fat.zip ) the original dataset split into shards, with one shard per sequence and one sample per frame the frames of the original dataset in shuffled order; this is useful for training segmentation and stereo algorithms, but not motion a derived dataset containing shuffled samples each representing three consecutive frames; this is useful for learning optical flow and motion segmentation gs://nvdata-ytsamples contains a small subset of the videos from Google's YouTube8m dataset this just contains 10 shards of 1Gbyte size each with short .mp4 clips When using WebDataset, it is common to create many derived datasets, rather than performing complex transformations on the fly on a single master dataset. That's because creating derived datasets is easy and fast, and because separating augmentation/transformation from deep learning also tends to be easier to manage. This may seem less efficient at first glance, but it is actually usually also more cost effective, since adding rotational drives for extra storage is cheaper than adding extra CPU and I/O performance necessary for on-the-fly augmentation. The examples presented here internally use TorchVision for video decoding and PIL for image decoding; these are CPU-based decoders. You can use GPU-based image and video decompression with WebDataset; we will illustrate that in a future set of examples. YTSamples Data This dataset is a tiny set of fixed sized clips from the YT8m dataset. It's just provided to illustrate video I/O. In this dataset, each sample is stored as four files: sample.info.json - content metadata sample.mp4.json - video metadata sample.mp4 - MP4 encoded video clip The default decoder in WebDataset doesn't decode videos. However, the library provides a simple decoder based on torchvision (you need to install torchvision for this to work, including Python's av package). The decode method takes as arguments a sequence of function callbacks for decoding the fields making up a sample. To decode videos, just add the wds.torch_video callback. For inspecting it, we can open the video dataset directly from its cloud storage bucket; for actual training, you want to store it on a local file/web/object server to get better I/O speed. ds = wds.Dataset(\"https://storage.googleapis.com/nvdata-ytsamples/yt8m-clips-{000000..000009}.tar\").decode(wds.torch_video) for sample in ds: break /home/tmb/.local/lib/python3.8/site-packages/torchvision/io/video.py:103: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'. warnings.warn( sample.keys() dict_keys(['__key__', 'info.json', 'mp4', 'mp4.json']) Note that torchvision returns two tensors, the first containing the video, the second containing the audio. Therefore sample[\"mp4\"][0] is the tensor containing the video clip. Let's look at the first frame. imshow(sample[\"mp4\"][0][0]) sample[\"mp4\"][0].shape torch.Size([250, 128, 256, 3]) Let's look at 36 frames from the video. figsize(16, 9) n = len(sample[\"mp4\"][0]) for i, f in enumerate(sample[\"mp4\"][0][0:n-36:n//36]): subplot(6, 6, i+1) imshow(f) Falling Things Data Clips in the YT8m dataset are stored as video files. In many applications, videos are actually stored as sequences of individual images. The alling-things-3frames-shuffled dataset provides an example of this (it is derived from the Falling Things dataset). !curl -L -s https://storage.googleapis.com/nvdata-fallingthings/clips/falling-things-3frames-shuffled-000000.tar | tar tvf - | head -rwxr-xr-x bigdata/bigdata 278214 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000001.image.jpg -rwxr-xr-x bigdata/bigdata 201386 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000002.depth.png -rwxr-xr-x bigdata/bigdata 242860 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000002.image.jpg -rwxr-xr-x bigdata/bigdata 117790 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000000.depth.png -rwxr-xr-x bigdata/bigdata 206575 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000000.image.jpg -rwxr-xr-x bigdata/bigdata 199519 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000001.depth.png -rwxr-xr-x bigdata/bigdata 362315 2020-09-03 07:52 fat/mixed/kitedemo_4/000716/frame.000001.depth.png -rwxr-xr-x bigdata/bigdata 289780 2020-09-03 07:52 fat/mixed/kitedemo_4/000716/frame.000001.image.jpg -rwxr-xr-x bigdata/bigdata 515337 2020-09-03 07:52 fat/mixed/kitedemo_4/000716/frame.000002.depth.png -rwxr-xr-x bigdata/bigdata 344835 2020-09-03 07:52 fat/mixed/kitedemo_4/000716/frame.000002.image.jpg tar: write error As you can see, each clip is stored as a collection of images with the same basename ( .../frame in this case, though there is nothing special about that), and then a numbered sequence of image files (again, this is just a convention, albeit a convenient one). We can open this like any other WebDataset. Since we want images decoded, we need to specify an image decoder. We can use a string shorthand for doing this; .decode(\"rgb\") is short for .decode(wds.imagehandler(\"rgb\")) . fat_3frames = \"https://storage.googleapis.com/nvdata-fallingthings/clips/falling-things-3frames-shuffled-{000000..000004}.tar\" ds = wds.Dataset(fat_3frames).decode(\"rgb\") for sample in ds: break sample.keys() dict_keys(['__key__', '000001.image.jpg', '000002.depth.png', '000002.image.jpg', '000000.depth.png', '000000.image.jpg', '000001.depth.png']) To reassemble the images into a 4D numerical array, we just loop through the frame numbers and combine the images we get. images = np.array([sample[f\"{i:06d}.image.jpg\"] for i in range(3)]) print(images.shape) (3, 540, 960, 3) imshow(np.amax(np.abs(images[1]-images[0]), axis=2)) <matplotlib.image.AxesImage at 0x7f2d460d6490> If you like, you can do this directly in the Dataset class using the .map method. def assemble_frames(sample): images = np.array([sample[f\"{i:06d}.image.jpg\"] for i in range(3)]) depths = np.array([sample[f\"{i:06d}.depth.png\"] for i in range(3)]) return images, depths ds = wds.Dataset(fat_3frames).decode(\"rgb\").map(assemble_frames) for images, depths in ds: break print(images.shape, depths.shape) (3, 540, 960, 3) (3, 540, 960, 3)","title":"Video Loading"},{"location":"video-loading-example/#simple-examples-of-reading-video-with-webdataset","text":"There are many different ways in which video can be used in deep learning. One of the more common ones is to break up large video sequences into fixed sized clips and load those clips into a time x channel x height x width tensor. Breaking up large sequences into clips can be done on-the-fly or it can be done ahead of time. On-the-fly splitting of video is common but can be resource intensive, difficult to debug, and result in poor shuffling. WebDataset can be used with on-the-fly splitting, but when training on video clips with WebDataset, we generally prefer to split videos into clips ahead of time, shuffle the clips, and then just treat them as if they were regular samples. Splitting and shuffling of videos for WebDataset can be done with a small shell script and using FFMPEG; here is a notebook that illustrates this. We're going to look at two common sets of representing videos for deep learning: each video clip is stored in a video format like .mp4 each video clip is represented as a sequence of frames, stored as separate images There are a couple of video datasets you can experiment with, stored in Google Cloud: gs://nvdata-fallingthings contains the Falling Things dataset from NVIDIA; this uses separate image storage; you can find here... the original dataset ( fat.zip ) the original dataset split into shards, with one shard per sequence and one sample per frame the frames of the original dataset in shuffled order; this is useful for training segmentation and stereo algorithms, but not motion a derived dataset containing shuffled samples each representing three consecutive frames; this is useful for learning optical flow and motion segmentation gs://nvdata-ytsamples contains a small subset of the videos from Google's YouTube8m dataset this just contains 10 shards of 1Gbyte size each with short .mp4 clips When using WebDataset, it is common to create many derived datasets, rather than performing complex transformations on the fly on a single master dataset. That's because creating derived datasets is easy and fast, and because separating augmentation/transformation from deep learning also tends to be easier to manage. This may seem less efficient at first glance, but it is actually usually also more cost effective, since adding rotational drives for extra storage is cheaper than adding extra CPU and I/O performance necessary for on-the-fly augmentation. The examples presented here internally use TorchVision for video decoding and PIL for image decoding; these are CPU-based decoders. You can use GPU-based image and video decompression with WebDataset; we will illustrate that in a future set of examples.","title":"Simple Examples of Reading Video with WebDataset"},{"location":"video-loading-example/#ytsamples-data","text":"This dataset is a tiny set of fixed sized clips from the YT8m dataset. It's just provided to illustrate video I/O. In this dataset, each sample is stored as four files: sample.info.json - content metadata sample.mp4.json - video metadata sample.mp4 - MP4 encoded video clip The default decoder in WebDataset doesn't decode videos. However, the library provides a simple decoder based on torchvision (you need to install torchvision for this to work, including Python's av package). The decode method takes as arguments a sequence of function callbacks for decoding the fields making up a sample. To decode videos, just add the wds.torch_video callback. For inspecting it, we can open the video dataset directly from its cloud storage bucket; for actual training, you want to store it on a local file/web/object server to get better I/O speed. ds = wds.Dataset(\"https://storage.googleapis.com/nvdata-ytsamples/yt8m-clips-{000000..000009}.tar\").decode(wds.torch_video) for sample in ds: break /home/tmb/.local/lib/python3.8/site-packages/torchvision/io/video.py:103: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'. warnings.warn( sample.keys() dict_keys(['__key__', 'info.json', 'mp4', 'mp4.json']) Note that torchvision returns two tensors, the first containing the video, the second containing the audio. Therefore sample[\"mp4\"][0] is the tensor containing the video clip. Let's look at the first frame. imshow(sample[\"mp4\"][0][0]) sample[\"mp4\"][0].shape torch.Size([250, 128, 256, 3]) Let's look at 36 frames from the video. figsize(16, 9) n = len(sample[\"mp4\"][0]) for i, f in enumerate(sample[\"mp4\"][0][0:n-36:n//36]): subplot(6, 6, i+1) imshow(f)","title":"YTSamples Data"},{"location":"video-loading-example/#falling-things-data","text":"Clips in the YT8m dataset are stored as video files. In many applications, videos are actually stored as sequences of individual images. The alling-things-3frames-shuffled dataset provides an example of this (it is derived from the Falling Things dataset). !curl -L -s https://storage.googleapis.com/nvdata-fallingthings/clips/falling-things-3frames-shuffled-000000.tar | tar tvf - | head -rwxr-xr-x bigdata/bigdata 278214 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000001.image.jpg -rwxr-xr-x bigdata/bigdata 201386 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000002.depth.png -rwxr-xr-x bigdata/bigdata 242860 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000002.image.jpg -rwxr-xr-x bigdata/bigdata 117790 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000000.depth.png -rwxr-xr-x bigdata/bigdata 206575 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000000.image.jpg -rwxr-xr-x bigdata/bigdata 199519 2020-09-03 07:52 fat/mixed/kitedemo_3/001166/frame.000001.depth.png -rwxr-xr-x bigdata/bigdata 362315 2020-09-03 07:52 fat/mixed/kitedemo_4/000716/frame.000001.depth.png -rwxr-xr-x bigdata/bigdata 289780 2020-09-03 07:52 fat/mixed/kitedemo_4/000716/frame.000001.image.jpg -rwxr-xr-x bigdata/bigdata 515337 2020-09-03 07:52 fat/mixed/kitedemo_4/000716/frame.000002.depth.png -rwxr-xr-x bigdata/bigdata 344835 2020-09-03 07:52 fat/mixed/kitedemo_4/000716/frame.000002.image.jpg tar: write error As you can see, each clip is stored as a collection of images with the same basename ( .../frame in this case, though there is nothing special about that), and then a numbered sequence of image files (again, this is just a convention, albeit a convenient one). We can open this like any other WebDataset. Since we want images decoded, we need to specify an image decoder. We can use a string shorthand for doing this; .decode(\"rgb\") is short for .decode(wds.imagehandler(\"rgb\")) . fat_3frames = \"https://storage.googleapis.com/nvdata-fallingthings/clips/falling-things-3frames-shuffled-{000000..000004}.tar\" ds = wds.Dataset(fat_3frames).decode(\"rgb\") for sample in ds: break sample.keys() dict_keys(['__key__', '000001.image.jpg', '000002.depth.png', '000002.image.jpg', '000000.depth.png', '000000.image.jpg', '000001.depth.png']) To reassemble the images into a 4D numerical array, we just loop through the frame numbers and combine the images we get. images = np.array([sample[f\"{i:06d}.image.jpg\"] for i in range(3)]) print(images.shape) (3, 540, 960, 3) imshow(np.amax(np.abs(images[1]-images[0]), axis=2)) <matplotlib.image.AxesImage at 0x7f2d460d6490> If you like, you can do this directly in the Dataset class using the .map method. def assemble_frames(sample): images = np.array([sample[f\"{i:06d}.image.jpg\"] for i in range(3)]) depths = np.array([sample[f\"{i:06d}.depth.png\"] for i in range(3)]) return images, depths ds = wds.Dataset(fat_3frames).decode(\"rgb\").map(assemble_frames) for images, depths in ds: break print(images.shape, depths.shape) (3, 540, 960, 3) (3, 540, 960, 3)","title":"Falling Things Data"},{"location":"writing/","text":"%pylab inline import torch from torch.utils.data import IterableDataset from torchvision import transforms import webdataset as wds from itertools import islice import numpy as np Populating the interactive namespace from numpy and matplotlib Writing Filters and Offline Augmentation Webdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels. def extract_class(data): # mock implementation return 0 def augment(a): a += torch.randn_like(a) * 0.01 return a def augment_wds(url, output, maxcount=999999999): src = ( wds.WebDataset(url) .decode(\"torchrgb\") .to_tuple(\"__key__\", \"jpg;png\", \"json\") .map_tuple(lambda x: x, augment) ) with wds.TarWriter(output) as dst: for key, image, data in islice(src, 0, maxcount): print(key) image = image.numpy().transpose(1, 2, 0) image -= amin(image) image /= amax(image) sample = { \"__key__\": key, \"png\": image, \"cls\": extract_class(data) } dst.write(sample) Now run the augmentation pipeline: url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" augment_wds(url, \"_temp.tar\", maxcount=5) e39871fd9fd74f55 f18b91585c4d3f3e ede6e66b2fb59aab ed600d57fcee4f94 ff47e649b23f446d To verify that things worked correctly, let's look at the output file: %%bash tar tf _temp.tar e39871fd9fd74f55.cls e39871fd9fd74f55.png f18b91585c4d3f3e.cls f18b91585c4d3f3e.png ede6e66b2fb59aab.cls ede6e66b2fb59aab.png ed600d57fcee4f94.cls ed600d57fcee4f94.png ff47e649b23f446d.cls ff47e649b23f446d.png If you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system. For example, using Dask, you could process all 554 shards in parallel using code like this: shards = braceexpand.braceexpand(\"{000000..000554}\") inputs = [f\"gs://bucket/openimages-{shard}.tar\" for shard in shards] outputs = [f\"gs://bucket2/openimages-augmented-{shard}.tar\" for shard in shards] results = [dask.delayed(augment_wds)(args) for args in zip(inputs, outputs)] dask.compute(*results) Note that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker. For very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes Job template, or using a workflow engine like Argo.","title":"Writing"},{"location":"writing/#writing-filters-and-offline-augmentation","text":"Webdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels. def extract_class(data): # mock implementation return 0 def augment(a): a += torch.randn_like(a) * 0.01 return a def augment_wds(url, output, maxcount=999999999): src = ( wds.WebDataset(url) .decode(\"torchrgb\") .to_tuple(\"__key__\", \"jpg;png\", \"json\") .map_tuple(lambda x: x, augment) ) with wds.TarWriter(output) as dst: for key, image, data in islice(src, 0, maxcount): print(key) image = image.numpy().transpose(1, 2, 0) image -= amin(image) image /= amax(image) sample = { \"__key__\": key, \"png\": image, \"cls\": extract_class(data) } dst.write(sample) Now run the augmentation pipeline: url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" augment_wds(url, \"_temp.tar\", maxcount=5) e39871fd9fd74f55 f18b91585c4d3f3e ede6e66b2fb59aab ed600d57fcee4f94 ff47e649b23f446d To verify that things worked correctly, let's look at the output file: %%bash tar tf _temp.tar e39871fd9fd74f55.cls e39871fd9fd74f55.png f18b91585c4d3f3e.cls f18b91585c4d3f3e.png ede6e66b2fb59aab.cls ede6e66b2fb59aab.png ed600d57fcee4f94.cls ed600d57fcee4f94.png ff47e649b23f446d.cls ff47e649b23f446d.png If you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system. For example, using Dask, you could process all 554 shards in parallel using code like this: shards = braceexpand.braceexpand(\"{000000..000554}\") inputs = [f\"gs://bucket/openimages-{shard}.tar\" for shard in shards] outputs = [f\"gs://bucket2/openimages-augmented-{shard}.tar\" for shard in shards] results = [dask.delayed(augment_wds)(args) for args in zip(inputs, outputs)] dask.compute(*results) Note that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker. For very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes Job template, or using a workflow engine like Argo.","title":"Writing Filters and Offline Augmentation"},{"location":"ytsamples-split/","text":"Getting Started This worksheet shows how you can preprocess and split videos for deep learning. In these examples, we use Google Cloud Storage. In order to actually run this worksheet, you need: Install Google's gsutil program Install the tarp utility But it's not necessary to run the worksheet; you can follow it and apply the principles to whatever cloud, object, or local storage you use. Sharded YouTube Files We have about 6M videos downloaded from YouTube (part of the YouTube 8m dataset released by Google). These videos are stored as 3000 shards, each containing about 2000 videos and each about 56 Gbytes large. There is just one sample shard stored in the public bucket gs://nvdata-ytsamples . !gsutil ls gs://nvdata-ytsamples/yt8m-lo-*.tar | head gs://nvdata-ytsamples/yt8m-lo-000000.tar Each shard contains the video itself ( .mp4 ) plus a lot of associated metadata. !gsutil cat gs://nvdata-ytsamples/yt8m-lo-000000.tar | tar tf - | head ---2pGwkL7M.annotations.xml ---2pGwkL7M.description ---2pGwkL7M.dllog ---2pGwkL7M.info.json ---2pGwkL7M.mp4 ---2pGwkL7M.txt -2cScG5TqjQ.annotations.xml -2cScG5TqjQ.description -2cScG5TqjQ.dllog -2cScG5TqjQ.info.json Splitting the Videos For training, we usually don't want to use the entire YouTube video at once; they are variable length and are hard to fit into the GPU. Instead, we want to split up the video into frames or short clips. Here, we are using a script based on ffmpeg to split up each video into a set of clips. We also rescale all the images to a common size. The input video for this script is assumed to be sample.mp4 , and the output clips will be stored in files like sample-000000.mp4 . %%writefile extract-segments.sh #!/bin/bash exec > $(dirname $0)/_extract-segments-last.log 1>&2 #ps auxw --sort -vsz | sed 5q set -e set -x set -a size=${size:-256:128} duration=${duration:-2} count=${count:-999999999} # get mp4 metadata (total length, etc.) ffprobe sample.mp4 -v quiet -print_format json -show_format -show_streams > sample.mp4.json # perform the rescaling and splitting ffmpeg -loglevel error -stats -i sample.mp4 \\ -vf \"scale=$size:force_original_aspect_ratio=decrease,pad=$size:(ow-iw)/2:(oh-ih)/2\" \\ -c:a copy -f segment -segment_time $duration -reset_timestamps 1 \\ -segment_format_options movflags=+faststart \\ sample-%06d.mp4 # copy the metadata into each video fragment for s in sample-??????.mp4; do b=$(basename $s .mp4) cp sample.mp4.json $b.mp4.json || true cp sample.info.json $b.info.json || true done Writing extract-segments.sh !chmod 755 ./extract-segments.sh Running the Script over All Videos in a Shard Next, we use the tarp command to iterate the above script over each .mp4 file in shard 000000 . !tarp proc --help Usage: tarp [OPTIONS] proc [proc-OPTIONS] [Inputs...] Application Options: -v verbose output Help Options: -h, --help Show this help message [proc command options] -f, --field= fields to extract; name or name=old1,old2,old3 -o, --outputs= output file --slice= slice of input stream -c, --command= shell command running in each sample dir -m, --multicommand= shell command running in each sample dir --shell= shell command running in each sample dir (default: /bin/bash) %%writefile splitit.sh gsutil cat gs://lpr-yt8m-lo-sharded/yt8m-lo-000000.tar | tarp proc -m $(pwd)/extract-segments.sh - -o - | tarp split - -o yt8m-clips-%06d.tar Writing splitit.sh We can now run this script using: $ bash ./splitit.sh It's best to do this outside Jupyter since Jupyter doesn't work with long running shell jobs. Of course, if you want to run this code over all 3000 shards, you probably will want to submit jobs based on splitit.sh to some job queuing system. Also note that the --post option to tarp split lets us upload output shards as soon as they have been created, allowing the script to work with very little local storage. Output Let's have a look at the output to make sure it's OK. !tar tf yt8m-clips-000000.tar | head ---2pGwkL7M/000000.mp4.json ---2pGwkL7M/000000.info.json ---2pGwkL7M/000000.mp4 ---2pGwkL7M/000001.info.json ---2pGwkL7M/000001.mp4 ---2pGwkL7M/000001.mp4.json ---2pGwkL7M/000002.info.json ---2pGwkL7M/000002.mp4 ---2pGwkL7M/000002.mp4.json ---2pGwkL7M/000003.info.json tar: write error These clips have been uploaded to gs://nvdata-ytsamples/yt8m-clips-{000000..000009}.tar . Processing Many Shards in Parallel One of the benefits of sharding is that you can process them in parallel. The YT8m-lo dataset consists of 3000 shards with about 2000 videos each. To process them in parallel, you can use standard tools. You need to modify the script a little to take a shard name and to upload the result into a common bucket. # splitshard.sh gsutil cat gs://lpr-yt8m-lo-sharded/yt8m-lo-$1.tar | tarp proc -m $(pwd)/extract-segments.sh - -o - | tarp split - -o yt8m-clips-$1-%06d.tar -p 'gsutil cp %s gs://my-output/%s && rm -f %s` On a single machine, you could run run this script with: for shard in {000000..003000}; do echo $shard; done | xargs -i bash splitshard.sh $shard More likely, you are going to run a job of this size in the cluster, using Kubernetes or some other job queuing program. Generally, you need to create a Docker container containing the above script (and all dependencies), and then submit it with a simple loop again: for shard in {000000..003000}; do echo $shard; done | submit-job --container=splitshard-container $1 If you store your shards on AIStore, you can also simply tell AIStore to transform a set of shards using the splitshard-container for you.","title":"Getting Started"},{"location":"ytsamples-split/#getting-started","text":"This worksheet shows how you can preprocess and split videos for deep learning. In these examples, we use Google Cloud Storage. In order to actually run this worksheet, you need: Install Google's gsutil program Install the tarp utility But it's not necessary to run the worksheet; you can follow it and apply the principles to whatever cloud, object, or local storage you use.","title":"Getting Started"},{"location":"ytsamples-split/#sharded-youtube-files","text":"We have about 6M videos downloaded from YouTube (part of the YouTube 8m dataset released by Google). These videos are stored as 3000 shards, each containing about 2000 videos and each about 56 Gbytes large. There is just one sample shard stored in the public bucket gs://nvdata-ytsamples . !gsutil ls gs://nvdata-ytsamples/yt8m-lo-*.tar | head gs://nvdata-ytsamples/yt8m-lo-000000.tar Each shard contains the video itself ( .mp4 ) plus a lot of associated metadata. !gsutil cat gs://nvdata-ytsamples/yt8m-lo-000000.tar | tar tf - | head ---2pGwkL7M.annotations.xml ---2pGwkL7M.description ---2pGwkL7M.dllog ---2pGwkL7M.info.json ---2pGwkL7M.mp4 ---2pGwkL7M.txt -2cScG5TqjQ.annotations.xml -2cScG5TqjQ.description -2cScG5TqjQ.dllog -2cScG5TqjQ.info.json","title":"Sharded YouTube Files"},{"location":"ytsamples-split/#splitting-the-videos","text":"For training, we usually don't want to use the entire YouTube video at once; they are variable length and are hard to fit into the GPU. Instead, we want to split up the video into frames or short clips. Here, we are using a script based on ffmpeg to split up each video into a set of clips. We also rescale all the images to a common size. The input video for this script is assumed to be sample.mp4 , and the output clips will be stored in files like sample-000000.mp4 . %%writefile extract-segments.sh #!/bin/bash exec > $(dirname $0)/_extract-segments-last.log 1>&2 #ps auxw --sort -vsz | sed 5q set -e set -x set -a size=${size:-256:128} duration=${duration:-2} count=${count:-999999999} # get mp4 metadata (total length, etc.) ffprobe sample.mp4 -v quiet -print_format json -show_format -show_streams > sample.mp4.json # perform the rescaling and splitting ffmpeg -loglevel error -stats -i sample.mp4 \\ -vf \"scale=$size:force_original_aspect_ratio=decrease,pad=$size:(ow-iw)/2:(oh-ih)/2\" \\ -c:a copy -f segment -segment_time $duration -reset_timestamps 1 \\ -segment_format_options movflags=+faststart \\ sample-%06d.mp4 # copy the metadata into each video fragment for s in sample-??????.mp4; do b=$(basename $s .mp4) cp sample.mp4.json $b.mp4.json || true cp sample.info.json $b.info.json || true done Writing extract-segments.sh !chmod 755 ./extract-segments.sh","title":"Splitting the Videos"},{"location":"ytsamples-split/#running-the-script-over-all-videos-in-a-shard","text":"Next, we use the tarp command to iterate the above script over each .mp4 file in shard 000000 . !tarp proc --help Usage: tarp [OPTIONS] proc [proc-OPTIONS] [Inputs...] Application Options: -v verbose output Help Options: -h, --help Show this help message [proc command options] -f, --field= fields to extract; name or name=old1,old2,old3 -o, --outputs= output file --slice= slice of input stream -c, --command= shell command running in each sample dir -m, --multicommand= shell command running in each sample dir --shell= shell command running in each sample dir (default: /bin/bash) %%writefile splitit.sh gsutil cat gs://lpr-yt8m-lo-sharded/yt8m-lo-000000.tar | tarp proc -m $(pwd)/extract-segments.sh - -o - | tarp split - -o yt8m-clips-%06d.tar Writing splitit.sh We can now run this script using: $ bash ./splitit.sh It's best to do this outside Jupyter since Jupyter doesn't work with long running shell jobs. Of course, if you want to run this code over all 3000 shards, you probably will want to submit jobs based on splitit.sh to some job queuing system. Also note that the --post option to tarp split lets us upload output shards as soon as they have been created, allowing the script to work with very little local storage.","title":"Running the Script over All Videos in a Shard"},{"location":"ytsamples-split/#output","text":"Let's have a look at the output to make sure it's OK. !tar tf yt8m-clips-000000.tar | head ---2pGwkL7M/000000.mp4.json ---2pGwkL7M/000000.info.json ---2pGwkL7M/000000.mp4 ---2pGwkL7M/000001.info.json ---2pGwkL7M/000001.mp4 ---2pGwkL7M/000001.mp4.json ---2pGwkL7M/000002.info.json ---2pGwkL7M/000002.mp4 ---2pGwkL7M/000002.mp4.json ---2pGwkL7M/000003.info.json tar: write error These clips have been uploaded to gs://nvdata-ytsamples/yt8m-clips-{000000..000009}.tar .","title":"Output"},{"location":"ytsamples-split/#processing-many-shards-in-parallel","text":"One of the benefits of sharding is that you can process them in parallel. The YT8m-lo dataset consists of 3000 shards with about 2000 videos each. To process them in parallel, you can use standard tools. You need to modify the script a little to take a shard name and to upload the result into a common bucket. # splitshard.sh gsutil cat gs://lpr-yt8m-lo-sharded/yt8m-lo-$1.tar | tarp proc -m $(pwd)/extract-segments.sh - -o - | tarp split - -o yt8m-clips-$1-%06d.tar -p 'gsutil cp %s gs://my-output/%s && rm -f %s` On a single machine, you could run run this script with: for shard in {000000..003000}; do echo $shard; done | xargs -i bash splitshard.sh $shard More likely, you are going to run a job of this size in the cluster, using Kubernetes or some other job queuing program. Generally, you need to create a Docker container containing the above script (and all dependencies), and then submit it with a simple loop again: for shard in {000000..003000}; do echo $shard; done | submit-job --container=splitshard-container $1 If you store your shards on AIStore, you can also simply tell AIStore to transform a set of shards using the splitshard-container for you.","title":"Processing Many Shards in Parallel"}]}